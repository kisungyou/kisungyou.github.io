[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Kisung You",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n  \n\n\n\n\nA Note on Angular Central Gaussian Distribution and its Matrix Variant\n\n\n\n\n\n\n\nnotes\n\n\ngeometric statistics\n\n\n\n\n\n\n\n\n\n\n\nAug 12, 2022\n\n\nKisung You\n\n\n\n\n\n\n  \n\n\n\n\nRodrigues’ formula for the Legendre polynomials\n\n\n\n\n\n\n\nnotes\n\n\ncalculus\n\n\n\n\n\n\n\n\n\n\n\nAug 10, 2022\n\n\nKisung You\n\n\n\n\n\n\n  \n\n\n\n\nMonte Carlo computation of \\(L_p\\) distance between two densities on the unit hypersphere\n\n\n\n\n\n\n\nnotes\n\n\ngeometric statistics\n\n\ncomputation\n\n\n\n\n\n\n\n\n\n\n\nAug 9, 2022\n\n\nKisung You\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Software",
    "section": "",
    "text": "Software\nGeometry and Topology\nRiemann\nLearning with Data on Riemannian manifold\n\n        \n        Website\n      \n        \n        View on Github\n      \n        \n        View on CRAN\n    \nRiemBase\nFunctions and C++ Headers for Computation on Manifolds\n\n        \n        View on Github\n      \n        \n        View on CRAN\n    \nTDAkit\nToolkit for Topological Data Analysis\n\n        \n        View on Github\n      \n        \n        View on CRAN\n    \nNetwork\ngraphon\nA Collection of Graphon Estimation Methods\n\n        \n        View on Github\n      \n        \n        View on CRAN\n    \nNetworkDistance\nDistance Measures for Networks\n\n        \n        View on Github\n      \n        \n        View on CRAN\n    \nStatistical Inference\nCovTools\nStatistical Tools for Covariance Analysis\n\n        \n        View on Github\n      \n        \n        View on CRAN\n    \nfilling\nMatrix Completion, Imputation, and Inpainting Methods\n\n        \n        View on Github\n      \n        \n        View on CRAN\n    \nRdimtools\nDimension Reduction and Estimation Methods\n\n        \n        Website\n      \n        \n        View on Github\n      \n        \n        View on CRAN\n    \nSBmedian\nScalable Bayes with Median of Subset Posteriors\n\n        \n        View on Github\n      \n        \n        View on CRAN\n    \nSHT\nStatistical Hypothesis Testing Toolbox\n\n        \n        Website\n      \n        \n        View on Github\n      \n        \n        View on CRAN\n    \nT4cluster\nTools for Cluster Analysis\n\n        \n        Website\n      \n        \n        View on Github\n      \n        \n        View on CRAN\n    \nOptimization and Applied Mathematics\nmaotai\nTools for Matrix Algebra, Optimization and Inference\n\n        \n        Website\n      \n        \n        View on Github\n      \n        \n        View on CRAN\n    \nRlinsolve\nIterative Solvers for (Sparse) Linear System of Equations\n\n        \n        View on Github\n      \n        \n        View on CRAN\n    \nT4transport\nTools for Computational Optimal Transport\n\n        \n        Website\n      \n        \n        View on Github\n      \n        \n        View on CRAN\n    \ntvR\nTotal Variation Regularization for Signals and Images\n\n        \n        View on Github\n      \n        \n        View on CRAN"
  },
  {
    "objectID": "publication.html",
    "href": "publication.html",
    "title": "Publications",
    "section": "",
    "text": "Publications\n\nPublished\n2024\nHuman-Algorithmic Interaction Using a Large Language Model-Augmented Artificial Intelligence Clinical Decision Support System Niroop Rajashekar, Yeo Eun Shin, Yuan Pu, Sunny Chung, KY, Mauro Giuffrè, Colleen Chan, Theo Saarinen, Allen Hsiao, Jasjeet Sekhon, Ambrose Wong, Leigh Evans, Rene Kizilcec, Loren Laine, Terika Mccall, and Dennis Shung.  CHI Conference on Human Factors in Computing Systems.\n        \n        Publisher's site\n    \nOn the Wasserstein median of probability measures KY, Dennis Shung, and Mauro Giuffrè.  Journal of Computational and Graphical Statistics.\n        \n        Preprint\n     \n        \n        Code\n     \n        \n        Publisher's site\n    \nSystematic review: The use of large language models as medical chatbots in digestive diseases Mauro Giuffrè, Simone Kresevic, KY, Johannes Dupont, Jack Huebner, Alyssa Grimshaw, and Dennis Shung.  Alimentary Pharmacology & Therapeutics.\n        \n        Publisher's site\n    \nBayesian Optimal Two-sample Tests for High-dimensional Gaussian Populations Kyoungjae Lee, KY, and Lizhen Lin.  Bayesian Analysis.\n        \n        Preprint\n     \n        \n        Publisher's site\n    \nAdoption of a gastroenterology hospitalist model and the impact on inpatient endoscopic practice volume: a controlled interrupted time-series analysis Dennis Shung, Darrick Li, KY, Kenneth Hung, Loren Laine, and Michelle Hughes.  iGIE.\n        \n        Publisher's site\n    \nDetection of Gastrointestinal Bleeding with Large Language Models to Aid Quality Improvement and Appropriate Reimbursement Neil Zheng, Vipina Keloth, KY, Daniel Kats, Darrick Li, Ohm Deshpande, Hamita Sachar, Hua Xu, Loren Laine, and Dennis Shung.  Gastroenterology.\n        \n        Publisher's site\n    \nPredicting response to non-selective beta-blockers with liver–spleen stiffness and heart rate in patients with liver cirrhosis and high-risk varices Mauro Giuffrè, Johannes Dupont, Alessia Visintin, Flora Masutti, Fabio Monica, KY, Dennis Shung, Lory Crocè, and The NSBB-Elasto-Response-Prediction Group.  Hepatology International.\n        \n        Publisher's site\n    \nOptimizing large language models in digestive disease: strategies and challenges to improve clinical outcomes Mauro Giuffrè, Simone Kresevic, Nicola Pugliese, KY, and Dennis Shung.  Liver International.\n        \n        Publisher's site\n    \nValidation of an Electronic Health Record–Based Machine Learning Model Compared With Clinical Risk Scores for Gastrointestinal Bleeding Dennis Shung, Colleen Chan, KY, Shinpei Nakamura, Theo Saarinen, Neil Zheng, Michael Simonov, Darrick Li, Cynthia Tsay, Yuki Kawamura, Matthew Shen, Allen Hsiao, Jasjeet Sekhon, and Loren Laine.  Gastroenterology.\n        \n        Publisher's site\n    \n2023\nOn the Spherical Laplace Distribution KY and Dennis Shung.  International Conference on Information Fusion (FUSION).\n        \n        Preprint\n     \n        \n        Publisher's site\n    \nSingle-cell analysis reveals inflammatory interactions driving macular degeneration Manik Kuchroo, Marcello DiStasio, Eric Song, Eda Calapkulu, Le Zhang, Maryam Ige, Amar Sheth, Abdelilah Majdoubi, Madhvi Menon, Alexander Tong, Abhinav Godavarthi, Yu Xing, Scott Gigante, Holly Steach, Jessie Huang, Guillaume Huguet, Janhavi Narain, KY, George Mourgkos, Rahul Dhodapkar, Matthew Hirn, Bastian Rieck, Guy Wolf, Smita Krishnaswamy, and Brian Hafler.  Nature Communications.\n        \n        Publisher's site\n    \nEvaluating ChatGPT in Medical Contexts: The Imperative to Guard Against Hallucinations and Partial Accuracies Mauro Giuffrè, KY, and Dennis Shung.  Clinical Gastroenterology and Hepatology.\n        \n        Publisher's site\n    \nAssessing the Usability of GutGPT: A Simulation Study of an AI Clinical Decision Support System for Gastrointestinal Bleeding Risk Colleen Chan, KY, Sunny Chung, Mauro Giuffrè, Theo Saarinen, Niroop Rajashekar, Yuan Pu, Yeo Eun Shin, Loren Laine, Ambrose Wong, Rene Kizilcec, Jasjeet Sekhon, and Dennis Shung.  Machine Learning for Health (ML4H) Symposium.\n        \n        Preprint\n    \n2022\nNetwork Distance Based on Laplacian Flows on Graphs Dianbin Bao, KY, and Lizhen Lin.  IEEE International Conference on Big Data (Big Data).\n        \n        Preprint\n     \n        \n        Code\n     \n        \n        Publisher's site\n    \nParameter estimation and model-based clustering with spherical normal distribution on the unit hypersphere KY and Changhee Suh.  Computational Statistics and Data Analysis.\n        \n        Preprint\n     \n        \n        Code\n     \n        \n        Publisher's site\n    \nRdimtools: An R package for Dimension Reduction and Intrinsic Dimension Estimation KY and Dennis Shung.  Software Impacts.\n        \n        Preprint\n     \n        \n        Code\n     \n        \n        Publisher's site\n    \n Learning Subspaces of Different Dimensions Brian St. Thomas, KY, Lizhen Lin, Lek-Heng Lim, and Sayan Mukherjee.  Journal of Computational and Graphical Statistics.\n        \n        Preprint\n     \n        \n        Code\n     \n        \n        Publisher's site\n    \nGeometric learning of functional brain network on the correlation manifold KY and Hae-Jeong Park.  Scientific Reports.\n        \n        Code\n     \n        \n        Publisher's site\n    \n2021\nRe-visiting Riemannian geometry of symmetric positive definite matrices for the analysis of functional connectivity KY and Hae-Jeong Park.  NeuroImage.\n        \n        Code\n     \n        \n        Publisher's site\n    \n2020\nData transforming augmentation for heteroscedastic models Hyungsuk Tak, KY, Sujit K. Ghosh, Bingyue Su, and Joseph Kelly.  Journal of Computational and Graphical Statistics.\n        \n        Preprint\n     \n        \n        Publisher's site\n    \n2019\nVolume change pattern of decompression of mandibular odontogenic keratocyst Jin Hoo Park, Eun-Jung Kwak, KY, Young-Soo Jung, and Hwi-Dong Jung.  Maxillofacial Plastic and Reconstructive Surgery.\n        \n        Publisher's site\n    \n2016\nVision-based detection of loosened bolts using the Hough transform and support vector machines Young-Jin Cha, KY, and Wooram Choi.  Automation in Construction.\n        \n        Publisher's site\n    \nPreprint\nPCA, SVD, and Centering of Data Donggun Kim and KY (2023).\n        \n        Preprint\n    \nComparing multiple latent space embeddings using topological analysis KY, Ilmun Kim, Ick Hoon Jin, Minjeong Jeon, and Dennis Shung (2022).\n        \n        Preprint\n     \n        \n        Code\n    \nShape-Preserving Dimensionality Reduction : An Algorithm and Measures of Topological Equivalence Byeongsu Yu and KY (2021).\n        \n        Preprint"
  },
  {
    "objectID": "posts/note003-angular-gaussian/index.html",
    "href": "posts/note003-angular-gaussian/index.html",
    "title": "A Note on Angular Central Gaussian Distribution and its Matrix Variant",
    "section": "",
    "text": "Introduction\nProbability distribution with explicit forms of densities are core elements of statistical inference. In this post, we review angular central gaussian (ACG) distribution on a unit hypersphere \\(\\mathbb{S}^{p-1} \\subset \\mathbb{R}^p\\) and its extension - matrix angular central gaussian (MACG) - defined on Stiefel \\(St(p,r)\\) and Grassman \\(Gr(p,r)\\) manifolds.\n\n\nAngular Central Gaussian Distribution\nOn \\(\\mathbb{S}^{p-1}\\), the ACG distribution \\(ACG_p (A)\\) as a density \\[\nf_{ACG} (x\\vert A) = |A|^{-1/2} (x^\\top Ax)^{-p/2}\n\\] for \\(x \\in \\mathbb{S}^{p-1}\\) and \\(A\\) a symmetric positive-definite matrix, i.e., \\(A=A^\\top \\in \\mathbb{R}^{p\\times p}\\) with \\(\\lambda_{min}(A)&gt;0\\). Let’s recap some properties of ACG distribution.\n\nProperty 1. \\(f_{ACG}(x|A) = f_{ACG}(-x|A)\\). This enables ACG as a distribution on the real projective space \\(\\mathbb{R}P^{p-1} = \\mathbb{S}^{p-1}/\\lbrace +1, -1 \\rbrace\\).\nProperty 2. \\(f_{ACG}(x|A) = f_{ACG}(x|cA),~c&gt;0\\). Common convention is to normalize the matrix \\(A\\) by a constraint \\(\\textrm{tr}(A) = p\\), which is useful (or even essential) in maximum likelihood estimation of the parameter to ensure algorithmic stability. If you want to show this property, simply use the fact that \\(|cA| = c^p|A|\\).\nProperty 3. When \\(x\\sim \\mathcal{N}_p (0,A) \\rightarrow x/\\|x\\| \\sim ACG_p (A)\\). This property is indeed an intuition behind its origination (Tyler 1987), which can be used for sampling.\n\n\nMaximum Likelihood Estimation\nGiven a random sample \\(x_1, \\ldots, x_p \\sim ACG_p (A)\\), Tyler (1987) proposed an iterative updating scheme to estimate the parameter \\(A\\) by\n\n\\[\\hat{A}_{k+1} = p \\left\\lbrace \\sum_{i=1}^n \\frac{1}{x_i^\\top \\hat{A}_k^{-1} x_i} \\right\\rbrace^{-1} \\sum_{i=1}^n \\frac{x_i x_i^\\top}{x_i^\\top \\hat{A}_k^{-1} x_i}, \\tag{1}\\]\n\nwhere \\(\\hat{A}_k\\) is the \\(k\\)-th iterate of an estimator with an initial starting point of an identity matrix \\(\\hat{A}_0 = I_p\\). While Equation 1 guarantees the convergence under mild conditions and abides by the constraint \\(\\textrm{tr}(\\hat{A}_k) = p\\), it is from the author’s previous work on \\(M\\)-estimation of the scatter matrix. Here, we provide a naive derivation of 2-step fixed-point iteration algorithm for pedagogical purpose.\n\n\\[\\hat{A}_{k'} = \\frac{p}{n}\\sum_{i=1}^n \\frac{x_i x_i^\\top}{x_i^\\top \\hat{A}_k^{-1} x_i}\\,\\,\\textrm{and}\\,\\, \\hat{A}_{k+1} = \\frac{p}{\\textrm{tr}(\\hat{A}_{k'})} \\hat{A}_{k'}. \\tag{2}\\]\n\nFirst, let’s write the log-likelihood \\[\n\\log L = -\\frac{n}{2}\\log\\det(A) - \\frac{p}{2} \\sum_{i=1}^n \\log (x_i^\\top A^{-1} x_i),\n\\] and recall two facts from matrix calculus (Petersen and Pedersen 2012) that \\[\n\\frac{\\partial \\log\\det(A)}{\\partial A} = A^{-1}\\,\\,\\textrm{and}\\,\\, \\frac{\\partial x^\\top A^{-1} x}{\\partial A} = -A^{-1}xx^\\top A^{-1}.\n\\] Then, the first-order condition for the log-likelihood can be written as\n\\[\n\\begin{gather*}\n    \\frac{\\partial \\log L}{\\partial A} = -\\frac{n}{2} A^{-1} + \\frac{p}{2} \\sum_{i=1}^n \\frac{A^{-1} x_i x_i^\\top A^{-1}}{x_i^\\top A^{-1} x_i} \\\\\n    A^{-1} = \\frac{p}{n} \\sum_{i=1}^n \\frac{A^{-1} x_i x_i^\\top A^{-1}}{x_i^\\top A^{-1} x_i} \\\\\n    A = \\frac{p}{n} \\sum_{i=1}^n \\frac{ x_i x_i^\\top }{x_i^\\top A^{-1} x_i}\n\\end{gather*}\n\\] where the last equality comes from multiplying \\(A\\) from left and right. Therefore, \\(\\hat{A}\\) is a solution of the matrix equation in a form \\(X = f(X)\\) where \\(f\\) is a contraction mapping under some conditions (Tyler 1987). This leads to Equation 2 while projection step is added to keep \\(\\text{tr}(\\hat{A}_k) = p\\) for all \\(k=1,2,\\cdots\\).\n\n\n\nMatrix Angular Central Gaussian Distribution\nChikuse (1990) extended the distribution to the matrix case, namely Stiefel and Grassmann manifolds \\[\n\\begin{gather*}\n    St(p,r) = \\{X\\in \\mathbb{R}^{p\\times r} ~\\vert~ X^\\top X = I_p\\}\\\\\n    Gr(p,r) = \\{\\text{Span}(X) ~\\vert~ X \\in \\mathbb{R}^{p\\times r},~\\text{rank}(X)=r\\}\n\\end{gather*}\n\\] which are sets of orthonormal \\(k\\)-frames and \\(k\\)-subspaces. The Matrix Angular Central Gaussian (MACG) distribution \\(MACG_{p,r}(\\Sigma)\\) has a density \\[\nf_{MACG}(X\\vert \\Sigma) = |\\Sigma|^{-r/2} |X^\\top \\Sigma^{-1} X|^{-p/2}\n\\] where \\(\\Sigma\\) is a symmetric positive-definite matrix. Note that the density is very similar to what we had before for vector-valued distribution. Likewise, it shares properties as before.\n\nProperty 1. \\(f_{MACG}(X|\\Sigma) = f_{MACG}(-X|\\Sigma)\\).\nProperty 2. \\(f_{MACG}(X|\\Sigma) = f_{MACG}(X|c\\Sigma),~c&gt;0\\).\nProperty 3. \\(f_{MACG}(X|\\Sigma) = f_{MACG}(XR|\\Sigma)\\) for \\(R\\in O(r)\\). This property enables to consider MACG as a distribution on Grassmann manifold, which are quotient by modulo orthogonal transformation.\n\n\nSampling from MACG\nIn order to draw random samples from \\(MACG_{p,r}(\\Sigma)\\), we need the following steps, which are common in directional statistics with Stiefel/Grassmann manifolds . First, draw \\(r\\) random vectors \\(x_1,\\ldots,x_r \\sim \\mathcal{N}_p (0,\\Sigma)\\) and stack them as columns \\(X=[x_1|\\cdots|x_r] \\in \\mathbb{R}^{p\\times r}\\). Then, \\[\n    Y = X (X^\\top X)^{-1/2} \\sim MACG_{p,r}(\\Sigma)\n\\] where the negative square root for a symmetric positive-definite matrix can be obtained from eigen-decomposition, \\[\n\\begin{gather*}\n\\Omega = UDU^\\top \\rightarrow \\Omega^{-1/2} = UD^{-1/2} U^\\top \\\\\n\\left[D^{-1/2}\\right]_{ij} = \\frac{1}{\\sqrt{d_{ij}}} \\textrm{ when } i = j \\textrm{ and }0\\textrm{ otherwise.}\n\\end{gather*}\n\\]\n\n\nMaximum Likelihood Estimation\nSimilar to the ACG case, given a random sample \\(X_1,X_2,\\ldots,X_n \\sim MACG_{p,r}(\\Sigma)\\), we can obtain a two-step iterative scheme to estimate the parameter \\(\\Sigma\\),\n\n\\[\\begin{gather*}\n\\hat{\\Sigma}_{k'} = \\frac{p}{nr} \\sum_{i=1}^n\nX_i (X_i^\\top \\Sigma^{-1} X_i)^{-1} X_i \\\\\n\\hat{\\Sigma}_{k+1} =  \\frac{p}{\\text{tr}(\\hat{\\Sigma}_{k'})} \\hat{\\Sigma}_{k'}.\\end{gather*} \\tag{3}\\]\n\nDerivation of formula Equation 3 follows the similar line of before. We need another fact from matrix calculus that \\[\n\\frac{\\partial }{\\partial \\Sigma} \\log\\det(X^\\top \\Sigma^{-1} X) = - \\Sigma^{-1} X (X^\\top \\Sigma^{-1} X)^{-1} X^\\top \\Sigma^{-1}.\n\\] First, log-likelihood is written as \\[\n    \\log L  = -\\frac{nr}{2}\\log\\det(\\Sigma) - \\frac{p}{2}\\sum_{i=1}^n \\log\\det (X_i^\\top \\Sigma^{-1} X_i)\n\\] where the first-order condition gives \\[\n\\begin{gather*}\n    \\frac{\\partial \\log L}{\\partial \\Sigma} = -\\frac{nr}{2}\\Sigma^{-1} + \\frac{p}{2}\\sum_{i=1}^n \\left( \\Sigma^{-1} X_i (X_i^\\top \\Sigma^{-1} X_i)^{-1} X_i^\\top \\Sigma^{-1} \\right)\\\\\n    \\frac{nr}{2} \\Sigma^{-1} = \\frac{p}{2}\\sum_{i=1}^n \\left( \\Sigma^{-1} X_i (X_i^\\top \\Sigma^{-1} X_i)^{-1} X_i^\\top \\Sigma^{-1} \\right) \\\\\n    \\Sigma = \\frac{p}{nr} \\sum_{i=1}^n X_i (X_i^\\top \\Sigma^{-1} X_i)^{-1} X_i^\\top\n\\end{gather*}\n\\] where the last equality comes from multiplying \\(\\Sigma\\) from left and right. Therefore, \\(\\hat{\\Sigma}\\) is a solution of the matrix equation, leading to the formula of Equation 3 with an additional projection step to keep \\(\\text{tr}(\\hat{\\Sigma}_k) = p\\) for all \\(k=1,2,\\cdots\\). Note that this matrix equation, up to my knowledge, has not known whether the mapping is contraction or not.\n\n\n\nConclusion\nACG and MACG distributions are simple yet rather little used in directional statistics. We hope that this brief note boosts probabilistic inference on corresponding manifolds at ease. An R package Riemann, which is also available on CRAN, implements density evaluation, random sample generation, and maximum likelihood estimation of the scatter parameters \\(A\\) and \\(\\Sigma\\) in the light of expecting handy utilization of the distributions we introduced.\n\n\nReferences\n\n\nChikuse, Yasuko. 1990. “The Matrix Angular Central Gaussian Distribution.” Journal of Multivariate Analysis 33 (2): 265–74. https://doi.org/10.1016/0047-259X(90)90050-R.\n\n\nPetersen, K. B., and M. S. Pedersen. 2012. “The Matrix Cookbook.” Technical University of Denmark.\n\n\nTyler, David E. 1987. “Statistical Analysis for the Angular Central Gaussian Distribution on the Sphere.” Biometrika 74 (3): 579–89. https://doi.org/10.1093/biomet/74.3.579.\n\n\n\n\n\n\nCitationBibTeX citation:@online{you2022,\n  author = {You, Kisung},\n  title = {A {Note} on {Angular} {Central} {Gaussian} {Distribution} and\n    Its {Matrix} {Variant}},\n  date = {2022-08-12},\n  url = {https://kisungyou.com/posts/note003-angular-gaussian},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nYou, Kisung. 2022. “A Note on Angular Central Gaussian\nDistribution and Its Matrix Variant.” August 12, 2022. https://kisungyou.com/posts/note003-angular-gaussian."
  },
  {
    "objectID": "posts/note002-rodrigues-formula/index.html",
    "href": "posts/note002-rodrigues-formula/index.html",
    "title": "Rodrigues’ formula for the Legendre polynomials",
    "section": "",
    "text": "Introduction\nLegendre polynomials \\(P_n (x)\\) are solutions of the following Legendre’s differential equation \\[\n(1-x^2) y'' - 2x y' + n(n+1) y = 0\n\\tag{1}\\] for some \\(n \\in \\mathbb{N}\\cup \\{0\\}\\). An explicit, compact expression for the polynomials is provided by Rodrigues’ formula \\[\nP_n (x) = \\frac{1}{2^n n!} \\frac{d^n}{dx^n} (x^2-1)^n.\n\\tag{2}\\]\nThis means that when \\(P_n (x)\\) is plugged in the position of \\(y\\) for Equation 1, it must satisfy the equality to 0. In this post, we show (a bit tedious) derivations to attain Equation 2.\n\n\nApproach\nI will proceed in two steps. Let \\(f_n (x) = (x^2 - 1)^n\\) then we first show that the \\(n\\)-th derivative of \\(f_n (x)\\) is a solution of Legendre equation. Then, we find a proper scaling factor of \\(1/ 2^n n!\\) to recover \\(P_n (x)\\) in line with a common constraint that \\(P_n (x) = 1\\) for all \\(n\\) when \\(x=1\\). For notational simplicity, we denote \\(g^{(n)}\\) for the \\(n\\)-th derivative of a function \\(g(x)\\), i.e, \\[\ng^{(n)} = \\frac{d^n}{dx^n} g(x).\n\\]\nBefore proceeding, we need (generalized) Leibniz’s rule. Suppose we have \\(n\\)-times differentiable functions \\(f(x)\\) and \\(g(x)\\), then \\[\n\\frac{d^n}{dx^n} f(x) g(x) = \\sum_{k=0}^n\n\\begin{pmatrix}\nn \\\\ k\n\\end{pmatrix}\nf^{(n-k)} (x) g^{(k)} (x) = \\sum_{k=0}^n \\frac{n!}{k! (n-k)!} f^{(n-k)} (x) g^{(k)} (x)\n\\tag{3}\\] where the choice of \\(f\\) and \\(g\\) can help in reducing the number of terms when there exists a polynomial term. For example, when \\(g(x) = x^2\\), \\(g^{(k)} = 0\\) for all \\(k \\geq 3\\).\n\n\nStep 1. \\(f_n^{(n)} (x)\\) is one solution.\nOur goal here is to show that \\(f_n^{(n)}(x)\\) is a solution for Equation 1. As a first step, let’s take derivative on \\(f_n (x)\\), \\[\n\\begin{align*}\n\\frac{d}{dx} f_n (x) &= 2 n (x^2  - 1)^{n-1} x \\\\\n&= 2nx (x^2 - 1)^{n-1}.\n\\end{align*}\n\\] By multiplying \\((x^2 - 1)\\) both side of the above, we have \\[\n(x^2 - 1) \\frac{d}{dx} f_n (x)  = 2nx (x^2 - 1)^n.\n\\]\nNow, differentiate both sides \\((n+1)\\) times, which leads to \\[\n\\begin{align*}\n\\frac{d^{n+1}}{dx^{n+1}} \\left[ \\frac{d}{dx} f_n (x) \\right]  (x^2 -1)\n&= \\sum_{k=0}^{n+1}\n\\begin{pmatrix}\nn+1 \\\\ k\n\\end{pmatrix}\n\\left( \\frac{d}{dx} f_n (x) \\right)^{(n+1-k)} (x^2-1)^{(k)} \\\\\n&=\n\\begin{pmatrix}\nn+1 \\\\ 0\n\\end{pmatrix}\nf_n^{(n+2)} (x) (x^2-1) +\n\\begin{pmatrix}\nn+1 \\\\ 1\n\\end{pmatrix}\n2x f_n^{(n+1)}  (x) +\n\\begin{pmatrix}\nn+1 \\\\ 2\n\\end{pmatrix}\nf_n ^{(n)} (x) \\cdot 2 \\\\\n&= (x^2-1) f_n^{(n+2)} (x) + 2(n+1)x f_n^{(n+1)} (x) +\nn(n+1) f_n^{(n)} (x)\n\\end{align*}\n\\] for the left-hand side. We also have that \\[\n\\begin{align*}\n\\frac{d^{n+1}}{dx^{n+1}} f_n(x) 2nx &=\n\\begin{pmatrix}\nn+1 \\\\ 0\n\\end{pmatrix}\nf_n^{(n+1)} (x) 2nx +\n\\begin{pmatrix}\nn+1 \\\\ 1\n\\end{pmatrix}\nf_n^{(n)} (x) 2n \\\\\n&= 2nx f_n^{(n+1)} (x) + 2n(n+1)f_n^{(n)}(x).\n\\end{align*}\n\\] Therefore, we have the following arrangement, \\[\n\\begin{equation*}\n    \\begin{gathered}\n    (x^2 - 1) f_n^{(n+2)} (x) + 2x(n+1)f_n^{(n+1)} (x) + n(n+1)f_n^{(n)} (x) =\n    2nx f_n^{(n+1)} (x) + 2n(n+1) f_n^{(n)} (x) \\\\\n    (x^2-1) f_n^{(n+2)} (x) + 2x f_n^{(n+1)}(x) - n(n+1)f_n^{(n)} (x) = 0 \\\\\n    (1- x^2) f_n^{(n+2)} (x) - 2x f_n^{(n+1)}(x) + n(n+1)f_n^{(n)} (x) = 0\n    \\end{gathered}\n\\end{equation*}\n\\] where the last line is in the form of Equation 1 so that we have \\(f_n^{(n)} (x)\\) as a solution.\n\n\nStep 2. Find a scaling factor.\nEven though \\(f_n^{(n)} (x)\\) as a solution, we have a requirement for the standard Legendre polynomial that \\(P_n (x)=1\\) for \\(x=1\\). Let us take a closer look at \\(f_n^{(n)}(x)\\) when evaluated at \\(x=1\\). \\[\n\\begin{align*}\nf_n^{(n)} (x) &= \\frac{d^n}{dx^n} (x^2-1)^n \\\\\n&= \\frac{d^n}{dx^n} (x+1)^n (x-1)^n \\\\\n&= \\sum_{k=0}^n \\begin{pmatrix}\nn \\\\ k\n\\end{pmatrix}\n\\left( (x+1)^n \\right)^{(k)} \\left( (x-1)^n \\right) ^{(n-k)} \\quad \\textrm{by the Leibniz's rule} \\\\\n&= \\sum_{k=0}^n \\begin{pmatrix}\nn \\\\ k\n\\end{pmatrix}\n\\frac{n!}{(n-k)!} (x+1)^{n-k} \\frac{n!}{k!} (x-1)^k \\qquad (*) \\\\\n&= n! \\sum_{k=0}^n \\begin{pmatrix}\nn \\\\ k\n\\end{pmatrix} \\frac{n!}{(n-k)! k!} (x+1)^{n-k} (x-1)^k \\\\\n&= n! \\sum_{k=0}^n \\begin{pmatrix}\nn \\\\ k\n\\end{pmatrix}^2 (x+1)^{n-k} (x-1)^k,\n\\end{align*}\n\\] Since we want to evaluate \\(f_n^{(n)} (x)\\) at \\(x=1\\), the last line of equations above tells us that all the terms but \\(k=0\\) become zero, \\[\n\\begin{equation*}\nf_n^{(n)} (x=1) = n! \\begin{pmatrix}\nn \\\\ 0\n\\end{pmatrix}^2 2^{n-0} = n! 2^n\n\\end{equation*}\n\\] which finally leads to define \\(P_n (x)\\) as \\[\nP_n (x) = \\frac{1}{n! 2^n}f_n^{(n)}(x)  = \\frac{1}{n! 2^n} \\frac{d^n}{dx^n} (x^2-1)^n\n\\] to fulfill the condition of \\(P_n (x) =1\\) for \\(x=1\\).\n\n\n\n\nCitationBibTeX citation:@online{you2022,\n  author = {You, Kisung},\n  title = {Rodrigues’ Formula for the {Legendre} Polynomials},\n  date = {2022-08-10},\n  url = {https://kisungyou.com/posts/note002-rodrigues-formula},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nYou, Kisung. 2022. “Rodrigues’ Formula for the Legendre\nPolynomials.” August 10, 2022. https://kisungyou.com/posts/note002-rodrigues-formula."
  },
  {
    "objectID": "posts/note001-spherical-distance/index.html",
    "href": "posts/note001-spherical-distance/index.html",
    "title": "Monte Carlo computation of \\(L_p\\) distance between two densities on the unit hypersphere",
    "section": "",
    "text": "Problem Statement\nA \\(d\\)-dimensional unit hypersphere \\(\\mathbb{S}^d = \\lbrace x \\in \\mathbb{R}^{d+1}~|~ \\|x\\|_2^2 = \\sum_{i=1}^{d+1} x_i^2 = 1\\rbrace\\) is one of the standard mathematical spaces in the field of objected-oriented data analysis (Marron and Dryden 2021). Let \\(\\mathcal{P}(\\mathbb{S}^d)\\) denote a space of probability densities on \\(\\mathbb{S}^d\\). For two densities \\(f,g\\in\\mathcal{P}(\\mathbb{S}^d)\\), it is frequently needed to measure dissimilarity between the two. Unfortunately, even for the most well-known distributions on the hypersphere, analytic formula for any discrepancy measure is rarely available, leading to require numerical schemes for approximation. Here we focus on \\(L_p\\) distance between the two densities, \\[\nL_p (f,g) = \\left( \\int_{\\mathbb{S}^d} |f(x) - g(x)|^p \\right)^{1/p}\n\\tag{1}\\] and we show how to combine Monte Carlo way of integration by means of importance sampling to approximate Equation 1.\n\n\nComputation\nImportance sampling requires a proposal density. The easiest choice is to use uniform density \\(u(x)\\) as an importance proposal since sampling from \\(u(x)\\) is trivial. First, take a random sample from standard normal distribution \\(x \\sim \\mathcal{N}(0,I)\\) in \\(\\mathbb{R}^{d+1}\\). Then, the rest is to take \\(L_2\\) normalization, i.e., \\(x \\leftarrow x / \\|x\\|_2\\), which makes a sampled vector to have a unit norm. Given the sample generation process, we have the following \\[\\begin{aligned}\nL_p (f,g)^p &= \\int_{\\mathbb{S}^d} |f(x)-g(x)|^p dx \\\\\n&= \\int_{\\mathbb{S}^d} \\frac{|f(x)-g(x)|^p}{u(x)} u(x)  dx \\\\\n&= \\mathbb{E}_{u(x)} \\left\\lbrack \\frac{|f(x)-g(x)|^p}{u(x)} \\right\\rbrack\\\\\n&\\approx \\frac{1}{N} \\sum_{n=1}^N \\frac{|f(x_n)-g(x_n)|^p}{u(x_n)} \\,\\,\\textrm{for}\\,\\, x_n \\overset{iid}{\\sim} u(x),\n\\end{aligned}\n\\] where the last term gets better approximation as \\(N\\rightarrow \\infty\\).\nHere the uniform density \\(u(x)\\) is an inverse of the surface area of the \\(d\\)-dimensional sphere \\(S_n\\), which is defined as \\[\nS_n = \\frac{2\\pi^{(n+1)/2}}{\\Gamma((n+1)/2)},\n\\] where \\(\\Gamma(x)\\) is the gamma function.\n\n\nReferences\n\n\nMarron, James Stephen, and I. L. Dryden. 2021. Object Oriented Data Analysis. Boca Raton: Taylor & Francis Group, LLC.\n\n\n\n\n\n\nCitationBibTeX citation:@online{you2022,\n  author = {You, Kisung},\n  title = {Monte {Carlo} Computation of {\\$L\\_p\\$} Distance Between Two\n    Densities on the Unit Hypersphere},\n  date = {2022-08-09},\n  url = {https://kisungyou.com/posts/note001-spherical-distance},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nYou, Kisung. 2022. “Monte Carlo Computation of $L_p$ Distance\nBetween Two Densities on the Unit Hypersphere.” August 9, 2022.\nhttps://kisungyou.com/posts/note001-spherical-distance."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Kisung You",
    "section": "",
    "text": "Welcome!\nI am an assistant professor in the Department of Mathematics at Baruch College, City University of New York and a research affiliate at the Yale School of Medicine. My research revolves around harnessing mathematical tools from statistics, geometry, and topology to design innovative methods aimed at unraveling intricate scientific inquiries. My primary focus resides within the domain of biomedical data analysis. Additionally, I have deep enthusiasm for high-performance computing and the development of statistical software, driven by the aspiration to make methodological breakthroughs accessible to a broader audience of practitioners.\n\n\nContact\nFor students, please use my email available from the department directory."
  },
  {
    "objectID": "dgkim_publication.html",
    "href": "dgkim_publication.html",
    "title": "Publications",
    "section": "",
    "text": "Publications\n\nPublished\n2024\nHuman-Algorithmic Interaction Using a Large Language Model-Augmented Artificial Intelligence Clinical Decision Support System Niroop Rajashekar, Yeo Eun Shin, Yuan Pu, Sunny Chung, KY, Mauro Giuffrè, Colleen Chan, Theo Saarinen, Allen Hsiao, Jasjeet Sekhon, Ambrose Wong, Leigh Evans, Rene Kizilcec, Loren Laine, Terika Mccall, and Dennis Shung.  CHI Conference on Human Factors in Computing Systems.\n    \n    Publisher's Site\n  \nOn the Wasserstein median of probability measures KY, Dennis Shung, and Mauro Giuffrè.  Journal of Computational and Graphical Statistics.\n    \n    arXiv\n   \n    \n    Code\n   \n    \n    Publisher's Site\n  \nSystematic review: The use of large language models as medical chatbots in digestive diseases Mauro Giuffrè, Simone Kresevic, KY, Johannes Dupont, Jack Huebner, Alyssa Grimshaw, and Dennis Shung.  Alimentary Pharmacology & Therapeutics.\n    \n    Publisher's Site\n  \nBayesian Optimal Two-sample Tests for High-dimensional Gaussian Populations Kyoungjae Lee, KY, and Lizhen Lin.  Bayesian Analysis.\n    \n    arXiv\n   \n    \n    Publisher's Site\n  \nAdoption of a gastroenterology hospitalist model and the impact on inpatient endoscopic practice volume: a controlled interrupted time-series analysis Dennis Shung, Darrick Li, KY, Kenneth Hung, Loren Laine, and Michelle Hughes.  iGIE.\n    \n    Publisher's Site\n  \nDetection of Gastrointestinal Bleeding with Large Language Models to Aid Quality Improvement and Appropriate Reimbursement Neil Zheng, Vipina Keloth, KY, Daniel Kats, Darrick Li, Ohm Deshpande, Hamita Sachar, Hua Xu, Loren Laine, and Dennis Shung.  Gastroenterology.\n    \n    Publisher's Site\n  \nPredicting response to non-selective beta-blockers with liver–spleen stiffness and heart rate in patients with liver cirrhosis and high-risk varices Mauro Giuffrè, Johannes Dupont, Alessia Visintin, Flora Masutti, Fabio Monica, KY, Dennis Shung, Lory Crocè, and The NSBB-Elasto-Response-Prediction Group.  Hepatology International.\n    \n    Publisher's Site\n  \nOptimizing large language models in digestive disease: strategies and challenges to improve clinical outcomes Mauro Giuffrè, Simone Kresevic, Nicola Pugliese, KY, and Dennis Shung.  Liver International.\n    \n    Publisher's Site\n  \nValidation of an Electronic Health Record–Based Machine Learning Model Compared With Clinical Risk Scores for Gastrointestinal Bleeding Dennis Shung, Colleen Chan, KY, Shinpei Nakamura, Theo Saarinen, Neil Zheng, Michael Simonov, Darrick Li, Cynthia Tsay, Yuki Kawamura, Matthew Shen, Allen Hsiao, Jasjeet Sekhon, and Loren Laine.  Gastroenterology.\n    \n    Publisher's Site\n  \n2023\nOn the Spherical Laplace Distribution KY and Dennis Shung.  International Conference on Information Fusion (FUSION).\n    \n    arXiv\n   \n    \n    Publisher's Site\n  \nSingle-cell analysis reveals inflammatory interactions driving macular degeneration Manik Kuchroo, Marcello DiStasio, Eric Song, Eda Calapkulu, Le Zhang, Maryam Ige, Amar Sheth, Abdelilah Majdoubi, Madhvi Menon, Alexander Tong, Abhinav Godavarthi, Yu Xing, Scott Gigante, Holly Steach, Jessie Huang, Guillaume Huguet, Janhavi Narain, KY, George Mourgkos, Rahul Dhodapkar, Matthew Hirn, Bastian Rieck, Guy Wolf, Smita Krishnaswamy, and Brian Hafler.  Nature Communications.\n    \n    Publisher's Site\n  \nEvaluating ChatGPT in Medical Contexts: The Imperative to Guard Against Hallucinations and Partial Accuracies Mauro Giuffrè, KY, and Dennis Shung.  Clinical Gastroenterology and Hepatology.\n    \n    Publisher's Site\n  \nAssessing the Usability of GutGPT: A Simulation Study of an AI Clinical Decision Support System for Gastrointestinal Bleeding Risk Colleen Chan, KY, Sunny Chung, Mauro Giuffrè, Theo Saarinen, Niroop Rajashekar, Yuan Pu, Yeo Eun Shin, Loren Laine, Ambrose Wong, Rene Kizilcec, Jasjeet Sekhon, and Dennis Shung.  Machine Learning for Health (ML4H) Symposium.\n    \n    arXiv\n   \n    \n    Publisher's Site\n  \n2022\nNetwork Distance Based on Laplacian Flows on Graphs Dianbin Bao, KY, and Lizhen Lin.  IEEE International Conference on Big Data (Big Data).\n    \n    arXiv\n   \n    \n    Code\n   \n    \n    Publisher's Site\n  \nParameter estimation and model-based clustering with spherical normal distribution on the unit hypersphere KY and Changhee Suh.  Computational Statistics and Data Analysis.\n    \n    arXiv\n   \n    \n    Code\n   \n    \n    Publisher's Site\n  \nRdimtools: An R package for Dimension Reduction and Intrinsic Dimension Estimation KY and Dennis Shung.  Software Impacts.\n    \n    arXiv\n   \n    \n    Code\n   \n    \n    Publisher's Site\n  \n Learning Subspaces of Different Dimensions Brian St. Thomas, KY, Lizhen Lin, Lek-Heng Lim, and Sayan Mukherjee.  Journal of Computational and Graphical Statistics.\n    \n    arXiv\n   \n    \n    Code\n   \n    \n    Publisher's Site\n  \nGeometric learning of functional brain network on the correlation manifold KY and Hae-Jeong Park.  Scientific Reports.\n    \n    Code\n   \n    \n    Publisher's Site\n  \n2021\nRe-visiting Riemannian geometry of symmetric positive definite matrices for the analysis of functional connectivity KY and Hae-Jeong Park.  NeuroImage.\n    \n    Code\n   \n    \n    Publisher's Site\n  \n2020\nData transforming augmentation for heteroscedastic models Hyungsuk Tak, KY, Sujit K. Ghosh, Bingyue Su, and Joseph Kelly.  Journal of Computational and Graphical Statistics.\n    \n    arXiv\n   \n    \n    Publisher's Site\n  \n2019\nVolume change pattern of decompression of mandibular odontogenic keratocyst Jin Hoo Park, Eun-Jung Kwak, KY, Young-Soo Jung, and Hwi-Dong Jung.  Maxillofacial Plastic and Reconstructive Surgery.\n    \n    Publisher's Site\n  \n2016\nVision-based detection of loosened bolts using the Hough transform and support vector machines Young-Jin Cha, KY, and Wooram Choi.  Automation in Construction.\n    \n    Publisher's Site\n  \nPreprint\nPCA, SVD, and Centering of Data Donggun Kim and KY (2023).\n    \n    arXiv\n  \nComparing multiple latent space embeddings using topological analysis KY, Ilmun Kim, Ick Hoon Jin, Minjeong Jeon, and Dennis Shung (2022).\n    \n    arXiv\n   \n    \n    Code\n  \nShape-Preserving Dimensionality Reduction : An Algorithm and Measures of Topological Equivalence Byeongsu Yu and KY (2021).\n    \n    arXiv"
  }
]