[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Kisung You",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n    \n  \n\n\n\n\n\n\n\n\n\n\nGradient of squared distance on a Riemannian manifold\n\n\n\n\n\nMay 24, 2025\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\nThe space of Dirac measures is dense in Wasserstein space\n\n\n\n\n\nMay 17, 2025\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\nWhat is the Fisher-Rao distance?\n\n\n\n\n\nFeb 5, 2025\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\nWhat is the Angular Central Gaussian distribution?\n\n\n\n\n\nAug 12, 2022\n\n5 min\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Blog_kr/kr_001_StatAgency.html",
    "href": "Blog_kr/kr_001_StatAgency.html",
    "title": "í†µê³„í•™ìë„, ë°ì´í„° ê³¼í•™ìë„ ì•„ë‹Œ ì‚¬ëŒì´ í†µê³„ì²­ì¥ì„ ë§¡ëŠ” ë‚˜ë¼",
    "section": "",
    "text": "í†µê³„ëŠ” ê²°ê³¼ê°€ ì•„ë‹ˆë¼ ì„ íƒì˜ ì‚°ë¬¼ì´ë‹¤.\nì§€ë‚œ 10ì—¬ ë…„ ë™ì•ˆ â€œë°ì´í„° ê¸°ë°˜ ì‚¬íšŒâ€ë¼ëŠ” êµ¬í˜¸ëŠ” ëŒ€ì¤‘ë§¤ì²´ì™€ ì •ì±…ë¬¸ì„œì—ì„œ ë°˜ë³µë˜ë©° ë§ˆì¹˜ ìƒì‹ì²˜ëŸ¼ ìë¦¬ ì¡ì•˜ë‹¤. ê·¸ëŸ¬ë‚˜ ì •ì‘ í†µê³„ë¥¼ ì „ë¬¸ì ìœ¼ë¡œ ë‹¤ë£¨ì§€ ì•ŠëŠ” ë‹¤ìˆ˜ì—ê²Œ í†µê³„ëŠ” ì—¬ì „íˆ ê°ê´€ì  ì§„ì‹¤ì˜ ìˆ«ìë¡œ ë°›ì•„ë“¤ì—¬ì§„ë‹¤. ì¸¡ì • ë°©ë²•ê³¼ í‘œë³¸ ì„¤ê³„, ëª¨í˜• ì„¤ì •ì´ë¼ëŠ” ì „ì œì¡°ê±´ì´ ë¹ ì§„ì±„ë¡œ ë§ì´ë‹¤. ì´ ê¸€ì€ ë°”ë¡œ ê·¸ ì§€ì ì„ ì§ˆë¬¸í•œë‹¤.\në°ì´í„°ëŠ” ëˆ„ê°€ í•´ì„í•˜ê³ , ê·¸ í•´ì„ì€ ì–´ë–¤ ì„ íƒ ìœ„ì— ì„œ ìˆëŠ”ê°€?\ní†µê³„ë¥¼ ë‘˜ëŸ¬ì‹¼ ëŒ€ì¤‘ì  ì˜¤í•´ë¥¼ ê°„ë‹¨íˆ ì •ë¦¬í•´ ë³´ì.\nì´ ì„¸ ê°€ì§€ëŠ” í†µê³„ë¥¼ â€œê°ê´€ì  ë¬´ì˜¤ë¥˜ì˜ ì§€í‘œâ€ë¡œ ì†Œë¹„í•˜ê²Œ ë§Œë“œëŠ” ìœ„í—˜í•œ í™˜ìƒì„ ë“œëŸ¬ë‚¸ë‹¤. ê·¸ë¦¬ê³  ì´ í™˜ìƒì„ ê¹¨ëœ¨ë¦´ ì±…ì„ì´ ëŒ€í•œë¯¼êµ­ í†µê³„ì²­ì— ìˆë‹¤."
  },
  {
    "objectID": "Blog_kr/kr_001_StatAgency.html#footnotes",
    "href": "Blog_kr/kr_001_StatAgency.html#footnotes",
    "title": "í†µê³„í•™ìë„, ë°ì´í„° ê³¼í•™ìë„ ì•„ë‹Œ ì‚¬ëŒì´ í†µê³„ì²­ì¥ì„ ë§¡ëŠ” ë‚˜ë¼",
    "section": "Footnotes",
    "text": "Footnotes\n\n\ní†µê³„ì²­ í™ˆí˜ì´ì§€ ì°¸ê³ â†©ï¸\nì´ëª…ë°• ì •ë¶€ ì´ë˜ë¡œ ì´ 10ëª…ì˜ í†µê³„ì²­ì¥ì´ ì„ëª…ë˜ì—ˆë‹¤. í–‰ì •ê³ ì‹œ ì´í›„ ìœ í•™ì„ í†µí•´ ê²½ì˜í•™ ì„ì‚¬ë¥¼ ë°›ì€ ì‚¬ëŒì€ 2ëª…, ê²½ì œí•™ ë°•ì‚¬ë¥¼ ë°›ì€ ì‚¬ëŒì€ 3ëª…ì´ë‹¤. ë‚˜ë¨¸ì§€ 5ëª…ì€ ëª¨ë‘ í•™ê³„ì—ì„œ ê²½ì œí•™ ë°•ì‚¬ë¥¼ ë°›ìœ¼ì‹  ë¶„ë“¤ë¡œì„œ ê·¸ë“¤ì˜ ì „ê³µ ë¶„ì•¼ëŠ” ê³„ëŸ‰ ê²½ì œí•™(1), ë§ˆë¥´í¬ìŠ¤ ê²½ì œí•™(1), ë…¸ë™ ê²½ì œí•™(2), ê·¸ë¦¬ê³  ì¬ì •í•™(1)ì´ë‹¤.â†©ï¸\ní†µê³„í•™ì„ ê³µë¶€í•˜ë©° ì´ ë§ì„ ë“¤ì–´ë³´ì§€ ëª»í•œ ì‚¬ëŒì€ ì•„ë§ˆë„ ì—†ì„ ê²ƒì´ë‹¤. ìˆ˜ì—…ì—ì„œë‚˜ ë…¼ë¬¸ì˜ ë¦¬ë¹„ì „ì—ì„œë„ ì •ë§ ëë„ ì—†ì´ ë‚˜ì˜¤ëŠ” ê±¸ ë³´ë©´ ì•„ë§ˆë„ ë§ëŠ” ë§ì´ ì•„ë‹ê¹Œ ì‹¶ë‹¤.â†©ï¸\nì´ ê¸€ì„ ì“°ëŠ” ì‹œì ì—ì„œëŠ” ê·¸ë³´ë‹¤ ë” ë‚˜ì•„ê°€ ì •ë¶€íš¨ìœ¨ë¶€(Department of Government Efficiency; DOGE)ë¥¼ í†µí•œ ëŒ€ëŒ€ì ì¸ ê°œí˜ì— ë‚˜ì„œê³  ìˆëŠ” í˜•í¸ì´ë‹¤.â†©ï¸\nê³„ëŸ‰ê²½ì œí•™ì˜ ì˜ì›í•œ ì¹œêµ¬ ì‹œê³„ì—´ ë¶„ì„ì„ ì œëŒ€ë¡œ ê°€ë¥´ì¹˜ëŠ” í†µê³„í•™ í•™ìœ„ ê³¼ì •ì€ ë¯¸êµ­ì—ì„œ ë§ì´ ì‚¬ë¼ì§„ ê²ƒìœ¼ë¡œ ì•Œê³  ìˆë‹¤. ë¬¼ë¡ , ë‚˜ì²˜ëŸ¼ ì•¼ë§¤ë¡œ ê³µë¶€í•´ì„œ ê°€ë¥´ì¹˜ëŠ” ê²½ìš°ëŠ” ë§ì§€ë§Œ.â†©ï¸\n2015ë…„ ì„œìš¸ì‹œ ìë£Œë¥¼ ì‚´í´ë³´ì.â†©ï¸"
  },
  {
    "objectID": "Teaching/note01-spherical-distance.html",
    "href": "Teaching/note01-spherical-distance.html",
    "title": "Monte Carlo computation of \\(L_p\\) distance between two densities on the unit hypersphere",
    "section": "",
    "text": "A \\(d\\)-dimensional unit hypersphere \\(\\mathbb{S}^d = \\lbrace x \\in \\mathbb{R}^{d+1}~|~ \\|x\\|_2^2 = \\sum_{i=1}^{d+1} x_i^2 = 1\\rbrace\\) is one of the standard mathematical spaces in the field of objected-oriented data analysis (Marron and Dryden 2021). Let \\(\\mathcal{P}(\\mathbb{S}^d)\\) denote a space of probability densities on \\(\\mathbb{S}^d\\). For two densities \\(f,g\\in\\mathcal{P}(\\mathbb{S}^d)\\), it is frequently needed to measure dissimilarity between the two. Unfortunately, even for the most well-known distributions on the hypersphere, analytic formula for any discrepancy measure is rarely available, leading to require numerical schemes for approximation. Here we focus on \\(L_p\\) distance between the two densities, \\[\nL_p (f,g) = \\left( \\int_{\\mathbb{S}^d} |f(x) - g(x)|^p \\right)^{1/p}\n\\tag{1}\\] and we show how to combine Monte Carlo way of integration by means of importance sampling to approximate EquationÂ 1."
  },
  {
    "objectID": "Teaching/note01-spherical-distance.html#problem-statement",
    "href": "Teaching/note01-spherical-distance.html#problem-statement",
    "title": "Monte Carlo computation of \\(L_p\\) distance between two densities on the unit hypersphere",
    "section": "",
    "text": "A \\(d\\)-dimensional unit hypersphere \\(\\mathbb{S}^d = \\lbrace x \\in \\mathbb{R}^{d+1}~|~ \\|x\\|_2^2 = \\sum_{i=1}^{d+1} x_i^2 = 1\\rbrace\\) is one of the standard mathematical spaces in the field of objected-oriented data analysis (Marron and Dryden 2021). Let \\(\\mathcal{P}(\\mathbb{S}^d)\\) denote a space of probability densities on \\(\\mathbb{S}^d\\). For two densities \\(f,g\\in\\mathcal{P}(\\mathbb{S}^d)\\), it is frequently needed to measure dissimilarity between the two. Unfortunately, even for the most well-known distributions on the hypersphere, analytic formula for any discrepancy measure is rarely available, leading to require numerical schemes for approximation. Here we focus on \\(L_p\\) distance between the two densities, \\[\nL_p (f,g) = \\left( \\int_{\\mathbb{S}^d} |f(x) - g(x)|^p \\right)^{1/p}\n\\tag{1}\\] and we show how to combine Monte Carlo way of integration by means of importance sampling to approximate EquationÂ 1."
  },
  {
    "objectID": "Teaching/note01-spherical-distance.html#computation",
    "href": "Teaching/note01-spherical-distance.html#computation",
    "title": "Monte Carlo computation of \\(L_p\\) distance between two densities on the unit hypersphere",
    "section": "Computation",
    "text": "Computation\nImportance sampling requires a proposal density. The easiest choice is to use uniform density \\(u(x)\\) as an importance proposal since sampling from \\(u(x)\\) is trivial. First, take a random sample from standard normal distribution \\(x \\sim \\mathcal{N}(0,I)\\) in \\(\\mathbb{R}^{d+1}\\). Then, the rest is to take \\(L_2\\) normalization, i.e., \\(x \\leftarrow x / \\|x\\|_2\\), which makes a sampled vector to have a unit norm. Given the sample generation process, we have the following \\[\\begin{aligned}\nL_p (f,g)^p &= \\int_{\\mathbb{S}^d} |f(x)-g(x)|^p dx \\\\\n&= \\int_{\\mathbb{S}^d} \\frac{|f(x)-g(x)|^p}{u(x)} u(x)  dx \\\\\n&= \\mathbb{E}_{u(x)} \\left\\lbrack \\frac{|f(x)-g(x)|^p}{u(x)} \\right\\rbrack\\\\\n&\\approx \\frac{1}{N} \\sum_{n=1}^N \\frac{|f(x_n)-g(x_n)|^p}{u(x_n)} \\,\\,\\textrm{for}\\,\\, x_n \\overset{iid}{\\sim} u(x),\n\\end{aligned}\n\\] where the last term gets better approximation as \\(N\\rightarrow \\infty\\).\nHere the uniform density \\(u(x)\\) is an inverse of the surface area of the \\(d\\)-dimensional sphere \\(S_n\\), which is defined as \\[\nS_n = \\frac{2\\pi^{(n+1)/2}}{\\Gamma((n+1)/2)},\n\\] where \\(\\Gamma(x)\\) is the gamma function."
  },
  {
    "objectID": "Teaching/note01-spherical-distance.html#references",
    "href": "Teaching/note01-spherical-distance.html#references",
    "title": "Monte Carlo computation of \\(L_p\\) distance between two densities on the unit hypersphere",
    "section": "References",
    "text": "References\n\n\nMarron, James Stephen, and I. L. Dryden. 2021. Object Oriented Data Analysis. Boca Raton: Taylor & Francis Group, LLC."
  },
  {
    "objectID": "Blog/blog_003_DiracDense.html",
    "href": "Blog/blog_003_DiracDense.html",
    "title": "The space of Dirac measures is dense in Wasserstein space",
    "section": "",
    "text": "\\[\n\\newcommand{\\bbR}{\\mathbb{R}}\n\\newcommand{\\calD}{\\mathcal{D}}\n\\newcommand{\\calP}{\\mathcal{P}}\n\\DeclareMathOperator{\\argmin}{argmin}\n\\]"
  },
  {
    "objectID": "Blog/blog_003_DiracDense.html#introduction",
    "href": "Blog/blog_003_DiracDense.html#introduction",
    "title": "The space of Dirac measures is dense in Wasserstein space",
    "section": "1 Introduction",
    "text": "1 Introduction\nIn the study of probability distributions, the 2-Wasserstein space of measures, often denoted as \\(\\mathcal{P}_2(\\mathbb{R}^d)\\), has emerged as a foundational structure in optimal transport and statistical inference (Villani 2003). This space supports notion of distance, geometry, and convergence that are both theoretically robust and computationally relevant.\nA particularly deep yet accessible fact about this space is the following:\n\nThe set of all finitely supported probability measures (i.e., finite mixture of point masses) is dense under the 2-Wasserstein metric.\n\nThis statement has substantial consequences for both theory and practice. It legitimizes the use of discrete approximations such as empirical distributions generated by MCMC in high-level statistical tasks such as Bayesian inference, model aggregation, and distributional optimization. This note offers a comprehensive explanation of what this density means, how to prove it, and why it is consequential."
  },
  {
    "objectID": "Blog/blog_003_DiracDense.html#preliminaries-the-wasserstein-space",
    "href": "Blog/blog_003_DiracDense.html#preliminaries-the-wasserstein-space",
    "title": "The space of Dirac measures is dense in Wasserstein space",
    "section": "2 Preliminaries: The Wasserstein Space",
    "text": "2 Preliminaries: The Wasserstein Space\nLet \\(\\calP_2 (\\bbR^d)\\) denote the set of all Borel probability measures \\(\\mu\\) supported on \\(\\bbR^d\\) with finite second moments \\[\n\\int_{\\bbR^d} \\|x\\|^2 d\\mu(x) &lt; \\infty.\n\\] The 2-Wasserstein distance between two measures \\(\\mu, \\nu \\in \\calP_2(\\bbR^d)\\) is defined by \\[\\begin{equation}\nW_2^2 (\\mu,\\nu):= \\underset{\\gamma \\in \\Gamma(\\mu, \\nu)}{\\inf}~ \\int_{\\bbR^d \\times \\bbR^d} \\|x-y\\|^2 d\\gamma(x,y),\n\\end{equation}\\] where \\(\\Gamma(\\mu, \\nu)\\) is the set of all joint distributions, also known as couplings, with marginals \\(\\mu\\) and \\(\\nu\\). Intuitively, \\(W_2\\) measures the optimal cost of transporting one distribution into the other when cost is quadratic in displacement.\nThis space is not only a complete and separable metric space, but it also carries a Riemannian-like geometry that underpins interpolation, gradient flow, and barycenter constructions (Otto 2001)."
  },
  {
    "objectID": "Blog/blog_003_DiracDense.html#what-does-dense-mean-in-wasserstein-geometry",
    "href": "Blog/blog_003_DiracDense.html#what-does-dense-mean-in-wasserstein-geometry",
    "title": "The space of Dirac measures is dense in Wasserstein space",
    "section": "3 What Does â€œDenseâ€ Mean in Wasserstein Geometry?",
    "text": "3 What Does â€œDenseâ€ Mean in Wasserstein Geometry?\nIn general topology, a subset \\(A\\) of a metric space \\((X,d)\\) is called dense if for all \\(x \\in X\\) and any \\(\\epsilon &gt; 0\\), there exists \\(a \\in A\\) such that \\(d(x,a) &lt; \\epsilon\\). In our context, we consider the set \\[\n\\calD := \\left\\lbrace\n\\sum_{i=1}^n \\lambda_i \\delta_{x_i}\\mid \\lambda_i &gt; 0, ~\\sum_{i=1}^n \\lambda_i = 1,~x_i\\in\\bbR^d,~ n\\in \\mathbb{N}\n\\right\\rbrace,\n\\] which consists of finite convex combinations of Dirac measures.\nThe notion of density (not to be confused with the density of a measure) is understood in terms of the Wasserstein metric, not in terms of weak convergence or total variation. To understand the distinction, consider the following example. Think of the true distribution as a smooth landscape and discrete point masses as piles of sand. Wasserstein distance measures the work needed to move the sand to match the landscape, capturing both where the mass lies (location) and how much lies there (moment). Weak convergence is like viewing the scene from far away. You see whether piles are roughly in the correct places but ignore how tall they are. That means, moments may diverge. Total variation demands every grain be placed exactly right, which can be far too strict for sample-based approximation.\nPositioned between those extremes, \\(W_2\\) guarantees (1) weak convergence plus convergence of second moments, and (2) expectations for functions with quadratic growth. These make it an ideal notion for statistical applications where second-moment behavior is central. With this clarified, now letâ€™s prove that \\(\\calD\\) is dense in \\((\\calP_2, W_2)\\)."
  },
  {
    "objectID": "Blog/blog_003_DiracDense.html#proof",
    "href": "Blog/blog_003_DiracDense.html#proof",
    "title": "The space of Dirac measures is dense in Wasserstein space",
    "section": "4 Proof",
    "text": "4 Proof\nThe proof consists of three parts for easy understanding.\n\n4.1 Truncation to Compact Support\nWe want to show that dirac measures can approximate any measure \\(\\mu \\in \\calP_2(\\bbR^d)\\). For each \\(R &gt; 0\\), define the closed ball of radius \\(R\\) centered at the origin as \\[\nB(0,R):=\\lbrace x\\in\\bbR^d \\mid \\|x\\| \\leq R \\rbrace.\n\\] We construct a truncated version of \\(\\mu\\) supported on this ball. Define the measure \\(\\mu_R\\) by \\[\\begin{equation}\n\\mu_R(A) = \\frac{\\mu(A\\cap B(0,R))}{\\mu(B(0,R))},\n\\end{equation}\\] for all Borel sets \\(A \\subset \\bbR^d\\). This is a probability measure supported on \\(B(0,R)\\), provided \\(\\mu(B(0,R))&gt;0\\). Since \\(\\mu \\in \\calP_2\\), we have \\(\\mu(B(0,R)) \\to 1\\) as \\(R \\to \\infty\\).\nWe now verify that \\(W_2(\\mu_R, \\mu) \\to 0\\) as \\(R \\to \\infty\\). To do this, define a coupling \\(\\gamma_R \\in \\Gamma(\\mu,\\mu_R)\\) by setting \\[\n\\gamma_R := \\frac{1}{\\mu(B(0,R))}\\cdot \\mu\\vert_{B(0,R)}(x) \\otimes \\delta_x (y) \\in \\calP_2(\\bbR^d \\times \\bbR^d).\n\\] This coupling transports mass inside \\(B(0,R)\\) directly to itself while the mass outside the ball is ignored. Then, \\[\n\\int \\|x-y\\|^2 d \\gamma_R (x,y) = \\frac{1}{\\mu(B(0,R))} \\int_{B(0,R)} \\|x - x\\|^2 d\\mu(x) = 0.\n\\] Therefore, the transport cost under this coupling is zero inside the ball, but there is unmatched mass outside, which we account for.\nNow consider the decomposition \\[\nW_2^2(\\mu,\\mu_R) \\leq \\int_{\\bbR^d \\backslash B(0,R)} \\|x\\|^2 d\\mu(x) + (1-\\mu(B(0,R)))\\cdot R^2.\n\\] The first term represents contribution of tail mass and the second term is cost of reallocating mass. As \\(R\\to\\infty\\), the first term vanishes because of integrability of \\(\\|x\\|^2\\). We also know that the second term vanishes as well since \\(\\mu(B(0,R)) \\rightarrow 1\\). Therefore, \\[\n\\lim_{R\\to \\infty} W_2 (\\mu, \\mu_R)  = 0.\n\\]\n\n\n4.2 Quantization to Finitely Supported Measures\nFix \\(R&gt;0\\) and let \\(\\mu_R\\) be the compactly supported measure we just constructed. Since \\(\\mu_R\\) is supported on a compact set \\(K = B(0,R)\\), we can apply a canonical quantization argument. Partition the set \\(K\\) into a finite number of disjoint Borel sets \\(Q_1, \\ldots, Q_k\\) such that:\nFor each \\(i=1,\\ldots,k\\), choose a representative point \\(x_i \\in Q_i\\) and define the finitely supported measure \\[\n\\mu_R^\\epsilon := \\sum_{i=1}^k \\mu_R (Q_i) \\delta_{x_i} \\in \\calD.\n\\] We can construct a coupling \\(\\gamma \\in \\Gamma(\\mu_R, \\mu_R^\\epsilon)\\) by pushing the entire mass from \\(Q_i\\) to \\(x_i\\). Then, \\[\nW_2^2(\\mu_R, \\mu_R^\\epsilon) \\leq \\sum_{i=1}^k \\int_{Q_i}  \\|x - x_i\\|^2 d\\mu_R(x) \\leq \\epsilon^2.\n\\] Hence, \\(W_2 (\\mu_R, \\mu_R^\\epsilon) \\leq \\epsilon\\).\n\n\n4.3 Convergence via Triangle Inequality\nNow fix an arbitrary \\(\\delta &gt; 0\\). From the first step, one can choose \\(R\\) large enough so that \\[\nW_2(\\mu,\\mu_R) &lt; \\frac{\\delta}{2}.\n\\] From the second step, \\(\\epsilon\\) can be chosen small enough so that \\[\nW_2 (\\mu_R, \\mu_R^\\epsilon) &lt; \\frac{\\delta}{2}.\n\\] By the triangle inequality, we arrive at the following result: \\[\nW_2(\\mu,\\mu_R^\\epsilon) \\leq W_2(\\mu,\\mu_R) + W_2 (\\mu_R, \\mu_R^\\epsilon) &lt; \\frac{\\delta}{2}+\\frac{\\delta}{2} = \\delta.\n\\] Since \\(\\mu_R^\\epsilon \\in \\calD\\), we have shown that for any \\(\\mu \\in \\calP_2(\\bbR^d)\\) and any \\(\\delta &gt; 0\\), there exists a finitely supported measure \\(\\nu \\in \\calD\\) such that \\(W_2(\\mu, \\nu) &lt; \\delta\\).\nThus, the set \\(\\calD\\) is dense in \\((\\calP_2(\\bbR^d), W_2)\\)."
  },
  {
    "objectID": "Blog/blog_003_DiracDense.html#why-this-matters-in-bayesian-inference",
    "href": "Blog/blog_003_DiracDense.html#why-this-matters-in-bayesian-inference",
    "title": "The space of Dirac measures is dense in Wasserstein space",
    "section": "5 Why This Matters in Bayesian Inference",
    "text": "5 Why This Matters in Bayesian Inference\nThe density of Dirac mixtures in Wasserstein space is not just a geometric curiosity. Indeed, it plays a foundational role in the theory and computation of Bayesian inference.\n\n5.1 Empirical Posteriors Are Dirac Mixtures\nIn Bayesian analysis, the posterior distribution \\(\\pi(\\theta\\mid x)\\) encodes all inferential information about the parameter \\(\\theta\\) given the data \\(x\\). In most real-world problems, \\(\\pi(\\theta\\mid x)\\) is analytically intractable. As a result, we approximate it using samples from methods like Markov chain Monte Carlo (MCMC), producing a sequence \\(\\theta_1, \\ldots, \\theta_N \\sim \\pi(\\theta\\mid x)\\).\nOnce a run is done, this gives rise to the empirical posterior distribution \\[\n\\hat{\\pi}_N = \\frac{1}{N}\\sum_{i=1}^N \\delta_{\\theta_i} \\in \\calD,\n\\] which is a discrete measure (a Dirac mixture) and lies in the dense subset \\(\\calD \\subset \\calP_2 (\\bbR^d)\\). Thanks to the density result, under very mild regularity conditions, i.e., \\(\\pi(\\theta\\mid x) \\in \\calP_2(\\bbR^d)\\), we know: \\[\nW_2(\\hat{\\pi}_N, \\pi) \\longrightarrow 0\\quad\\text{as }N\\to \\infty.\n\\]\nThis implies that the convergence of the empirical posterior \\(\\hat{\\pi}_N\\) to the true posterior \\(\\pi(\\theta\\mid x)\\) is not just in distribution but also in terms of second moments, which is crucial for many statistical applications1.\n\n\n5.2 Why This Matters Practically\nThis convergence has powerful consequences.\n\nConsistency of posterior expectations: For many function classes such as Lipschitz or quadratic-growth functions, expectations under \\(\\hat{\\pi}_N\\) converge to expectations under \\(\\pi\\). This ensures reliable estimation of posterior summaries including means, variances, and credible intervals.\nStability of statistical functionals: Quantities like credible regions, predictive distributions, Bayes factors, and risk functionals are well-approximated by their empirical analogs.\nFoundation for resampling and ensembling: The theoretical legitimacy of sampling-based posterior approximations ensures that tools like posterior bootstrap, weighted resampling, and kernel density estimation on \\(\\hat{\\pi}_N\\) can be interpreted rigorously."
  },
  {
    "objectID": "Blog/blog_003_DiracDense.html#broader-implications",
    "href": "Blog/blog_003_DiracDense.html#broader-implications",
    "title": "The space of Dirac measures is dense in Wasserstein space",
    "section": "6 Broader Implications",
    "text": "6 Broader Implications\nThe density of finitely supported measures in Wasserstein space has far-reaching implications that extend well beyond Bayesian computation. At its core, this result gives us permission to treat discrete, empirical approximations as valid proxies for continuous distributions not only heuristically, but with full mathematical justification. This has become increasingly important as modern statistics moves into spaces where distributions are themselves the objects of inference, optimization, or learning.\nFor instance, a wide range of statistical and machine learning tasks now involve minimizing functionals over probability distributions. Examples include optimal transport-based clustering, distributional regression, and geometric learning in Wasserstein space. These problems are naturally infinite-dimensional, yet thanks to the density of point masses, we can safely approximate them using discrete measures. Algorithms that work with samples or particles, such as those in variational inference, particle filtering, or Wasserstein generative modeling, operate entirely within this discrete subset, and the density result assures us that they remain close, in a precise metric sense, to the true solution.\nMoreover, this result forms the basis for understanding the behavior of empirical distributions in a quantitative way. In high-dimensional settings, we care not just about whether empirical averages converge, but about how distributions themselves behave under finite sampling. Rate of convergence results like Fournier and Guillin (2015), which provide non-asymptotic convergence rates for empirical measures in Wasserstein distance, rely on the fact that empirical distributions belong to a dense subset of the space. Without such a density property, these convergence rates would lack grounding.\nIt also enables the practical use of geometric concepts such as Wasserstein barycenters, FrÃ©chet means, and transport-based medians. These objects, while defined over general distributions, are often computed from discrete empirical measures. That this is not just a computational shortcut but a theoretically valid procedure is precisely because of this density: the geometric structure of Wasserstein space remains stable under approximation by point masses.\nPerhaps most importantly, the result builds a conceptual bridge between infinite-dimensional theory and finite-sample practice. In probability, statistics, and machine learning, we often formulate problems in terms of idealized objects - true distributions, functional risk, geometric flows - but ultimately implement solutions with finite data. The density of Dirac mixtures in Wasserstein space assures us that this transition from the infinite to the finite is not a compromise, but a convergence."
  },
  {
    "objectID": "Blog/blog_003_DiracDense.html#conclusion",
    "href": "Blog/blog_003_DiracDense.html#conclusion",
    "title": "The space of Dirac measures is dense in Wasserstein space",
    "section": "7 Conclusion",
    "text": "7 Conclusion\nThe fact that Dirac mixtures are dense in the Wasserstein space \\(\\calP_2(\\bbR^d)\\) is more than a technical result. It is a cornerstone of modern computational probability. It tells us that empirical approximations, such as those arising from MCMC, live in the same geometric space as the distributions they seek to estimate. It validates our use of discrete measures in place of continuous ones, not only for computing expectations or variances, but for solving full distributional problems.\nThis result is what allows us to define geometric estimators on sampled data, aggregate distributions across models or machines, and design optimization algorithms that operate directly on measures. It is what ensures that Wasserstein gradient flows, barycenters, and interpolation paths remain meaningful when applied to empirical distributions. It is what gives us the confidence to work with particles, while knowing we are still faithfully navigating the geometry of probability."
  },
  {
    "objectID": "Blog/blog_003_DiracDense.html#references",
    "href": "Blog/blog_003_DiracDense.html#references",
    "title": "The space of Dirac measures is dense in Wasserstein space",
    "section": "References",
    "text": "References\n\n\nFournier, Nicolas, and Arnaud Guillin. 2015. â€œOn the Rate of Convergence in Wasserstein Distance of the Empirical Measure.â€ Probability Theory and Related Fields 162 (3-4): 707â€“38. https://doi.org/10.1007/s00440-014-0583-7.\n\n\nOtto, Felix. 2001. â€œThe Geometry of Dissipative Evolution Equations: The Porous Medium Equation.â€ Communications in Partial Differential Equations 26 (1-2): 101â€“74. https://doi.org/10.1081/PDE-100002243.\n\n\nVillani, CÃ©dric. 2003. Topics in Optimal Transportation. Vol. 58. Graduate Studies in Mathematics. S.l.: American Mathematical Society."
  },
  {
    "objectID": "Blog/blog_003_DiracDense.html#footnotes",
    "href": "Blog/blog_003_DiracDense.html#footnotes",
    "title": "The space of Dirac measures is dense in Wasserstein space",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLetâ€™s be honest, skewness and kurtosis, really?â†©ï¸"
  },
  {
    "objectID": "Blog/blog_002_ACG.html",
    "href": "Blog/blog_002_ACG.html",
    "title": "What is the Angular Central Gaussian distribution?",
    "section": "",
    "text": "Probability distribution with explicit forms of densities are core elements of statistical inference. In this post, we review angular central gaussian (ACG) distribution on a unit hypersphere \\(\\mathbb{S}^{p-1} \\subset \\mathbb{R}^p\\) and its extension - matrix angular central gaussian (MACG) - defined on Stiefel \\(St(p,r)\\) and Grassman \\(Gr(p,r)\\) manifolds."
  },
  {
    "objectID": "Blog/blog_002_ACG.html#introduction",
    "href": "Blog/blog_002_ACG.html#introduction",
    "title": "What is the Angular Central Gaussian distribution?",
    "section": "",
    "text": "Probability distribution with explicit forms of densities are core elements of statistical inference. In this post, we review angular central gaussian (ACG) distribution on a unit hypersphere \\(\\mathbb{S}^{p-1} \\subset \\mathbb{R}^p\\) and its extension - matrix angular central gaussian (MACG) - defined on Stiefel \\(St(p,r)\\) and Grassman \\(Gr(p,r)\\) manifolds."
  },
  {
    "objectID": "Blog/blog_002_ACG.html#angular-central-gaussian-distribution",
    "href": "Blog/blog_002_ACG.html#angular-central-gaussian-distribution",
    "title": "What is the Angular Central Gaussian distribution?",
    "section": "Angular Central Gaussian Distribution",
    "text": "Angular Central Gaussian Distribution\nOn \\(\\mathbb{S}^{p-1}\\), the ACG distribution \\(ACG_p (A)\\) as a density \\[\nf_{ACG} (x\\vert A) = |A|^{-1/2} (x^\\top Ax)^{-p/2}\n\\] for \\(x \\in \\mathbb{S}^{p-1}\\) and \\(A\\) a symmetric positive-definite matrix, i.e., \\(A=A^\\top \\in \\mathbb{R}^{p\\times p}\\) with \\(\\lambda_{min}(A)&gt;0\\). Letâ€™s recap some properties of ACG distribution.\n\nProperty 1. \\(f_{ACG}(x|A) = f_{ACG}(-x|A)\\). This enables ACG as a distribution on the real projective space \\(\\mathbb{R}P^{p-1} = \\mathbb{S}^{p-1}/\\lbrace +1, -1 \\rbrace\\).\nProperty 2. \\(f_{ACG}(x|A) = f_{ACG}(x|cA),~c&gt;0\\). Common convention is to normalize the matrix \\(A\\) by a constraint \\(\\textrm{tr}(A) = p\\), which is useful (or even essential) in maximum likelihood estimation of the parameter to ensure algorithmic stability. If you want to show this property, simply use the fact that \\(|cA| = c^p|A|\\).\nProperty 3. When \\(x\\sim \\mathcal{N}_p (0,A) \\rightarrow x/\\|x\\| \\sim ACG_p (A)\\). This property is indeed an intuition behind its origination (Tyler 1987), which can be used for sampling.\n\n\nMaximum Likelihood Estimation\nGiven a random sample \\(x_1, \\ldots, x_p \\sim ACG_p (A)\\), Tyler (1987) proposed an iterative updating scheme to estimate the parameter \\(A\\) by\n\n\\[\\hat{A}_{k+1} = p \\left\\lbrace \\sum_{i=1}^n \\frac{1}{x_i^\\top \\hat{A}_k^{-1} x_i} \\right\\rbrace^{-1} \\sum_{i=1}^n \\frac{x_i x_i^\\top}{x_i^\\top \\hat{A}_k^{-1} x_i}, \\tag{1}\\]\n\nwhere \\(\\hat{A}_k\\) is the \\(k\\)-th iterate of an estimator with an initial starting point of an identity matrix \\(\\hat{A}_0 = I_p\\). While EquationÂ 1 guarantees the convergence under mild conditions and abides by the constraint \\(\\textrm{tr}(\\hat{A}_k) = p\\), it is from the authorâ€™s previous work on \\(M\\)-estimation of the scatter matrix. Here, we provide a naive derivation of 2-step fixed-point iteration algorithm for pedagogical purpose.\n\n\\[\\hat{A}_{k'} = \\frac{p}{n}\\sum_{i=1}^n \\frac{x_i x_i^\\top}{x_i^\\top \\hat{A}_k^{-1} x_i}\\,\\,\\textrm{and}\\,\\, \\hat{A}_{k+1} = \\frac{p}{\\textrm{tr}(\\hat{A}_{k'})} \\hat{A}_{k'}. \\tag{2}\\]\n\nFirst, letâ€™s write the log-likelihood \\[\n\\log L = -\\frac{n}{2}\\log\\det(A) - \\frac{p}{2} \\sum_{i=1}^n \\log (x_i^\\top A^{-1} x_i),\n\\] and recall two facts from matrix calculus (Petersen and Pedersen 2012/nov) that \\[\n\\frac{\\partial \\log\\det(A)}{\\partial A} = A^{-1}\\,\\,\\textrm{and}\\,\\, \\frac{\\partial x^\\top A^{-1} x}{\\partial A} = -A^{-1}xx^\\top A^{-1}.\n\\] Then, the first-order condition for the log-likelihood can be written as\n\\[\n\\begin{gather*}\n    \\frac{\\partial \\log L}{\\partial A} = -\\frac{n}{2} A^{-1} + \\frac{p}{2} \\sum_{i=1}^n \\frac{A^{-1} x_i x_i^\\top A^{-1}}{x_i^\\top A^{-1} x_i} \\\\\n    A^{-1} = \\frac{p}{n} \\sum_{i=1}^n \\frac{A^{-1} x_i x_i^\\top A^{-1}}{x_i^\\top A^{-1} x_i} \\\\\n    A = \\frac{p}{n} \\sum_{i=1}^n \\frac{ x_i x_i^\\top }{x_i^\\top A^{-1} x_i}\n\\end{gather*}\n\\] where the last equality comes from multiplying \\(A\\) from left and right. Therefore, \\(\\hat{A}\\) is a solution of the matrix equation in a form \\(X = f(X)\\) where \\(f\\) is a contraction mapping under some conditions (Tyler 1987). This leads to EquationÂ 2 while projection step is added to keep \\(\\text{tr}(\\hat{A}_k) = p\\) for all \\(k=1,2,\\cdots\\)."
  },
  {
    "objectID": "Blog/blog_002_ACG.html#matrix-angular-central-gaussian-distribution",
    "href": "Blog/blog_002_ACG.html#matrix-angular-central-gaussian-distribution",
    "title": "What is the Angular Central Gaussian distribution?",
    "section": "Matrix Angular Central Gaussian Distribution",
    "text": "Matrix Angular Central Gaussian Distribution\nChikuse (2003) extended the distribution to the matrix case, namely Stiefel and Grassmann manifolds \\[\n\\begin{gather*}\n    St(p,r) = \\{X\\in \\mathbb{R}^{p\\times r} ~\\vert~ X^\\top X = I_p\\}\\\\\n    Gr(p,r) = \\{\\text{Span}(X) ~\\vert~ X \\in \\mathbb{R}^{p\\times r},~\\text{rank}(X)=r\\}\n\\end{gather*}\n\\] which are sets of orthonormal \\(k\\)-frames and \\(k\\)-subspaces. The Matrix Angular Central Gaussian (MACG) distribution \\(MACG_{p,r}(\\Sigma)\\) has a density \\[\nf_{MACG}(X\\vert \\Sigma) = |\\Sigma|^{-r/2} |X^\\top \\Sigma^{-1} X|^{-p/2}\n\\] where \\(\\Sigma\\) is a symmetric positive-definite matrix. Note that the density is very similar to what we had before for vector-valued distribution. Likewise, it shares properties as before.\n\nProperty 1. \\(f_{MACG}(X|\\Sigma) = f_{MACG}(-X|\\Sigma)\\).\nProperty 2. \\(f_{MACG}(X|\\Sigma) = f_{MACG}(X|c\\Sigma),~c&gt;0\\).\nProperty 3. \\(f_{MACG}(X|\\Sigma) = f_{MACG}(XR|\\Sigma)\\) for \\(R\\in O(r)\\). This property enables to consider MACG as a distribution on Grassmann manifold, which are quotient by modulo orthogonal transformation.\n\n\nSampling from MACG\nIn order to draw random samples from \\(MACG_{p,r}(\\Sigma)\\), we need the following steps, which are common in directional statistics with Stiefel/Grassmann manifolds . First, draw \\(r\\) random vectors \\(x_1,\\ldots,x_r \\sim \\mathcal{N}_p (0,\\Sigma)\\) and stack them as columns \\(X=[x_1|\\cdots|x_r] \\in \\mathbb{R}^{p\\times r}\\). Then, \\[\n    Y = X (X^\\top X)^{-1/2} \\sim MACG_{p,r}(\\Sigma)\n\\] where the negative square root for a symmetric positive-definite matrix can be obtained from eigen-decomposition, \\[\n\\begin{gather*}\n\\Omega = UDU^\\top \\rightarrow \\Omega^{-1/2} = UD^{-1/2} U^\\top \\\\\n\\left[D^{-1/2}\\right]_{ij} = \\frac{1}{\\sqrt{d_{ij}}} \\textrm{ when } i = j \\textrm{ and }0\\textrm{ otherwise.}\n\\end{gather*}\n\\]\n\n\nMaximum Likelihood Estimation\nSimilar to the ACG case, given a random sample \\(X_1,X_2,\\ldots,X_n \\sim MACG_{p,r}(\\Sigma)\\), we can obtain a two-step iterative scheme to estimate the parameter \\(\\Sigma\\),\n\n\\[\\begin{gather*}\n\\hat{\\Sigma}_{k'} = \\frac{p}{nr} \\sum_{i=1}^n\nX_i (X_i^\\top \\Sigma^{-1} X_i)^{-1} X_i \\\\\n\\hat{\\Sigma}_{k+1} =  \\frac{p}{\\text{tr}(\\hat{\\Sigma}_{k'})} \\hat{\\Sigma}_{k'}.\\end{gather*} \\tag{3}\\]\n\nDerivation of formula EquationÂ 3 follows the similar line of before. We need another fact from matrix calculus that \\[\n\\frac{\\partial }{\\partial \\Sigma} \\log\\det(X^\\top \\Sigma^{-1} X) = - \\Sigma^{-1} X (X^\\top \\Sigma^{-1} X)^{-1} X^\\top \\Sigma^{-1}.\n\\] First, log-likelihood is written as \\[\n    \\log L  = -\\frac{nr}{2}\\log\\det(\\Sigma) - \\frac{p}{2}\\sum_{i=1}^n \\log\\det (X_i^\\top \\Sigma^{-1} X_i)\n\\] where the first-order condition gives \\[\n\\begin{gather*}\n    \\frac{\\partial \\log L}{\\partial \\Sigma} = -\\frac{nr}{2}\\Sigma^{-1} + \\frac{p}{2}\\sum_{i=1}^n \\left( \\Sigma^{-1} X_i (X_i^\\top \\Sigma^{-1} X_i)^{-1} X_i^\\top \\Sigma^{-1} \\right)\\\\\n    \\frac{nr}{2} \\Sigma^{-1} = \\frac{p}{2}\\sum_{i=1}^n \\left( \\Sigma^{-1} X_i (X_i^\\top \\Sigma^{-1} X_i)^{-1} X_i^\\top \\Sigma^{-1} \\right) \\\\\n    \\Sigma = \\frac{p}{nr} \\sum_{i=1}^n X_i (X_i^\\top \\Sigma^{-1} X_i)^{-1} X_i^\\top\n\\end{gather*}\n\\] where the last equality comes from multiplying \\(\\Sigma\\) from left and right. Therefore, \\(\\hat{\\Sigma}\\) is a solution of the matrix equation, leading to the formula of EquationÂ 3 with an additional projection step to keep \\(\\text{tr}(\\hat{\\Sigma}_k) = p\\) for all \\(k=1,2,\\cdots\\). Note that this matrix equation, up to my knowledge, has not known whether the mapping is contraction or not."
  },
  {
    "objectID": "Blog/blog_002_ACG.html#conclusion",
    "href": "Blog/blog_002_ACG.html#conclusion",
    "title": "What is the Angular Central Gaussian distribution?",
    "section": "Conclusion",
    "text": "Conclusion\nACG and MACG distributions are simple yet rather little used in directional statistics. We hope that this brief note boosts probabilistic inference on corresponding manifolds at ease. An R package Riemann, which is also available on CRAN, implements density evaluation, random sample generation, and maximum likelihood estimation of the scatter parameters \\(A\\) and \\(\\Sigma\\) in the light of expecting handy utilization of the distributions we introduced."
  },
  {
    "objectID": "Blog/blog_002_ACG.html#references",
    "href": "Blog/blog_002_ACG.html#references",
    "title": "What is the Angular Central Gaussian distribution?",
    "section": "References",
    "text": "References\n\n\nChikuse, Yasuko. 2003. Statistics on Special Manifolds. Lecture Notes in Statistics 174. New York: Springer.\n\n\nPetersen, K. B., and M. S. Pedersen. 2012/nov. The Matrix Cookbook. Version 20121115. Technical Univeresity of Denmark.\n\n\nTyler, David E. 1987. â€œStatistical Analysis for the Angular Central Gaussian Distribution on the Sphere.â€ Biometrika 74 (3): 579â€“89. https://doi.org/10.1093/biomet/74.3.579."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Kisung You",
    "section": "",
    "text": "Welcome!\nI am an assistant professor in the Department of Mathematics at Baruch College, City University of New York. My research integrates tools from statistics, differential geometry, and computational mathematics to develop new methods for complex scientific problems, with a strong focus on biomedical data analysis. I am also passionate about high-performance computing and statistical software development, aiming to make advanced methods practically accessible across disciplines.\n\nğŸ“° News\n\n\n\n\n\n2025/08/19\n\n\nNew article: Usability and adoption in a randomized trial of GutGPT a GenAI tool for gastrointestinal bleeding is published in npj Digital Medicine.\n\n\n\n\n2025/07/04\n\n\nNew article: Scalable geometric learning with correlation-based functional brain networks is published in Scientific Reports.\n\n\n\n\n2025/05/03\n\n\nNew article: Expert of Experts Verification and Alignment (EVAL) is published in npj Digital Medicine.\n\n\n\n\n\n\n\nğŸ“ You can find me around NYC:\n\n\n\n\n Last updated on November 09, 2025"
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "Teaching\nCourses at Baruch College\n\n\n\nCourse\nSemester\n\n\n\n\nMTH 4430 Mathematics of Inferential Statistics\n23F, 24F\n\n\nMTH 4130 Mathematics of Data Analysis\n24F\n\n\nMTH 4125 Introduction to Stochastic Processes\n24S\n\n\nMTH 2003 Precalculus and Elements of Calculus 1A\n23F\n\n\n\n\nMiscellaneous Notes\nI maintain a collection of concise technical notes to supplement my classes.\n\n(2022/08/10) Rodriguesâ€™ formula for the Legendre polynomials.\n(2022/08/09) Monte Carlo computation of \\(L_p\\) distance between two densities on the unit hypersphere."
  },
  {
    "objectID": "publication.html",
    "href": "publication.html",
    "title": "Publications",
    "section": "",
    "text": "The list below includes my published articles (count: 25) and preprints manuscripts (count: 7) in reverse chronological order as of November 10, 2025."
  },
  {
    "objectID": "publication.html#published",
    "href": "publication.html#published",
    "title": "Publications",
    "section": "Published",
    "text": "Published\n\n\n2025\nUsability and adoption in a randomized trial of GutGPT a GenAI tool for gastrointestinal bleeding Sunny Chung, Mauro GiuffrÃ¨, Niroop Rajashekar, Yuan Pu, Yeo Eun Shin, Simone Kresevic, Colleen Chan, Shinpei Nakamura-Sakai, KY, Theo Saarinen, Allen Hsiao, Ambrose Wong, Leigh Evans, Terika McCall, Rene Kizilcec, Jasjeet Sekhon, Loren Laine, and Dennis Shung.  npj Digital Medicine.\n        \n        Publisher's site\n    \nScalable geometric learning with correlation-based functional brain networks KY, Yelim Lee, and Hae-Jeong Park.  Scientific Reports.\n        \n        Preprint\n     \n        \n        Code\n     \n        \n        Publisher's site\n    \nExpert of Experts Verification and Alignment (EVAL) Framework for Large Language Models Safety in Gastroenterology Mauro GiuffrÃ¨, KY, Ziteng Pang, Simone Kresevic, Sunny Chung, Ryan Chen, Youngmin Ko, Colleen Chan, Theo Saarinen, Milos Ajcevic, Lory CrocÃ¨, Guadalupe Garsia-Tsao, Ian Gralnek, Joseph Sung, Alan Barkun, Loren Laine, Jasjeet Sekhon, Bradly Stadie, and Dennis Shung.  npj Digital Medicine.\n        \n        Publisher's site\n    \nOn the Wasserstein median of probability measures KY, Dennis Shung, and Mauro GiuffrÃ¨.  Journal of Computational and Graphical Statistics.\n        \n        Preprint\n     \n        \n        Publisher's site\n    \n\n\n2024\nValidation of an Electronic Health Recordâ€“Based Machine Learning Model Compared With Clinical Risk Scores for Gastrointestinal Bleeding Dennis Shung, Colleen Chan, KY, Shinpei Nakamura, Theo Saarinen, Neil Zheng, Michael Simonov, Darrick Li, Cynthia Tsay, Yuki Kawamura, Matthew Shen, Allen Hsiao, Jasjeet Sekhon, and Loren Laine.  Gastroenterology.\n        \n        Publisher's site\n    \nOptimizing large language models in digestive disease: strategies and challenges to improve clinical outcomes Mauro GiuffrÃ¨, Simone Kresevic, Nicola Pugliese, KY, and Dennis Shung.  Liver International.\n        \n        Publisher's site\n    \nPredicting response to non-selective beta-blockers with liverâ€“spleen stiffness and heart rate in patients with liver cirrhosis and high-risk varices Mauro GiuffrÃ¨, Johannes Dupont, Alessia Visintin, Flora Masutti, Fabio Monica, KY, Dennis Shung, Lory CrocÃ¨, and The NSBB-Elasto-Response-Prediction Group.  Hepatology International.\n        \n        Publisher's site\n    \nDetection of Gastrointestinal Bleeding with Large Language Models to Aid Quality Improvement and Appropriate Reimbursement Neil Zheng, Vipina Keloth, KY, Daniel Kats, Darrick Li, Ohm Deshpande, Hamita Sachar, Hua Xu, Loren Laine, and Dennis Shung.  Gastroenterology.\n        \n        Publisher's site\n    \nAdoption of a gastroenterology hospitalist model and the impact on inpatient endoscopic practice volume: a controlled interrupted time-series analysis Dennis Shung, Darrick Li, KY, Kenneth Hung, Loren Laine, and Michelle Hughes.  iGIE.\n        \n        Publisher's site\n    \nBayesian Optimal Two-sample Tests for High-dimensional Gaussian Populations Kyoungjae Lee, KY, and Lizhen Lin.  Bayesian Analysis.\n        \n        Preprint\n     \n        \n        Publisher's site\n    \nSystematic review: The use of large language models as medical chatbots in digestive diseases Mauro GiuffrÃ¨, Simone Kresevic, KY, Johannes Dupont, Jack Huebner, Alyssa Grimshaw, and Dennis Shung.  Alimentary Pharmacology & Therapeutics.\n        \n        Publisher's site\n    \nHuman-Algorithmic Interaction Using a Large Language Model-Augmented Artificial Intelligence Clinical Decision Support System Niroop Rajashekar, Yeo Eun Shin, Yuan Pu, Sunny Chung, KY, Mauro GiuffrÃ¨, Colleen Chan, Theo Saarinen, Allen Hsiao, Jasjeet Sekhon, Ambrose Wong, Leigh Evans, Rene Kizilcec, Loren Laine, Terika McCall, and Dennis Shung.  CHI Conference on Human Factors in Computing Systems.\n        \n        Publisher's site\n    \n\n\n2023\nAssessing the Usability of GutGPT: A Simulation Study of an AI Clinical Decision Support System for Gastrointestinal Bleeding Risk Colleen Chan, KY, Sunny Chung, Mauro GiuffrÃ¨, Theo Saarinen, Niroop Rajashekar, Yuan Pu, Yeo Eun Shin, Loren Laine, Ambrose Wong, Rene Kizilcec, Jasjeet Sekhon, and Dennis Shung.  Machine Learning for Health (ML4H) Symposium.\n        \n        Preprint\n    \nEvaluating ChatGPT in Medical Contexts: The Imperative to Guard Against Hallucinations and Partial Accuracies Mauro GiuffrÃ¨, KY, and Dennis Shung.  Clinical Gastroenterology and Hepatology.\n        \n        Publisher's site\n    \nSingle-cell analysis reveals inflammatory interactions driving macular degeneration Manik Kuchroo, Marcello DiStasio, Eric Song, Eda Calapkulu, Le Zhang, Maryam Ige, Amar Sheth, Abdelilah Majdoubi, Madhvi Menon, Alexander Tong, Abhinav Godavarthi, Yu Xing, Scott Gigante, Holly Steach, Jessie Huang, Guillaume Huguet, Janhavi Narain, KY, George Mourgkos, Rahul Dhodapkar, Matthew Hirn, Bastian Rieck, Guy Wolf, Smita Krishnaswamy, and Brian Hafler.  Nature Communications.\n        \n        Publisher's site\n    \nOn the Spherical Laplace Distribution KY and Dennis Shung.  International Conference on Information Fusion (FUSION).\n        \n        Preprint\n     \n        \n        Publisher's site\n    \n\n\n2022\nGeometric learning of functional brain network on the correlation manifold KY and Hae-Jeong Park.  Scientific Reports.\n        \n        Code\n     \n        \n        Publisher's site\n    \n Learning Subspaces of Different Dimensions Brian St. Thomas, KY, Lizhen Lin, Lek-Heng Lim, and Sayan Mukherjee.  Journal of Computational and Graphical Statistics.\n        \n        Preprint\n     \n        \n        Code\n     \n        \n        Publisher's site\n    \nRdimtools: An R package for Dimension Reduction and Intrinsic Dimension Estimation KY and Dennis Shung.  Software Impacts.\n        \n        Preprint\n     \n        \n        Code\n     \n        \n        Publisher's site\n    \nParameter estimation and model-based clustering with spherical normal distribution on the unit hypersphere KY and Changhee Suh.  Computational Statistics and Data Analysis.\n        \n        Preprint\n     \n        \n        Code\n     \n        \n        Publisher's site\n    \nNetwork Distance Based on Laplacian Flows on Graphs Dianbin Bao, KY, and Lizhen Lin.  IEEE International Conference on Big Data (Big Data).\n        \n        Preprint\n     \n        \n        Code\n     \n        \n        Publisher's site\n    \n\n\n2021\nRe-visiting Riemannian geometry of symmetric positive definite matrices for the analysis of functional connectivity KY and Hae-Jeong Park.  NeuroImage.\n        \n        Code\n     \n        \n        Publisher's site\n    \n\n\n2020\nData transforming augmentation for heteroscedastic models Hyungsuk Tak, KY, Sujit K. Ghosh, Bingyue Su, and Joseph Kelly.  Journal of Computational and Graphical Statistics.\n        \n        Preprint\n     \n        \n        Publisher's site\n    \n\n\n2019\nVolume change pattern of decompression of mandibular odontogenic keratocyst Jin Hoo Park, Eun-Jung Kwak, KY, Young-Soo Jung, and Hwi-Dong Jung.  Maxillofacial Plastic and Reconstructive Surgery.\n        \n        Publisher's site\n    \n\n\n2016\nVision-based detection of loosened bolts using the Hough transform and support vector machines Young-Jin Cha, KY, and Wooram Choi.  Automation in Construction.\n        \n        Publisher's site"
  },
  {
    "objectID": "publication.html#preprints",
    "href": "publication.html#preprints",
    "title": "Publications",
    "section": "Preprints",
    "text": "Preprints\n\nA Particle-Flow Algorithm for Free-Support Wasserstein Barycenters KY (2025).\n        \n        Preprint\n    \nLearning over von Mises-Fisher Distributions via a Wasserstein-like Geometry KY, Dennis Shung, and Mauro GiuffrÃ¨ (2025).\n        \n        Preprint\n    \nSemantics at an Angle: When Cosine Similarity Works Until It Doesn't KY (2025).\n        \n        Preprint\n    \nGeometric medians on product manifolds KY and Jiewon Park (2025).\n        \n        Preprint\n    \nPCA, SVD, and Centering of Data Donggun Kim and KY (2023).\n        \n        Preprint\n    \nComparing multiple latent space embeddings using topological analysis KY, Ilmun Kim, Ick Hoon Jin, Minjeong Jeon, and Dennis Shung (2022).\n        \n        Preprint\n     \n        \n        Code\n    \nShape-Preserving Dimensionality Reduction : An Algorithm and Measures of Topological Equivalence Byeongsu Yu and KY (2021).\n        \n        Preprint"
  },
  {
    "objectID": "Blog/blog_004_GradientSquaredDistance.html",
    "href": "Blog/blog_004_GradientSquaredDistance.html",
    "title": "Gradient of squared distance on a Riemannian manifold",
    "section": "",
    "text": "\\[\n\\newcommand{\\bbR}{\\mathbb{R}}\n\\newcommand{\\calD}{\\mathcal{D}}\n\\newcommand{\\calM}{\\mathcal{M}}\n\\newcommand{\\calP}{\\mathcal{P}}\n\\DeclareMathOperator{\\argmin}{argmin}\n\\]\nOne of the most fundamental objects in geometric statistics and computation thereof is the squared Riemannian distance function \\(d^2(p,q)\\) defined on a Riemannian manifold \\((\\calM, g)\\). This function is central to optimization, variational formulations, and the definition of FrÃ©chet means on manifolds. A key identity often invoked but not always derived is: \\[\n\\nabla_x d^2(x,y) = -2\\log_x y,\n\\] where \\(\\log_x y\\) is the logarithmic map, which pulls a point \\(y\\) to the tangent space at \\(x\\)1. This is the manifold generalization of the Euclidean identity \\(\\nabla_x \\|x-y\\|^2 = -2(y-x)\\), where \\(\\|\\cdot\\|\\) is the Euclidean norm.\nI first encountered this identity when I was writing my own code during my doctoral studies. I encountered the proof on Stack Exchange, but at that time it was of no use for me. Later I learned that one can show this identity in several ways, and here is mine.\nLet \\(f(x) = d^2(x,y)\\) be the square distance function. As you can see, this is a function of \\(x\\) only with a fixed \\(y\\in\\calM\\). Our goal is to compute the Riemannian gradient \\(\\nabla_x f(x)\\).\nWe start with a smooth curve \\(\\gamma(t)\\) on the manifold \\(\\calM\\) such that \\(\\gamma(0) = x\\) (initial location) and \\(\\dot{\\gamma}(0) = \\xi \\in T_x \\calM\\) (initial velocity). Then the directional derivative of \\(f\\) at \\(x\\) along \\(\\xi\\) is defined as \\[\nDf(x)[\\xi] = \\frac{d}{dt}f(\\gamma(t))\\bigg|_{t=0} = \\frac{d}{dt}d^2(\\gamma(t),y)\\bigg|_{t=0},\n\\] and we can apply the chain rule to the squared distance: \\[\n\\frac{d}{dt}d^2(\\gamma(t),y)\\bigg|_{t=0} = 2d(\\gamma(t),y)\\cdot  \\frac{d}{dt}d(\\gamma(t),y)\\bigg|_{t=0} = 2d(x,y)\\cdot \\frac{d}{dt}d(\\gamma(t),y)\\bigg|_{t=0}.\n\\] To compute the derivative, we invoke the first variation formula for the Riemannian distance. Let \\(\\gamma_s (t)\\) denote the minimizing geodesic from \\(\\alpha(s)\\) to \\(y\\), and define the variation of geodesics \\[\n\\tilde{\\gamma}(s,t):= \\gamma_s (t),\n\\] with \\(\\tilde{\\gamma}(0,t) = \\gamma(t)\\) being the geodesic from \\(x\\) to \\(y\\). Let \\(E(s) := d(\\alpha(s), y)\\). Then \\(E(s)^2\\) is the energy of the minimizing geodesic \\(\\gamma_s\\), i.e., \\[\nE(s)^2 = \\int_{0}^{1} \\bigg\\| \\frac{\\partial \\tilde{\\gamma}}{\\partial t}(s,t) \\bigg\\|^2 dt.\n\\] Hence, \\[\nE'(0) = \\frac{1}{2d(x,y)} \\cdot \\left. \\frac{d}{ds} \\int_{0}^{1} \\left\\| \\frac{\\partial \\tilde{\\gamma}}{\\partial t}(s,t) \\right\\|^2 dt \\right|_{s=0}.\n\\] We now use the first variation formula for energy functional: if \\(\\tilde{\\gamma}(s,t)\\) is a variation of geodesics such that \\(\\tilde{\\gamma}(s,1) = y\\) is fixed and \\(\\tilde{\\gamma}(s,0) = \\alpha(s)\\), then \\[\n\\left. \\frac{d}{ds} \\int_{0}^{1} \\left\\| \\frac{\\partial \\tilde{\\gamma}}{\\partial t}(s,t) \\right\\|^2 dt \\right|_{s=0} = -2 \\left\\langle \\dot{\\gamma}(0), \\dot{\\alpha}(0) \\right\\rangle.\n\\] Here, \\(\\gamma(t) = \\tilde{\\gamma}(0,t)\\) is the minimizing geodesic from \\(x\\) to \\(y\\), and \\(\\dot{\\gamma}(0)\\) is its initial velocity vector. By definition of the Riemannian logarithmic map, \\[\n\\dot{\\gamma}(0) = \\frac{\\log_x y}{\\|\\log_x y \\|}\\cdot d(x,y).\n\\] Therefore, \\[\nE'(0) = \\frac{1}{2d(x,y)}\\cdot \\left(-2 \\langle \\dot{\\gamma}(0), \\xi \\rangle \\right) = -\\left\\langle\n\\frac{\\log_x y}{\\|\\log_x y \\|}, \\xi\n\\right\\rangle,\n\\] which completes the proof of the first variation formula. This classical result states that for a smooth variation of points \\(\\gamma(t)\\) and corresponding minimizing geodesics \\(c_t(s)\\) from \\(\\gamma(t)\\) to \\(y\\), we have: \\[\n\\frac{d}{dt}d(\\gamma(t),y)\\bigg|_{t=0} = - \\left\\langle \\frac{\\log_x y}{\\|\\log_x y\\|}, \\dot{\\gamma}(0)  \\right\\rangle.\n\\] Plugging this into the chain rule, we obtain \\[\n\\frac{d}{dt}d^2(\\gamma(t), y)\\bigg|_{t=0} = -2 \\langle \\log_x y, \\xi \\rangle.\n\\] By the definition of the Riemannian gradient, we have: \\[\nD f(x)[\\xi] = \\langle \\nabla_x f(x), \\xi \\rangle.\n\\] Since this identity must hold for every tangent vector \\(\\xi \\in T_x \\calM\\), it follows that \\[\n\\nabla_x f(x) = -2\\log_x y.\n\\]"
  },
  {
    "objectID": "Blog/blog_004_GradientSquaredDistance.html#footnotes",
    "href": "Blog/blog_004_GradientSquaredDistance.html#footnotes",
    "title": "Gradient of squared distance on a Riemannian manifold",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWe assume that the logarithmic map is well-defined. For instance, you canâ€™t define a unique logarithmic map on a point on the unit hypersphere to its antipodal point.â†©ï¸"
  },
  {
    "objectID": "Blog/blog_001_FisherRao.html",
    "href": "Blog/blog_001_FisherRao.html",
    "title": "What is the Fisher-Rao distance?",
    "section": "",
    "text": "How can we measure the difference between two probability distributions in a principled way? One approach is through the Fisher-Rao distance, a geometric measure of dissimilarity based on the Fisher information matrix (FIM).\nTo understand this distance, we first examine the role of the FIM in statistical inference and its interpretation as a Riemannian metric. This metric structure naturally leads to the Fisher-Rao distance, which measures the shortest pathâ€”or geodesicâ€”between distributions on the statistical manifold. However, directly computing this distance is often intractable.\nTo address this challenge, we introduce an alternative approach: the square-root transformation, which embeds probability densities into a Hilbert space. A key result follows from this transformation: the Fisher-Rao distance is exactly twice the geodesic distance between transformed densities on the unit sphere. This connection not only simplifies computations but also offers new insights into the geometry of probability distributions."
  },
  {
    "objectID": "Blog/blog_001_FisherRao.html#introduction",
    "href": "Blog/blog_001_FisherRao.html#introduction",
    "title": "What is the Fisher-Rao distance?",
    "section": "",
    "text": "How can we measure the difference between two probability distributions in a principled way? One approach is through the Fisher-Rao distance, a geometric measure of dissimilarity based on the Fisher information matrix (FIM).\nTo understand this distance, we first examine the role of the FIM in statistical inference and its interpretation as a Riemannian metric. This metric structure naturally leads to the Fisher-Rao distance, which measures the shortest pathâ€”or geodesicâ€”between distributions on the statistical manifold. However, directly computing this distance is often intractable.\nTo address this challenge, we introduce an alternative approach: the square-root transformation, which embeds probability densities into a Hilbert space. A key result follows from this transformation: the Fisher-Rao distance is exactly twice the geodesic distance between transformed densities on the unit sphere. This connection not only simplifies computations but also offers new insights into the geometry of probability distributions."
  },
  {
    "objectID": "Blog/blog_001_FisherRao.html#fisher-information-matrix",
    "href": "Blog/blog_001_FisherRao.html#fisher-information-matrix",
    "title": "What is the Fisher-Rao distance?",
    "section": "Fisher Information Matrix",
    "text": "Fisher Information Matrix\nIn a standard mathematical statistics course, one encounters the Fisher information matrix (FIM) for a probability density function \\(p(x\\vert \\theta)\\) with some parameter \\(\\theta \\in \\Theta \\subset \\mathbb{R}^d\\), defined as: \\[\\begin{equation*}\nI(\\theta) = \\mathbb{E} \\left[\n\\left( \\frac{\\partial}{\\partial \\theta} \\log p(x\\vert \\theta) \\right) \\left( \\frac{\\partial}{\\partial \\theta} \\log p(x\\vert \\theta) \\right)^\\top\n\\right].\n\\end{equation*}\\] Alternatively, it can be expressed in terms of second derivatives: \\[\\begin{equation*}\nI(\\theta) = -\\mathbb{E} \\left[\\frac{\\partial^2 \\log p(x\\vert \\theta)}{\\partial \\theta \\partial \\theta^\\top}\\right].\n\\end{equation*}\\]\nThe Fisher information matrix plays a central role in statistical inference. In maximum likelihood estimation (MLE), it helps assess the asymptotic variance of the MLE: \\[\\begin{equation}\n\\sqrt{n} (\\hat{\\theta}_n - \\theta) \\xrightarrow{d} \\mathcal{N}(0, I^{-1}(\\theta)).\n\\end{equation}\\] The CramÃ©r-Rao bound states that for any unbiased estimator \\(\\hat{\\theta}\\): \\[\\begin{equation}\n\\textrm{Cov}(\\hat{\\theta}) \\succeq I^{-1}(\\theta),\n\\end{equation}\\] where \\(A \\succeq B\\) indicates that \\(A-B\\) is positive semi-definite. In other words, the covariance matrix of any unbiased estimator is bounded below by the inverse of the FIM. In Bayesian statistics, the FIM is related to constructing priors1. The Jeffreys prior, an invariant and non-informative prior, is given by: \\[\\begin{equation}\n\\pi(\\theta) \\propto \\sqrt{\\textrm{det}(I(\\theta))},\n\\end{equation}\\] which is an objective, non-information prior that is invariant under reparametrization."
  },
  {
    "objectID": "Blog/blog_001_FisherRao.html#fisher-rao-metric",
    "href": "Blog/blog_001_FisherRao.html#fisher-rao-metric",
    "title": "What is the Fisher-Rao distance?",
    "section": "Fisher-Rao Metric",
    "text": "Fisher-Rao Metric\nA key discovery in information geometry (Amari et al. 2007) is that the FIM induces a Riemannian metric on the statistical manifold \\(\\mathcal{M} = \\lbrace p(x\\vert \\theta) \\rbrace\\), known as the Fisher-Rao metric. Given an infinitesimal displacement \\(d\\theta\\) in the parameter space \\(\\Theta\\), the squared length of the displacement under the Fisher-Rao metric is given by: \\[\\begin{equation}\nds^2 = d\\theta^\\top I(\\theta) d\\theta.\n\\end{equation}\\] Thus, the Fisher-Rao metric captures the local geometry of the statistical manifold by taking the FIM as the local matrix form in a given coordinate system."
  },
  {
    "objectID": "Blog/blog_001_FisherRao.html#fisher-rao-distance",
    "href": "Blog/blog_001_FisherRao.html#fisher-rao-distance",
    "title": "What is the Fisher-Rao distance?",
    "section": "Fisher-Rao Distance",
    "text": "Fisher-Rao Distance\nSince the Fisher-Rao metric defines a Riemannian structure, one can define a distance between two points in the manifold. The Fisher-Rao distance between two distributions, \\(p_1(x) = p(x\\vert \\theta_1)\\) and \\(p_2(x) = p(x\\vert \\theta_2)\\), is the geodesic length connecting \\(\\theta_1\\) and \\(\\theta_2\\):\n\\[\nd_{FR}(p_1, p_2):= d_{FR}(\\theta_1, \\theta_2) = \\underset{\\gamma}{\\inf} \\int \\sqrt{\\dot{\\gamma}(t)^\\top I(\\gamma(t)) \\dot{\\gamma}(t)} dt,\n\\tag{1}\\] where the infimum is taken over all smooth curves \\(\\gamma:[0,1] \\rightarrow \\Theta\\) such that \\(\\gamma (0) = \\theta_1\\) and \\(\\gamma (1) = \\theta_2\\). This formulation reveals that the Fisher-Rao distance is a Riemannian geodesic distance."
  },
  {
    "objectID": "Blog/blog_001_FisherRao.html#computing-the-fisher-rao-distance",
    "href": "Blog/blog_001_FisherRao.html#computing-the-fisher-rao-distance",
    "title": "What is the Fisher-Rao distance?",
    "section": "Computing the Fisher-Rao Distance",
    "text": "Computing the Fisher-Rao Distance\nWhile theoretically elegant, computing the Fisher-Rao distance as in EquationÂ 1 is challenging because it requires solving the geodesic equations: \\[\n\\frac{d^2 \\theta^i}{dt^2} + \\sum_{j,k} \\Gamma^i_{jk} \\frac{d\\theta^j}{dt} \\frac{d\\theta^k}{dt} = 0,\n\\] where \\(\\Gamma^i_{jk}\\) are Christoffel symbols derived from the Fisher information metric: \\[\n\\Gamma^i_{jk} = \\frac{1}{2} \\sum_m I^{im} \\left( \\frac{\\partial I_{mj}}{\\partial \\theta^k} + \\frac{\\partial I_{mk}}{\\partial \\theta^j} - \\frac{\\partial I_{jk}}{\\partial \\theta^m} \\right).\n\\] Exact solutions are known to exist for only a handful of distributions (Miyamoto et al. 2024)."
  },
  {
    "objectID": "Blog/blog_001_FisherRao.html#square-root-transformation-and-geodesic-distance",
    "href": "Blog/blog_001_FisherRao.html#square-root-transformation-and-geodesic-distance",
    "title": "What is the Fisher-Rao distance?",
    "section": "Square-Root Transformation and Geodesic Distance",
    "text": "Square-Root Transformation and Geodesic Distance\nA practical alternative for computing the Fisher-Rao distance is the square-root transformation: \\[\np(x) \\mapsto \\sqrt{p(x)}.\n\\] This procedure embeds probability densities into the Hilbert space \\(L_2\\) with the standard inner product. For \\(f,g \\in L_2\\), the inner product is given by \\[\n\\langle f,g \\rangle = \\int f(x) g(x) dx.\n\\]\nIt is straightforward to verify that the transformed functions lie on the infinite-dimensional unit sphere \\(S^\\infty\\). Specifically, let \\(f \\in L_2\\) be the transformed function of some density \\(\\phi\\), i.e., \\(f(x) = \\sqrt{\\phi(x)}\\). Then, we have \\[\n\\| f \\|^2 = \\langle f, f \\rangle = \\int f(x)^2 dx = \\int \\phi(x) = 1,\n\\] where the last equality follows from the fact that \\(\\phi(x)\\) is a probability density function and thus integrates to 1.\nOn the unit sphere in \\(L^2\\), the geodesic curve between two points is given by the great circle connecting them. Let \\(\\psi_1\\) and \\(\\psi_2\\) be two elements of \\(S^\\infty\\). Then, the geodesic distance between them is canonically known by \\(\\textrm{arccos}(\\langle \\psi_1, \\psi_2 \\rangle)\\)."
  },
  {
    "objectID": "Blog/blog_001_FisherRao.html#equivalence-of-fisher-rao-distance-and-geodesic-distance-of-square-root-densities",
    "href": "Blog/blog_001_FisherRao.html#equivalence-of-fisher-rao-distance-and-geodesic-distance-of-square-root-densities",
    "title": "What is the Fisher-Rao distance?",
    "section": "Equivalence of Fisher-Rao Distance and Geodesic Distance of Square Root Densities",
    "text": "Equivalence of Fisher-Rao Distance and Geodesic Distance of Square Root Densities\nWe are now ready to state the main result of this discussion. The Fisher-Rao distance between two distributions, \\(p_1(x) = p(x\\vert \\theta_1)\\) and \\(p_2(x) = p(x\\vert \\theta_2)\\), is twice the geodesic distance between their square-root transformed densities, \\(\\psi_1(x) = \\sqrt{p_1(x)}\\) and \\(\\psi_2(x) = \\sqrt{p_2(x)}\\), on \\(S^\\infty\\):\n\n\\[d_{FR}(p_1,p_2) = 2 \\textrm{arccos} \\left(\n\\int \\sqrt{p_1 (x) p_2(x)} dx\n\\right) \\tag{2}\\]\n\nTo verify EquationÂ 2, consider an infinitesimal perturbation in \\(p(x)\\) by introducing a small variation: \\[\np(x) \\mapsto p(x) + \\epsilon h(x),\n\\tag{3}\\] where \\(h(x)\\) is an arbitrary function that integrates to 0 and \\(\\epsilon &gt; 0\\). This constraint ensures that \\(p(x) + \\epsilon h(x)\\) remains as a valid density. Our goal is to quantify how this small variation in \\(p(x)\\) induces a corresponding change in \\(\\psi(x)\\). In other words, we seek to measure the variation in the transformed function. Viewing the right-hand side of EquationÂ 3 as a function of \\(\\epsilon\\), we apply a first-order Taylor expansion to the square-root transformation: \\[\n\\sqrt{p + \\epsilon h} \\approx \\sqrt{p} + \\frac{1}{2} \\frac{\\epsilon h}{\\sqrt{p}} + O(\\epsilon^2).\n\\] Thus, the infinitesimal perturbation in \\(\\psi(x)\\) is: \\[\n\\delta \\psi(x) = \\sqrt{p(x) + \\epsilon h(x)} - \\sqrt{p(x)}\n= \\frac{1}{2}\\frac{h(x)}{\\sqrt{p(x)}} \\epsilon + O(\\epsilon^2).\n\\tag{4}\\]\nFrom EquationÂ 4, we observe that \\(\\delta \\psi(x)\\) is linear in \\(h(x)\\), demonstrating how small changes in \\(p(x)\\) translate to changes in \\(\\psi(x)\\)2. Recall that the Fisher-Rao metric corresponds to infinitesimal displacements in probability space. Given the embedding of densities via the square-root transformation, it is natural to measure perturbations by computing the squared norm: \\[\nds^2 = \\|\\delta \\psi \\|^2 = \\int \\left( \\frac{1}{2}\\frac{h(x)}{\\sqrt{p(x)}} \\right)^2 dx = \\frac{\\epsilon^2}{4} \\int \\frac{h(x)^2}{p(x)} dx.\n\\tag{5}\\]\nSince \\(h(x)\\) represents an infinitesimal change in \\(p(x)\\), we recognize that the integral in EquationÂ 5 corresponds precisely to the Fisher information metric evaluated for an infinitesimal displacement. Consequently, we obtain \\[\nds^2 = \\frac{1}{4} \\int \\frac{h(x)^2}{p(x)} dx.\n\\] which reveals that the Fisher-Rao metric is \\(\\frac{1}{4}\\) times the standard metric on \\(S^\\infty\\) induced by the square-root transformation.\nBy combining these observations with the fact that geodesic distances scale inversely with the metric factor, we conclude that the geodesic distance computed in the Fisher-Rao metric is twice the standard geodesic distance on the unit sphere in \\(L^2\\). That is, for two densities \\(p_1\\) and \\(p_2\\) and their transformations \\(\\psi_1 = \\sqrt{p_1}\\) and \\(\\psi_2 = \\sqrt{p_2}\\) in \\(L_2\\), we have \\[\nd_{FR}(p_1, p_2) = 2 \\times d_{S^\\infty} (\\psi_1, \\psi_2) = 2 \\textrm{arccos} \\left( \\int\n\\sqrt{p_1 (x) p_2 (x)} dx\n\\right).\n\\]"
  },
  {
    "objectID": "Blog/blog_001_FisherRao.html#final-thoughts",
    "href": "Blog/blog_001_FisherRao.html#final-thoughts",
    "title": "What is the Fisher-Rao distance?",
    "section": "Final Thoughts",
    "text": "Final Thoughts\n\nIn the derivation of the Fisher-Rao distance in terms of the scaled geodesic distance on \\(S^\\infty\\), no specific choice or constraint is imposed on \\(\\theta\\). This formulation naturally extends to nonparametric probability densities, as the geodesic distance in \\(L^2\\) depends solely on the embedding.\nDespite the equivalence, some concerns remain regarding the computability of the Fisher-Rao distance. For instance, evaluating the integral can be challenging in many cases, particularly for high-dimensional distributions or arbitrary nonparametric densities, where numerical integration becomes intractable. Approximation via Monte Carlo methods, while useful, presents its own set of difficulties. Moreover, when densities are estimated, they may contain regions of extremely small values, leading to numerical precision issues.\nI would like to express my appreciation to Prof.Â Marco Radeschi for his kindness and patience in enduring my endless questions in his differential geometry course - many of which, in hindsight, seem absurd."
  },
  {
    "objectID": "Blog/blog_001_FisherRao.html#references",
    "href": "Blog/blog_001_FisherRao.html#references",
    "title": "What is the Fisher-Rao distance?",
    "section": "References",
    "text": "References\n\n\nAmari, Shunâ€™ichi, Hiroshi Nagaoka, Shunâ€™ichi Amari, and Shunâ€™ichi Amari. 2007. Methods of Information Geometry. Translated by Daishi Harada. Nachdruck. Translations of Mathematical Monographs 191. Providence, Rhode Island: American Mathematical Society.\n\n\nMiyamoto, Henrique K., FÃ¡bio C. C. Meneghetti, Julianna Pinele, and Sueli I. R. Costa. 2024. â€œOn Closed-Form Expressions for the Fisher-Rao Distance.â€ Information Geometry 7 (2): 311â€“54. https://doi.org/10.1007/s41884-024-00143-2."
  },
  {
    "objectID": "Blog/blog_001_FisherRao.html#footnotes",
    "href": "Blog/blog_001_FisherRao.html#footnotes",
    "title": "What is the Fisher-Rao distance?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThere is an interesting connection to the objective Bayesian framework, which will appear in a later post.â†©ï¸\nFrom a geometric point of view, \\(\\delta \\psi(x)\\) belongs to the tangent space of \\(S^\\infty\\) at the point \\(\\psi(x)\\).â†©ï¸"
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Software",
    "section": "",
    "text": "Software\nI enjoy building open-source tools for science, mostly at the intersection of statistics, machine learning, and computational mathematics. Most projects are developed in R and/or Python, with their computational cores written in C++ to ensure speed and efficiency. I focus on clear APIs, reproducible results, and practical performance. Click a package name to visit its documentation site, when available.\nStatistical & Machine Learning\n\n\n\n\nPackage\nRepo\nDescription\n\n\n\n\nCovTools\nÂ Â \nStatistical Tools for Covariance Analysis\n\n\nfilling\nÂ Â \nMatrix Completion, Imputation, and Inpainting Methods\n\n\nRdimtools\nÂ Â \nDimension Reduction and Estimation Methods\n\n\nrepsim\nÂ Â Â Â \nMeasures of Representational Similarity\n\n\nSBmedian\nÂ Â \nScalable Bayes with Median of Subset Posteriors\n\n\nSHT\nÂ Â \nStatistical Hypothesis Testing Toolbox\n\n\nT4cluster\nÂ Â \nTools for Cluster Analysis\n\n\n\n\n\n\nFoundational Mathematical & Computational Routines\n\n\n\n\nPackage\nRepo\nDescription\n\n\n\n\nmaotai\nÂ Â \nTools for Matrix Algebra, Optimization and Inference\n\n\nRlinsolve\nÂ Â \nIterative Solvers for (Sparse) Linear System of Equations\n\n\nT4transport\nÂ Â \nTools for Computational Optimal Transport\n\n\ntvR\nÂ Â \nTotal Variation Regularization for Signals and Images\n\n\n\n\n\n\nNetwork Analysis\n\n\n\n\nPackage\nRepo\nDescription\n\n\n\n\ngraphon\nÂ Â \nA Collection of Graphon Estimation Methods\n\n\nNetworkDistance\nÂ Â \nDistance Measures for Networks\n\n\n\n\n\n\nGeometry & Topology\n\n\n\n\nPackage\nRepo\nDescription\n\n\n\n\nCORRbox\nÂ Â \nScalable Learning with Correlation-based Functional Networks\n\n\nRiemann\nÂ Â \nLearning with Data on Riemannian manifold\n\n\nRiemBase\nÂ Â \nFunctions and C++ Headers for Computation on Manifolds\n\n\nSPDtoolbox\nÂ Â \nGeometric Learning of Brain Functional Networks\n\n\nTDAkit\nÂ Â \nToolkit for Topological Data Analysis"
  },
  {
    "objectID": "Teaching/note02-rodriguez.html",
    "href": "Teaching/note02-rodriguez.html",
    "title": "Rodriguesâ€™ formula for the Legendre polynomials",
    "section": "",
    "text": "Legendre polynomials \\(P_n (x)\\) are solutions of the following Legendreâ€™s differential equation \\[\n(1-x^2) y'' - 2x y' + n(n+1) y = 0\n\\tag{1}\\] for some \\(n \\in \\mathbb{N}\\cup \\{0\\}\\). An explicit, compact expression for the polynomials is provided by Rodriguesâ€™ formula \\[\nP_n (x) = \\frac{1}{2^n n!} \\frac{d^n}{dx^n} (x^2-1)^n.\n\\tag{2}\\]\nThis means that when \\(P_n (x)\\) is plugged in the position of \\(y\\) for EquationÂ 1, it must satisfy the equality to 0. In this post, we show (a bit tedious) derivations to attain EquationÂ 2."
  },
  {
    "objectID": "Teaching/note02-rodriguez.html#introduction",
    "href": "Teaching/note02-rodriguez.html#introduction",
    "title": "Rodriguesâ€™ formula for the Legendre polynomials",
    "section": "",
    "text": "Legendre polynomials \\(P_n (x)\\) are solutions of the following Legendreâ€™s differential equation \\[\n(1-x^2) y'' - 2x y' + n(n+1) y = 0\n\\tag{1}\\] for some \\(n \\in \\mathbb{N}\\cup \\{0\\}\\). An explicit, compact expression for the polynomials is provided by Rodriguesâ€™ formula \\[\nP_n (x) = \\frac{1}{2^n n!} \\frac{d^n}{dx^n} (x^2-1)^n.\n\\tag{2}\\]\nThis means that when \\(P_n (x)\\) is plugged in the position of \\(y\\) for EquationÂ 1, it must satisfy the equality to 0. In this post, we show (a bit tedious) derivations to attain EquationÂ 2."
  },
  {
    "objectID": "Teaching/note02-rodriguez.html#approach",
    "href": "Teaching/note02-rodriguez.html#approach",
    "title": "Rodriguesâ€™ formula for the Legendre polynomials",
    "section": "Approach",
    "text": "Approach\nI will proceed in two steps. Let \\(f_n (x) = (x^2 - 1)^n\\) then we first show that the \\(n\\)-th derivative of \\(f_n (x)\\) is a solution of Legendre equation. Then, we find a proper scaling factor of \\(1/ 2^n n!\\) to recover \\(P_n (x)\\) in line with a common constraint that \\(P_n (x) = 1\\) for all \\(n\\) when \\(x=1\\). For notational simplicity, we denote \\(g^{(n)}\\) for the \\(n\\)-th derivative of a function \\(g(x)\\), i.e, \\[\ng^{(n)} = \\frac{d^n}{dx^n} g(x).\n\\]\nBefore proceeding, we need (generalized) Leibnizâ€™s rule. Suppose we have \\(n\\)-times differentiable functions \\(f(x)\\) and \\(g(x)\\), then \\[\n\\frac{d^n}{dx^n} f(x) g(x) = \\sum_{k=0}^n\n\\begin{pmatrix}\nn \\\\ k\n\\end{pmatrix}\nf^{(n-k)} (x) g^{(k)} (x) = \\sum_{k=0}^n \\frac{n!}{k! (n-k)!} f^{(n-k)} (x) g^{(k)} (x)\n\\tag{3}\\] where the choice of \\(f\\) and \\(g\\) can help in reducing the number of terms when there exists a polynomial term. For example, when \\(g(x) = x^2\\), \\(g^{(k)} = 0\\) for all \\(k \\geq 3\\).\n\nStep 1. \\(f_n^{(n)} (x)\\) is one solution.\nOur goal here is to show that \\(f_n^{(n)}(x)\\) is a solution for EquationÂ 1. As a first step, letâ€™s take derivative on \\(f_n (x)\\), \\[\n\\begin{align*}\n\\frac{d}{dx} f_n (x) &= 2 n (x^2  - 1)^{n-1} x \\\\\n&= 2nx (x^2 - 1)^{n-1}.\n\\end{align*}\n\\] By multiplying \\((x^2 - 1)\\) both side of the above, we have \\[\n(x^2 - 1) \\frac{d}{dx} f_n (x)  = 2nx (x^2 - 1)^n.\n\\]\nNow, differentiate both sides \\((n+1)\\) times, which leads to \\[\n\\begin{align*}\n\\frac{d^{n+1}}{dx^{n+1}} \\left[ \\frac{d}{dx} f_n (x) \\right]  (x^2 -1)\n&= \\sum_{k=0}^{n+1}\n\\begin{pmatrix}\nn+1 \\\\ k\n\\end{pmatrix}\n\\left( \\frac{d}{dx} f_n (x) \\right)^{(n+1-k)} (x^2-1)^{(k)} \\\\\n&=\n\\begin{pmatrix}\nn+1 \\\\ 0\n\\end{pmatrix}\nf_n^{(n+2)} (x) (x^2-1) +\n\\begin{pmatrix}\nn+1 \\\\ 1\n\\end{pmatrix}\n2x f_n^{(n+1)}  (x) +\n\\begin{pmatrix}\nn+1 \\\\ 2\n\\end{pmatrix}\nf_n ^{(n)} (x) \\cdot 2 \\\\\n&= (x^2-1) f_n^{(n+2)} (x) + 2(n+1)x f_n^{(n+1)} (x) +\nn(n+1) f_n^{(n)} (x)\n\\end{align*}\n\\] for the left-hand side. We also have that \\[\n\\begin{align*}\n\\frac{d^{n+1}}{dx^{n+1}} f_n(x) 2nx &=\n\\begin{pmatrix}\nn+1 \\\\ 0\n\\end{pmatrix}\nf_n^{(n+1)} (x) 2nx +\n\\begin{pmatrix}\nn+1 \\\\ 1\n\\end{pmatrix}\nf_n^{(n)} (x) 2n \\\\\n&= 2nx f_n^{(n+1)} (x) + 2n(n+1)f_n^{(n)}(x).\n\\end{align*}\n\\] Therefore, we have the following arrangement, \\[\n\\begin{equation*}\n    \\begin{gathered}\n    (x^2 - 1) f_n^{(n+2)} (x) + 2x(n+1)f_n^{(n+1)} (x) + n(n+1)f_n^{(n)} (x) =\n    2nx f_n^{(n+1)} (x) + 2n(n+1) f_n^{(n)} (x) \\\\\n    (x^2-1) f_n^{(n+2)} (x) + 2x f_n^{(n+1)}(x) - n(n+1)f_n^{(n)} (x) = 0 \\\\\n    (1- x^2) f_n^{(n+2)} (x) - 2x f_n^{(n+1)}(x) + n(n+1)f_n^{(n)} (x) = 0\n    \\end{gathered}\n\\end{equation*}\n\\] where the last line is in the form of EquationÂ 1 so that we have \\(f_n^{(n)} (x)\\) as a solution.\n\n\nStep 2. Find a scaling factor.\nEven though \\(f_n^{(n)} (x)\\) as a solution, we have a requirement for the standard Legendre polynomial that \\(P_n (x)=1\\) for \\(x=1\\). Let us take a closer look at \\(f_n^{(n)}(x)\\) when evaluated at \\(x=1\\). \\[\n\\begin{align*}\nf_n^{(n)} (x) &= \\frac{d^n}{dx^n} (x^2-1)^n \\\\\n&= \\frac{d^n}{dx^n} (x+1)^n (x-1)^n \\\\\n&= \\sum_{k=0}^n \\begin{pmatrix}\nn \\\\ k\n\\end{pmatrix}\n\\left( (x+1)^n \\right)^{(k)} \\left( (x-1)^n \\right) ^{(n-k)} \\quad \\textrm{by the Leibniz's rule} \\\\\n&= \\sum_{k=0}^n \\begin{pmatrix}\nn \\\\ k\n\\end{pmatrix}\n\\frac{n!}{(n-k)!} (x+1)^{n-k} \\frac{n!}{k!} (x-1)^k \\qquad (*) \\\\\n&= n! \\sum_{k=0}^n \\begin{pmatrix}\nn \\\\ k\n\\end{pmatrix} \\frac{n!}{(n-k)! k!} (x+1)^{n-k} (x-1)^k \\\\\n&= n! \\sum_{k=0}^n \\begin{pmatrix}\nn \\\\ k\n\\end{pmatrix}^2 (x+1)^{n-k} (x-1)^k,\n\\end{align*}\n\\] Since we want to evaluate \\(f_n^{(n)} (x)\\) at \\(x=1\\), the last line of equations above tells us that all the terms but \\(k=0\\) become zero, \\[\n\\begin{equation*}\nf_n^{(n)} (x=1) = n! \\begin{pmatrix}\nn \\\\ 0\n\\end{pmatrix}^2 2^{n-0} = n! 2^n\n\\end{equation*}\n\\] which finally leads to define \\(P_n (x)\\) as \\[\nP_n (x) = \\frac{1}{n! 2^n}f_n^{(n)}(x)  = \\frac{1}{n! 2^n} \\frac{d^n}{dx^n} (x^2-1)^n\n\\] to fulfill the condition of \\(P_n (x) =1\\) for \\(x=1\\)."
  },
  {
    "objectID": "blog_kr.html",
    "href": "blog_kr.html",
    "title": "Kisung You",
    "section": "",
    "text": "í†µê³„í•™ìë„, ë°ì´í„° ê³¼í•™ìë„ ì•„ë‹Œ ì‚¬ëŒì´ í†µê³„ì²­ì¥ì„ ë§¡ëŠ” ë‚˜ë¼\n\n\n\n\n\nMay 17, 2025\n\n\n\n\n\nNo matching items"
  }
]