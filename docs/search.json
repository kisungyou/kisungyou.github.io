[
  {
    "objectID": "publication.html",
    "href": "publication.html",
    "title": "Publication",
    "section": "",
    "text": "On the Wasserstein median of probability measures KY and Dennis Shung  Submitted to Journal of American Statistical Association.\n        \n        arXiv\n    \nGeometric Learning of Functional Connectivity on the Correlation Manifold KY and Hae-Jeong Park  Submitted to Scientific Reports.\nOn the spherical Laplace distribution KY and Dennis Shung  Submitted to Journal of Multivariate Analysis.\n        \n        arXiv\n    \nNetwork Distance Based on Laplacian Flows on Graphs Dianbin Bao, KY, and Lizhen Lin  Submitted to IEEE Big Data 2022.\n        \n        arXiv\n     \n        \n        Code\n    \nBayesian Optimal Two-sample Tests for High-dimensional Gaussian Populations Kyoungjae Lee, KY, and Lizhen Lin  Submitted to Bayesian Analysis.\n        \n        arXiv\n    \nShape-Preserving Dimensionality Reduction : An Algorithm and Measures of Topological Equivalence Byeongsu Yu and KY  Submitted to IEEE Transactions on Pattern Analysis and Machine Intelligence.\n        \n        arXiv\n    \nPublished\n2022\nParameter estimation and model-based clustering with spherical normal distribution on the unit hypersphere KY and Changhee Suh.  Computational Statistics & Data Analysis (2022).\n        \n        arXiv\n     \n        \n        Code\n     \n        \n        Publisher's Site\n    \n Learning Subspaces of Different Dimensions Brian St. Thomas, KY, Lizhen Lin, Lek-Heng Lim, and Sayan Mukherjee.  Journal of Computational and Graphical Statistics (2022).\n        \n        arXiv\n     \n        \n        Code\n     \n        \n        Publisher's Site\n    \nRdimtools: An R package for Dimension Reduction and Intrinsic Dimension Estimation KY and Dennis Shung.  Software Impacts (2022).\n        \n        arXiv\n     \n        \n        Code\n     \n        \n        Publisher's Site\n    \n2021\nRe-visiting Riemannian geometry of symmetric positive definite matrices for the analysis of functional connectivity KY and Hae-Jeong Park.  NeuroImage (2021).\n        \n        Code\n     \n        \n        Publisher's Site\n    \n2020\nData transforming augmentation for heteroscedastic models Hyungsuk Tak, KY, Sujit K. Ghosh, Bingyue Su, and Joseph Kelly.  Journal of Computational and Graphical Statistics (2020).\n        \n        arXiv\n     \n        \n        Publisher's Site\n    \n2019\nVolume change pattern of decompression of mandibular odontogenic keratocyst Jin Hoo Park, Eun-Jung Kwak, KY, Young-Soo Jung, and Hwi-Dong Jung.  Maxillofacial Plastic and Reconstructive Surgery (2019).\n        \n        Publisher's Site\n    \n2016\nVision-based detection of loosened bolts using the Hough transform and support vector machines Young-Jin Cha, KY, and Wooram Choi.  Automation in Construction (2016).\n        \n        Publisher's Site\n    \nArchived Preprint\nComparing multiple latent space embeddings using topological analysis KY, Ilmun Kim, Ick Hoon Jin, Minjeong Jeon, and Dennis Shung (2022).\n        \n        arXiv\n     \n        \n        Code"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Kisung You",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n  \n\n\n\n\nA Note on Angular Central Gaussian Distribution and its Matrix Variant\n\n\n\n\n\n\n\nnotes\n\n\ngeometric statistics\n\n\n\n\n\n\n\n\n\n\n\nAug 12, 2022\n\n\n\n\n\n\n  \n\n\n\n\nRodrigues’ formula for the Legendre polynomials\n\n\n\n\n\n\n\nnotes\n\n\ncalculus\n\n\n\n\n\n\n\n\n\n\n\nAug 10, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMonte Carlo computation of \\(L_p\\) distance between two densities on the unit hypersphere\n\n\n\n\n\n\n\nnotes\n\n\ngeometric statistics\n\n\ncomputation\n\n\n\n\n\n\n\n\n\n\n\nAug 9, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Kisung You",
    "section": "",
    "text": "I am a postdoctoral associate in the Department of Internal Medicine at Yale School of Medicine working with Prof. Dennis Shung. My research focuses on the use of mathematical tools from statistics, geometry, and topology in developing novel methods to answer abstruse scientific questions. The primary application domain is biomedical data analysis. I am also deeply interested in high-performance computing and statistical software development to democratize methodological innovations for practitioners.\n\n\n\n\n\nUniversity of Notre Dame\nPhD in Statistics\n Advisors: Lizhen Lin, Ick Hoon Jin\n Thesis: Topics in Geometric and Topological Data Analysis\n\n\nNotre Dame, IN\nAugust 2021\n\n\n\nYonsei University\nMS in Mathematics\n BBA in Business Administration\nBS in Mathematics\n\nSeoul, South Korea\nAugust 2015\n August 2013"
  },
  {
    "objectID": "posts/note002-rodrigues-formula/index.html",
    "href": "posts/note002-rodrigues-formula/index.html",
    "title": "Rodrigues’ formula for the Legendre polynomials",
    "section": "",
    "text": "Approach\nI will proceed in two steps. Let \\(f_n (x) = (x^2 - 1)^n\\) then we first show that the \\(n\\)-th derivative of \\(f_n (x)\\) is a solution of Legendre equation. Then, we find a proper scaling factor of \\(1/ 2^n n!\\) to recover \\(P_n (x)\\) in line with a common constraint that \\(P_n (x) = 1\\) for all \\(n\\) when \\(x=1\\). For notational simplicity, we denote \\(g^{(n)}\\) for the \\(n\\)-th derivative of a function \\(g(x)\\), i.e,\n\\[\ng^{(n)} = \\frac{d^n}{dx^n} g(x).\n\\]\nBefore proceeding, we need (generalized) Leibniz’s rule. Suppose we have \\(n\\)-times differentiable functions \\(f(x)\\) and \\(g(x)\\), then\n\\[\n\\frac{d^n}{dx^n} f(x) g(x) = \\sum_{k=0}^n\n\\begin{pmatrix}\nn \\\\ k\n\\end{pmatrix}\nf^{(n-k)} (x) g^{(k)} (x) = \\sum_{k=0}^n \\frac{n!}{k! (n-k)!} f^{(n-k)} (x) g^{(k)} (x)\n\\tag{3}\\]\nwhere the choice of \\(f\\) and \\(g\\) can help in reducing the number of terms when there exists a polynomial term. For example, when \\(g(x) = x^2\\), \\(g^{(k)} = 0\\) for all \\(k \\geq 3\\).\n\n\nStep 1. \\(f_n^{(n)} (x)\\) is one solution.\nOur goal here is to show that \\(f_n^{(n)}(x)\\) is a solution for Equation 1. As a first step, let’s take derivative on \\(f_n (x)\\),\n\\[\n\\begin{align*}\n\\frac{d}{dx} f_n (x) &= 2 n (x^2  - 1)^{n-1} x \\\\\n&= 2nx (x^2 - 1)^{n-1}.\n\\end{align*}\n\\]\nBy multiplying \\((x^2 - 1)\\) both side of the above, we have\n\\[\n(x^2 - 1) \\frac{d}{dx} f_n (x)  = 2nx (x^2 - 1)^n.\n\\]\nNow, differentiate both sides \\((n+1)\\) times, which leads to\n\\[\n\\begin{align*}\n\\frac{d^{n+1}}{dx^{n+1}} \\left[ \\frac{d}{dx} f_n (x) \\right]  (x^2 -1)\n&= \\sum_{k=0}^{n+1}\n\\begin{pmatrix}\nn+1 \\\\ k\n\\end{pmatrix}\n\\left( \\frac{d}{dx} f_n (x) \\right)^{(n+1-k)} (x^2-1)^{(k)} \\\\\n&=\n\\begin{pmatrix}\nn+1 \\\\ 0\n\\end{pmatrix}\nf_n^{(n+2)} (x) (x^2-1) +\n\\begin{pmatrix}\nn+1 \\\\ 1\n\\end{pmatrix}\n2x f_n^{(n+1)}  (x) +\n\\begin{pmatrix}\nn+1 \\\\ 2\n\\end{pmatrix}\nf_n ^{(n)} (x) \\cdot 2 \\\\\n&= (x^2-1) f_n^{(n+2)} (x) + 2(n+1)x f_n^{(n+1)} (x) +\nn(n+1) f_n^{(n)} (x)\n\\end{align*}\n\\]\nfor the left-hand side. We also have that\n\\[\n\\begin{align*}\n\\frac{d^{n+1}}{dx^{n+1}} f_n(x) 2nx &=\n\\begin{pmatrix}\nn+1 \\\\ 0\n\\end{pmatrix}\nf_n^{(n+1)} (x) 2nx +\n\\begin{pmatrix}\nn+1 \\\\ 1\n\\end{pmatrix}\nf_n^{(n)} (x) 2n \\\\\n&= 2nx f_n^{(n+1)} (x) + 2n(n+1)f_n^{(n)}(x).\n\\end{align*}\n\\]\nTherefore, we have the following arrangement,\n\\[\n\\begin{equation*}\n    \\begin{gathered}\n    (x^2 - 1) f_n^{(n+2)} (x) + 2x(n+1)f_n^{(n+1)} (x) + n(n+1)f_n^{(n)} (x) =\n    2nx f_n^{(n+1)} (x) + 2n(n+1) f_n^{(n)} (x) \\\\\n    (x^2-1) f_n^{(n+2)} (x) + 2x f_n^{(n+1)}(x) - n(n+1)f_n^{(n)} (x) = 0 \\\\\n    (1- x^2) f_n^{(n+2)} (x) - 2x f_n^{(n+1)}(x) + n(n+1)f_n^{(n)} (x) = 0\n    \\end{gathered}\n\\end{equation*}\n\\]\nwhere the last line is in the form of Equation 1 so that we have \\(f_n^{(n)} (x)\\) as a solution.\n\n\nStep 2. Find a scaling factor.\nEven though \\(f_n^{(n)} (x)\\) as a solution, we have a requirement for the standard Legendre polynomial that \\(P_n (x)=1\\) for \\(x=1\\). Let us take a closer look at \\(f_n^{(n)}(x)\\) when evaluated at \\(x=1\\).\n\\[\n\\begin{align*}\nf_n^{(n)} (x) &= \\frac{d^n}{dx^n} (x^2-1)^n \\\\\n&= \\frac{d^n}{dx^n} (x+1)^n (x-1)^n \\\\\n&= \\sum_{k=0}^n \\begin{pmatrix}\nn \\\\ k\n\\end{pmatrix}\n\\left( (x+1)^n \\right)^{(k)} \\left( (x-1)^n \\right) ^{(n-k)} \\quad \\textrm{by the Leibniz's rule} \\\\\n&= \\sum_{k=0}^n \\begin{pmatrix}\nn \\\\ k\n\\end{pmatrix}\n\\frac{n!}{(n-k)!} (x+1)^{n-k} \\frac{n!}{k!} (x-1)^k \\qquad (*) \\\\\n&= n! \\sum_{k=0}^n \\begin{pmatrix}\nn \\\\ k\n\\end{pmatrix} \\frac{n!}{(n-k)! k!} (x+1)^{n-k} (x-1)^k \\\\\n&= n! \\sum_{k=0}^n \\begin{pmatrix}\nn \\\\ k\n\\end{pmatrix}^2 (x+1)^{n-k} (x-1)^k,\n\\end{align*}\n\\]\nSince we want to evaluate \\(f_n^{(n)} (x)\\) at \\(x=1\\), the last line of equations above tells us that all the terms but \\(k=0\\) become zero,\n\\[\n\\begin{equation*}\nf_n^{(n)} (x=1) = n! \\begin{pmatrix}\nn \\\\ 0\n\\end{pmatrix}^2 2^{n-0} = n! 2^n\n\\end{equation*}\n\\]\nwhich finally leads to define \\(P_n (x)\\) as\n\\[\nP_n (x) = \\frac{1}{n! 2^n}f_n^{(n)}(x)  = \\frac{1}{n! 2^n} \\frac{d^n}{dx^n} (x^2-1)^n\n\\]\nto fulfill the condition of \\(P_n (x) =1\\) for \\(x=1\\).\n\n\n\n\nCitationBibTeX citation:@online{you2022,\n  author = {Kisung You},\n  title = {Rodrigues’ Formula for the {Legendre} Polynomials},\n  date = {2022-08-10},\n  url = {https://kisungyou.com/posts/note002-rodrigues-formula},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nKisung You. 2022. “Rodrigues’ Formula for the Legendre\nPolynomials.” August 10, 2022. https://kisungyou.com/posts/note002-rodrigues-formula."
  },
  {
    "objectID": "posts/note001-spherical-distance/index.html",
    "href": "posts/note001-spherical-distance/index.html",
    "title": "Monte Carlo computation of \\(L_p\\) distance between two densities on the unit hypersphere",
    "section": "",
    "text": "Computation\nImportance sampling requires a proposal density. The easiest choice is to use uniform density \\(u(x)\\) as an importance proposal since sampling from \\(u(x)\\) is trivial. First, take a random sample from standard normal distribution \\(x \\sim \\mathcal{N}(0,I)\\) in \\(\\mathbb{R}^{d+1}\\). Then, the rest is to take \\(L_2\\) normalization, i.e., \\(x \\leftarrow x / \\|x\\|_2\\), which makes a sampled vector to have a unit norm. Given the sample generation process, we have the following\n\\[\\begin{aligned}\nL_p (f,g)^p &= \\int_{\\mathbb{S}^d} |f(x)-g(x)|^p dx \\\\\n&= \\int_{\\mathbb{S}^d} \\frac{|f(x)-g(x)|^p}{u(x)} u(x)  dx \\\\\n&= \\mathbb{E}_{u(x)} \\left\\lbrack \\frac{|f(x)-g(x)|^p}{u(x)} \\right\\rbrack\\\\\n&\\approx \\frac{1}{N} \\sum_{n=1}^N \\frac{|f(x)-g(x)|^p}{u(x)} \\,\\,\\textrm{for}\\,\\, x_n \\overset{iid}{\\sim} u(x),\n\\end{aligned}\n\\] where the last term gets better approximation as \\(N\\rightarrow \\infty\\).\n\n\nReferences\n\n\nMarron, James Stephen, and I. L. Dryden. 2021. Object Oriented Data Analysis. Boca Raton: Taylor & Francis Group, LLC.\n\n\n\n\n\n\nCitationBibTeX citation:@online{you2022,\n  author = {Kisung You},\n  title = {Monte {Carlo} Computation of {\\$L\\_p\\$} Distance Between Two\n    Densities on the Unit Hypersphere},\n  date = {2022-08-09},\n  url = {https://kisungyou.com/posts/note001-spherical-distance},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nKisung You. 2022. “Monte Carlo Computation of $L_p$ Distance\nBetween Two Densities on the Unit Hypersphere.” August 9, 2022.\nhttps://kisungyou.com/posts/note001-spherical-distance."
  },
  {
    "objectID": "posts/note003-angular-gaussian/index.html",
    "href": "posts/note003-angular-gaussian/index.html",
    "title": "A Note on Angular Central Gaussian Distribution and its Matrix Variant",
    "section": "",
    "text": "Angular Central Gaussian Distribution\nOn \\(\\mathbb{S}^{p-1}\\), the ACG distribution \\(ACG_p (A)\\) as a density\n\\[\nf_{ACG} (x\\vert A) = |A|^{-1/2} (x^\\top Ax)^{-p/2}\n\\]\nfor \\(x \\in \\mathbb{S}^{p-1}\\) and \\(A\\) a symmetric positive-definite matrix, i.e., \\(A=A^\\top \\in \\mathbb{R}^{p\\times p}\\) with \\(\\lambda_{min}(A)>0\\). Let’s recap some properties of ACG distribution.\n\nProperty 1. \\(f_{ACG}(x|A) = f_{ACG}(-x|A)\\). This enables ACG as a distribution on the real projective space \\(\\mathbb{R}P^{p-1} = \\mathbb{S}^{p-1}/\\lbrace +1, -1 \\rbrace\\).\nProperty 2. \\(f_{ACG}(x|A) = f_{ACG}(x|cA),~c>0\\). Common convention is to normalize the matrix \\(A\\) by a constraint \\(\\textrm{tr}(A) = p\\), which is useful (or even essential) in maximum likelihood estimation of the parameter to ensure algorithmic stability. If you want to show this property, simply use the fact that \\(|cA| = c^p|A|\\).\nProperty 3. When \\(x\\sim \\mathcal{N}_p (0,A) \\rightarrow x/\\|x\\| \\sim ACG_p (A)\\). This property is indeed an intuition behind its origination (Tyler 1987), which can be used for sampling.\n\n\nMaximum Likelihood Estimation\nGiven a random sample \\(x_1, \\ldots, x_p \\sim ACG_p (A)\\), Tyler (1987) proposed an iterative updating scheme to estimate the parameter \\(A\\) by\n\n\\[\\hat{A}_{k+1} = p \\left\\lbrace \\sum_{i=1}^n \\frac{1}{x_i^\\top \\hat{A}_k^{-1} x_i} \\right\\rbrace^{-1} \\sum_{i=1}^n \\frac{x_i x_i^\\top}{x_i^\\top \\hat{A}_k^{-1} x_i}, \\tag{1}\\]\n\nwhere \\(\\hat{A}_k\\) is the \\(k\\)-th iterate of an estimator with an initial starting point of an identity matrix \\(\\hat{A}_0 = I_p\\). While Equation 1 guarantees the convergence under mild conditions and abides by the constraint \\(\\textrm{tr}(\\hat{A}_k) = p\\), it is from the author’s previous work on \\(M\\)-estimation of the scatter matrix. Here, we provide a naive derivation of 2-step fixed-point iteration algorithm for pedagogical purpose.\n\n\\[\\hat{A}_{k'} = \\frac{p}{n}\\sum_{i=1}^n \\frac{x_i x_i^\\top}{x_i^\\top \\hat{A}_k^{-1} x_i}\\,\\,\\textrm{and}\\,\\, \\hat{A}_{k+1} = \\frac{p}{\\textrm{tr}(\\hat{A}_{k'})} \\hat{A}_{k'}. \\tag{2}\\]\n\nFirst, let’s write the log-likelihood\n\\[\n\\log L = -\\frac{n}{2}\\log\\det(A) - \\frac{p}{2} \\sum_{i=1}^n \\log (x_i^\\top A^{-1} x_i),\n\\]\nand recall two facts from matrix calculus (Petersen and Pedersen 2012) that\n\\[\n\\frac{\\partial \\log\\det(A)}{\\partial A} = A^{-1}\\,\\,\\textrm{and}\\,\\, \\frac{\\partial x^\\top A^{-1} x}{\\partial A} = -A^{-1}xx^\\top A^{-1}.\n\\]\nThen, the first-order condition for the log-likelihood can be written as\n\\[\n\\begin{gather*}\n    \\frac{\\partial \\log L}{\\partial A} = -\\frac{n}{2} A^{-1} + \\frac{p}{2} \\sum_{i=1}^n \\frac{A^{-1} x_i x_i^\\top A^{-1}}{x_i^\\top A^{-1} x_i} \\\\\n    A^{-1} = \\frac{p}{n} \\sum_{i=1}^n \\frac{A^{-1} x_i x_i^\\top A^{-1}}{x_i^\\top A^{-1} x_i} \\\\\n    A = \\frac{p}{n} \\sum_{i=1}^n \\frac{ x_i x_i^\\top }{x_i^\\top A^{-1} x_i}\n\\end{gather*}\n\\]\nwhere the last equality comes from multiplying \\(A\\) from left and right. Therefore, \\(\\hat{A}\\) is a solution of the matrix equation in a form \\(X = f(X)\\) where \\(f\\) is a contraction mapping under some conditions (Tyler 1987). This leads to Equation 2 while projection step is added to keep \\(\\text{tr}(\\hat{A}_k) = p\\) for all \\(k=1,2,\\cdots\\).\n\n\n\nMatrix Angular Central Gaussian Distribution\nChikuse (1990) extended the distribution to the matrix case, namely Stiefel and Grassmann manifolds\n\\[\n\\begin{gather*}\n    St(p,r) = \\{X\\in \\mathbb{R}^{p\\times r} ~\\vert~ X^\\top X = I_p\\}\\\\\n    Gr(p,r) = \\{\\text{Span}(X) ~\\vert~ X \\in \\mathbb{R}^{p\\times r},~\\text{rank}(X)=r\\}\n\\end{gather*}\n\\]\nwhich are sets of orthonormal \\(k\\)-frames and \\(k\\)-subspaces. The Matrix Angular Central Gaussian (MACG) distribution \\(MACG_{p,r}(\\Sigma)\\) has a density\n\\[\nf_{MACG}(X\\vert \\Sigma) = |\\Sigma|^{-r/2} |X^\\top \\Sigma^{-1} X|^{-p/2}\n\\]\nwhere \\(\\Sigma\\) is a symmetric positive-definite matrix. Note that the density is very similar to what we had before for vector-valued distribution. Likewise, it shares properties as before.\n\nProperty 1. \\(f_{MACG}(X|\\Sigma) = f_{MACG}(-X|\\Sigma)\\).\nProperty 2. \\(f_{MACG}(X|\\Sigma) = f_{MACG}(X|c\\Sigma),~c>0\\).\nProperty 3. \\(f_{MACG}(X|\\Sigma) = f_{MACG}(XR|\\Sigma)\\) for \\(R\\in O(r)\\). This property enables to consider MACG as a distribution on Grassmann manifold, which are quotient by modulo orthogonal transformation.\n\n\nSampling from MACG\nIn order to draw random samples from \\(MACG_{p,r}(\\Sigma)\\), we need the following steps, which are common in directional statistics with Stiefel/Grassmann manifolds . First, draw \\(r\\) random vectors \\(x_1,\\ldots,x_r \\sim \\mathcal{N}_p (0,\\Sigma)\\) and stack them as columns \\(X=[x_1|\\cdots|x_r] \\in \\mathbb{R}^{p\\times r}\\). Then,\n\\[\n    Y = X (X^\\top X)^{-1/2} \\sim MACG_{p,r}(\\Sigma)\n\\]\nwhere the negative square root for a symmetric positive-definite matrix can be obtained from eigen-decomposition,\n\\[\n\\begin{gather*}\n\\Omega = UDU^\\top \\rightarrow \\Omega^{-1/2} = UD^{-1/2} U^\\top \\\\\n\\left[D^{-1/2}\\right]_{ij} = \\frac{1}{\\sqrt{d_{ij}}} \\textrm{ when } i = j \\textrm{ and }0\\textrm{ otherwise.}\n\\end{gather*}\n\\]\n\n\nMaximum Likelihood Estimation\nSimilar to the ACG case, given a random sample \\(X_1,X_2,\\ldots,X_n \\sim MACG_{p,r}(\\Sigma)\\), we can obtain a two-step iterative scheme to estimate the parameter \\(\\Sigma\\),\n\n\\[\\begin{gather*}\n\\hat{\\Sigma}_{k'} = \\frac{p}{nr} \\sum_{i=1}^n\nX_i (X_i^\\top \\Sigma^{-1} X_i)^{-1} X_i \\\\\n\\hat{\\Sigma}_{k+1} =  \\frac{p}{\\text{tr}(\\hat{\\Sigma}_{k'})} \\hat{\\Sigma}_{k'}.\\end{gather*} \\tag{3}\\]\n\nDerivation of formula Equation 3 follows the similar line of before. We need another fact from matrix calculus that\n\\[\n\\frac{\\partial }{\\partial \\Sigma} \\log\\det(X^\\top \\Sigma^{-1} X) = - \\Sigma^{-1} X (X^\\top \\Sigma^{-1} X)^{-1} X^\\top \\Sigma^{-1}.\n\\]\nFirst, log-likelihood is written as\n\\[\n    \\log L  = -\\frac{nr}{2}\\log\\det(\\Sigma) - \\frac{p}{2}\\sum_{i=1}^n \\log\\det (X_i^\\top \\Sigma^{-1} X_i)\n\\]\nwhere the first-order condition gives\n\\[\n\\begin{gather*}\n    \\frac{\\partial \\log L}{\\partial \\Sigma} = -\\frac{nr}{2}\\Sigma^{-1} + \\frac{p}{2}\\sum_{i=1}^n \\left( \\Sigma^{-1} X_i (X_i^\\top \\Sigma^{-1} X_i)^{-1} X_i^\\top \\Sigma^{-1} \\right)\\\\\n    \\frac{nr}{2} \\Sigma^{-1} = \\frac{p}{2}\\sum_{i=1}^n \\left( \\Sigma^{-1} X_i (X_i^\\top \\Sigma^{-1} X_i)^{-1} X_i^\\top \\Sigma^{-1} \\right) \\\\\n    \\Sigma = \\frac{p}{nr} \\sum_{i=1}^n X_i (X_i^\\top \\Sigma^{-1} X_i)^{-1} X_i^\\top\n\\end{gather*}\n\\]\nwhere the last equality comes from multiplying \\(\\Sigma\\) from left and right. Therefore, \\(\\hat{\\Sigma}\\) is a solution of the matrix equation, leading to the formula of Equation 3 with an additional projection step to keep \\(\\text{tr}(\\hat{\\Sigma}_k) = p\\) for all \\(k=1,2,\\cdots\\). Note that this matrix equation, up to my knowledge, has not known whether the mapping is contraction or not.\n\n\n\nConclusion\nACG and MACG distributions are simple yet rather little used in directional statistics. We hope that this brief note boosts probabilistic inference on corresponding manifolds at ease. An R package Riemann, which is also available on CRAN, implements density evaluation, random sample generation, and maximum likelihood estimation of the scatter parameters \\(A\\) and \\(\\Sigma\\) in the light of expecting handy utilization of the distributions we introduced.\n\n\nReferences\n\n\nChikuse, Yasuko. 1990. “The Matrix Angular Central Gaussian Distribution.” Journal of Multivariate Analysis 33 (2): 265–74. https://doi.org/10.1016/0047-259X(90)90050-R.\n\n\nPetersen, K. B., and M. S. Pedersen. 2012. “The Matrix Cookbook.” Technical University of Denmark.\n\n\nTyler, David E. 1987. “Statistical Analysis for the Angular Central Gaussian Distribution on the Sphere.” Biometrika 74 (3): 579–89. https://doi.org/10.1093/biomet/74.3.579.\n\n\n\n\n\n\nCitationBibTeX citation:@online{you2022,\n  author = {Kisung You},\n  title = {A {Note} on {Angular} {Central} {Gaussian} {Distribution} and\n    Its {Matrix} {Variant}},\n  date = {2022-08-12},\n  url = {https://kisungyou.com/posts/note003-angular-gaussian},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nKisung You. 2022. “A Note on Angular Central Gaussian Distribution\nand Its Matrix Variant.” August 12, 2022. https://kisungyou.com/posts/note003-angular-gaussian."
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Software",
    "section": "",
    "text": "Riemann\nLearning with Data on Riemannian manifold\n\n        \n        Website\n      \n        \n        View on Github\n      \n        \n        View on CRAN\n    \nRiemBase\nFunctions and C++ Headers for Computation on Manifolds\n\n        \n        View on Github\n      \n        \n        View on CRAN\n    \nTDAkit\nToolkit for Topological Data Analysis\n\n        \n        View on Github\n      \n        \n        View on CRAN\n    \nNetwork\ngraphon\nA Collection of Graphon Estimation Methods\n\n        \n        View on Github\n      \n        \n        View on CRAN\n    \nNetworkDistance\nDistance Measures for Networks\n\n        \n        View on Github\n      \n        \n        View on CRAN\n    \nStatistical Inference\nCovTools\nStatistical Tools for Covariance Analysis\n\n        \n        View on Github\n      \n        \n        View on CRAN\n    \nfilling\nMatrix Completion, Imputation, and Inpainting Methods\n\n        \n        View on Github\n      \n        \n        View on CRAN\n    \nRdimtools\nDimension Reduction and Estimation Methods\n\n        \n        Website\n      \n        \n        View on Github\n      \n        \n        View on CRAN\n    \nSBmedian\nScalable Bayes with Median of Subset Posteriors\n\n        \n        View on Github\n      \n        \n        View on CRAN\n    \nSHT\nStatistical Hypothesis Testing Toolbox\n\n        \n        Website\n      \n        \n        View on Github\n      \n        \n        View on CRAN\n    \nT4cluster\nTools for Cluster Analysis\n\n        \n        Website\n      \n        \n        View on Github\n      \n        \n        View on CRAN\n    \nOptimization and Applied Mathematics\nmaotai\nTools for Matrix Algebra, Optimization and Inference\n\n        \n        Website\n      \n        \n        View on Github\n      \n        \n        View on CRAN\n    \nRlinsolve\nIterative Solvers for (Sparse) Linear System of Equations\n\n        \n        View on Github\n      \n        \n        View on CRAN\n    \nT4transport\nTools for Computational Optimal Transport\n\n        \n        Website\n      \n        \n        View on Github\n      \n        \n        View on CRAN\n    \ntvR\nTotal Variation Regularization for Signals and Images\n\n        \n        View on Github\n      \n        \n        View on CRAN"
  }
]