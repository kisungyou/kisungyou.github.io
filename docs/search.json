[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Kisung You",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n  \n\n\n\n\nWhat is Fisher-Rao distance?\n\n\n\n\n\n\n\n\n\nFeb 5, 2025\n\n\n6 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notes-teaching/note03-angular-gaussian.html",
    "href": "notes-teaching/note03-angular-gaussian.html",
    "title": "Angular Central Gaussian Distribution and its Matrix Variant",
    "section": "",
    "text": "Probability distribution with explicit forms of densities are core elements of statistical inference. In this post, we review angular central gaussian (ACG) distribution on a unit hypersphere \\(\\mathbb{S}^{p-1} \\subset \\mathbb{R}^p\\) and its extension - matrix angular central gaussian (MACG) - defined on Stiefel \\(St(p,r)\\) and Grassman \\(Gr(p,r)\\) manifolds."
  },
  {
    "objectID": "notes-teaching/note03-angular-gaussian.html#introduction",
    "href": "notes-teaching/note03-angular-gaussian.html#introduction",
    "title": "Angular Central Gaussian Distribution and its Matrix Variant",
    "section": "",
    "text": "Probability distribution with explicit forms of densities are core elements of statistical inference. In this post, we review angular central gaussian (ACG) distribution on a unit hypersphere \\(\\mathbb{S}^{p-1} \\subset \\mathbb{R}^p\\) and its extension - matrix angular central gaussian (MACG) - defined on Stiefel \\(St(p,r)\\) and Grassman \\(Gr(p,r)\\) manifolds."
  },
  {
    "objectID": "notes-teaching/note03-angular-gaussian.html#angular-central-gaussian-distribution",
    "href": "notes-teaching/note03-angular-gaussian.html#angular-central-gaussian-distribution",
    "title": "Angular Central Gaussian Distribution and its Matrix Variant",
    "section": "Angular Central Gaussian Distribution",
    "text": "Angular Central Gaussian Distribution\nOn \\(\\mathbb{S}^{p-1}\\), the ACG distribution \\(ACG_p (A)\\) as a density \\[\nf_{ACG} (x\\vert A) = |A|^{-1/2} (x^\\top Ax)^{-p/2}\n\\] for \\(x \\in \\mathbb{S}^{p-1}\\) and \\(A\\) a symmetric positive-definite matrix, i.e., \\(A=A^\\top \\in \\mathbb{R}^{p\\times p}\\) with \\(\\lambda_{min}(A)&gt;0\\). Let’s recap some properties of ACG distribution.\n\nProperty 1. \\(f_{ACG}(x|A) = f_{ACG}(-x|A)\\). This enables ACG as a distribution on the real projective space \\(\\mathbb{R}P^{p-1} = \\mathbb{S}^{p-1}/\\lbrace +1, -1 \\rbrace\\).\nProperty 2. \\(f_{ACG}(x|A) = f_{ACG}(x|cA),~c&gt;0\\). Common convention is to normalize the matrix \\(A\\) by a constraint \\(\\textrm{tr}(A) = p\\), which is useful (or even essential) in maximum likelihood estimation of the parameter to ensure algorithmic stability. If you want to show this property, simply use the fact that \\(|cA| = c^p|A|\\).\nProperty 3. When \\(x\\sim \\mathcal{N}_p (0,A) \\rightarrow x/\\|x\\| \\sim ACG_p (A)\\). This property is indeed an intuition behind its origination (Tyler 1987), which can be used for sampling.\n\n\nMaximum Likelihood Estimation\nGiven a random sample \\(x_1, \\ldots, x_p \\sim ACG_p (A)\\), Tyler (1987) proposed an iterative updating scheme to estimate the parameter \\(A\\) by\n\n\\[\\hat{A}_{k+1} = p \\left\\lbrace \\sum_{i=1}^n \\frac{1}{x_i^\\top \\hat{A}_k^{-1} x_i} \\right\\rbrace^{-1} \\sum_{i=1}^n \\frac{x_i x_i^\\top}{x_i^\\top \\hat{A}_k^{-1} x_i}, \\tag{1}\\]\n\nwhere \\(\\hat{A}_k\\) is the \\(k\\)-th iterate of an estimator with an initial starting point of an identity matrix \\(\\hat{A}_0 = I_p\\). While Equation 1 guarantees the convergence under mild conditions and abides by the constraint \\(\\textrm{tr}(\\hat{A}_k) = p\\), it is from the author’s previous work on \\(M\\)-estimation of the scatter matrix. Here, we provide a naive derivation of 2-step fixed-point iteration algorithm for pedagogical purpose.\n\n\\[\\hat{A}_{k'} = \\frac{p}{n}\\sum_{i=1}^n \\frac{x_i x_i^\\top}{x_i^\\top \\hat{A}_k^{-1} x_i}\\,\\,\\textrm{and}\\,\\, \\hat{A}_{k+1} = \\frac{p}{\\textrm{tr}(\\hat{A}_{k'})} \\hat{A}_{k'}. \\tag{2}\\]\n\nFirst, let’s write the log-likelihood \\[\n\\log L = -\\frac{n}{2}\\log\\det(A) - \\frac{p}{2} \\sum_{i=1}^n \\log (x_i^\\top A^{-1} x_i),\n\\] and recall two facts from matrix calculus (Petersen and Pedersen 2012) that \\[\n\\frac{\\partial \\log\\det(A)}{\\partial A} = A^{-1}\\,\\,\\textrm{and}\\,\\, \\frac{\\partial x^\\top A^{-1} x}{\\partial A} = -A^{-1}xx^\\top A^{-1}.\n\\] Then, the first-order condition for the log-likelihood can be written as\n\\[\n\\begin{gather*}\n    \\frac{\\partial \\log L}{\\partial A} = -\\frac{n}{2} A^{-1} + \\frac{p}{2} \\sum_{i=1}^n \\frac{A^{-1} x_i x_i^\\top A^{-1}}{x_i^\\top A^{-1} x_i} \\\\\n    A^{-1} = \\frac{p}{n} \\sum_{i=1}^n \\frac{A^{-1} x_i x_i^\\top A^{-1}}{x_i^\\top A^{-1} x_i} \\\\\n    A = \\frac{p}{n} \\sum_{i=1}^n \\frac{ x_i x_i^\\top }{x_i^\\top A^{-1} x_i}\n\\end{gather*}\n\\] where the last equality comes from multiplying \\(A\\) from left and right. Therefore, \\(\\hat{A}\\) is a solution of the matrix equation in a form \\(X = f(X)\\) where \\(f\\) is a contraction mapping under some conditions (Tyler 1987). This leads to Equation 2 while projection step is added to keep \\(\\text{tr}(\\hat{A}_k) = p\\) for all \\(k=1,2,\\cdots\\)."
  },
  {
    "objectID": "notes-teaching/note03-angular-gaussian.html#matrix-angular-central-gaussian-distribution",
    "href": "notes-teaching/note03-angular-gaussian.html#matrix-angular-central-gaussian-distribution",
    "title": "Angular Central Gaussian Distribution and its Matrix Variant",
    "section": "Matrix Angular Central Gaussian Distribution",
    "text": "Matrix Angular Central Gaussian Distribution\nChikuse (1990) extended the distribution to the matrix case, namely Stiefel and Grassmann manifolds \\[\n\\begin{gather*}\n    St(p,r) = \\{X\\in \\mathbb{R}^{p\\times r} ~\\vert~ X^\\top X = I_p\\}\\\\\n    Gr(p,r) = \\{\\text{Span}(X) ~\\vert~ X \\in \\mathbb{R}^{p\\times r},~\\text{rank}(X)=r\\}\n\\end{gather*}\n\\] which are sets of orthonormal \\(k\\)-frames and \\(k\\)-subspaces. The Matrix Angular Central Gaussian (MACG) distribution \\(MACG_{p,r}(\\Sigma)\\) has a density \\[\nf_{MACG}(X\\vert \\Sigma) = |\\Sigma|^{-r/2} |X^\\top \\Sigma^{-1} X|^{-p/2}\n\\] where \\(\\Sigma\\) is a symmetric positive-definite matrix. Note that the density is very similar to what we had before for vector-valued distribution. Likewise, it shares properties as before.\n\nProperty 1. \\(f_{MACG}(X|\\Sigma) = f_{MACG}(-X|\\Sigma)\\).\nProperty 2. \\(f_{MACG}(X|\\Sigma) = f_{MACG}(X|c\\Sigma),~c&gt;0\\).\nProperty 3. \\(f_{MACG}(X|\\Sigma) = f_{MACG}(XR|\\Sigma)\\) for \\(R\\in O(r)\\). This property enables to consider MACG as a distribution on Grassmann manifold, which are quotient by modulo orthogonal transformation.\n\n\nSampling from MACG\nIn order to draw random samples from \\(MACG_{p,r}(\\Sigma)\\), we need the following steps, which are common in directional statistics with Stiefel/Grassmann manifolds . First, draw \\(r\\) random vectors \\(x_1,\\ldots,x_r \\sim \\mathcal{N}_p (0,\\Sigma)\\) and stack them as columns \\(X=[x_1|\\cdots|x_r] \\in \\mathbb{R}^{p\\times r}\\). Then, \\[\n    Y = X (X^\\top X)^{-1/2} \\sim MACG_{p,r}(\\Sigma)\n\\] where the negative square root for a symmetric positive-definite matrix can be obtained from eigen-decomposition, \\[\n\\begin{gather*}\n\\Omega = UDU^\\top \\rightarrow \\Omega^{-1/2} = UD^{-1/2} U^\\top \\\\\n\\left[D^{-1/2}\\right]_{ij} = \\frac{1}{\\sqrt{d_{ij}}} \\textrm{ when } i = j \\textrm{ and }0\\textrm{ otherwise.}\n\\end{gather*}\n\\]\n\n\nMaximum Likelihood Estimation\nSimilar to the ACG case, given a random sample \\(X_1,X_2,\\ldots,X_n \\sim MACG_{p,r}(\\Sigma)\\), we can obtain a two-step iterative scheme to estimate the parameter \\(\\Sigma\\),\n\n\\[\\begin{gather*}\n\\hat{\\Sigma}_{k'} = \\frac{p}{nr} \\sum_{i=1}^n\nX_i (X_i^\\top \\Sigma^{-1} X_i)^{-1} X_i \\\\\n\\hat{\\Sigma}_{k+1} =  \\frac{p}{\\text{tr}(\\hat{\\Sigma}_{k'})} \\hat{\\Sigma}_{k'}.\\end{gather*} \\tag{3}\\]\n\nDerivation of formula Equation 3 follows the similar line of before. We need another fact from matrix calculus that \\[\n\\frac{\\partial }{\\partial \\Sigma} \\log\\det(X^\\top \\Sigma^{-1} X) = - \\Sigma^{-1} X (X^\\top \\Sigma^{-1} X)^{-1} X^\\top \\Sigma^{-1}.\n\\] First, log-likelihood is written as \\[\n    \\log L  = -\\frac{nr}{2}\\log\\det(\\Sigma) - \\frac{p}{2}\\sum_{i=1}^n \\log\\det (X_i^\\top \\Sigma^{-1} X_i)\n\\] where the first-order condition gives \\[\n\\begin{gather*}\n    \\frac{\\partial \\log L}{\\partial \\Sigma} = -\\frac{nr}{2}\\Sigma^{-1} + \\frac{p}{2}\\sum_{i=1}^n \\left( \\Sigma^{-1} X_i (X_i^\\top \\Sigma^{-1} X_i)^{-1} X_i^\\top \\Sigma^{-1} \\right)\\\\\n    \\frac{nr}{2} \\Sigma^{-1} = \\frac{p}{2}\\sum_{i=1}^n \\left( \\Sigma^{-1} X_i (X_i^\\top \\Sigma^{-1} X_i)^{-1} X_i^\\top \\Sigma^{-1} \\right) \\\\\n    \\Sigma = \\frac{p}{nr} \\sum_{i=1}^n X_i (X_i^\\top \\Sigma^{-1} X_i)^{-1} X_i^\\top\n\\end{gather*}\n\\] where the last equality comes from multiplying \\(\\Sigma\\) from left and right. Therefore, \\(\\hat{\\Sigma}\\) is a solution of the matrix equation, leading to the formula of Equation 3 with an additional projection step to keep \\(\\text{tr}(\\hat{\\Sigma}_k) = p\\) for all \\(k=1,2,\\cdots\\). Note that this matrix equation, up to my knowledge, has not known whether the mapping is contraction or not."
  },
  {
    "objectID": "notes-teaching/note03-angular-gaussian.html#conclusion",
    "href": "notes-teaching/note03-angular-gaussian.html#conclusion",
    "title": "Angular Central Gaussian Distribution and its Matrix Variant",
    "section": "Conclusion",
    "text": "Conclusion\nACG and MACG distributions are simple yet rather little used in directional statistics. We hope that this brief note boosts probabilistic inference on corresponding manifolds at ease. An R package Riemann, which is also available on CRAN, implements density evaluation, random sample generation, and maximum likelihood estimation of the scatter parameters \\(A\\) and \\(\\Sigma\\) in the light of expecting handy utilization of the distributions we introduced."
  },
  {
    "objectID": "notes-teaching/note03-angular-gaussian.html#references",
    "href": "notes-teaching/note03-angular-gaussian.html#references",
    "title": "Angular Central Gaussian Distribution and its Matrix Variant",
    "section": "References",
    "text": "References\n\n\nChikuse, Yasuko. 1990. “The Matrix Angular Central Gaussian Distribution.” Journal of Multivariate Analysis 33 (2): 265–74. https://doi.org/10.1016/0047-259X(90)90050-R.\n\n\nPetersen, K. B., and M. S. Pedersen. 2012. “The Matrix Cookbook.” Technical University of Denmark.\n\n\nTyler, David E. 1987. “Statistical Analysis for the Angular Central Gaussian Distribution on the Sphere.” Biometrika 74 (3): 579–89. https://doi.org/10.1093/biomet/74.3.579."
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Software",
    "section": "",
    "text": "Software\nGeometry and Topology\nRiemann\nLearning with Data on Riemannian manifold\n\n        \n        Website\n      \n        \n        View on Github\n      \n        \n        View on CRAN\n    \nRiemBase\nFunctions and C++ Headers for Computation on Manifolds\n\n        \n        View on Github\n      \n        \n        View on CRAN\n    \nTDAkit\nToolkit for Topological Data Analysis\n\n        \n        View on Github\n      \n        \n        View on CRAN\n    \nNetwork\ngraphon\nA Collection of Graphon Estimation Methods\n\n        \n        View on Github\n      \n        \n        View on CRAN\n    \nNetworkDistance\nDistance Measures for Networks\n\n        \n        View on Github\n      \n        \n        View on CRAN\n    \nStatistical Inference\nCovTools\nStatistical Tools for Covariance Analysis\n\n        \n        View on Github\n      \n        \n        View on CRAN\n    \nfilling\nMatrix Completion, Imputation, and Inpainting Methods\n\n        \n        View on Github\n      \n        \n        View on CRAN\n    \nRdimtools\nDimension Reduction and Estimation Methods\n\n        \n        Website\n      \n        \n        View on Github\n      \n        \n        View on CRAN\n    \nSBmedian\nScalable Bayes with Median of Subset Posteriors\n\n        \n        View on Github\n      \n        \n        View on CRAN\n    \nSHT\nStatistical Hypothesis Testing Toolbox\n\n        \n        Website\n      \n        \n        View on Github\n      \n        \n        View on CRAN\n    \nT4cluster\nTools for Cluster Analysis\n\n        \n        Website\n      \n        \n        View on Github\n      \n        \n        View on CRAN\n    \nOptimization and Applied Mathematics\nmaotai\nTools for Matrix Algebra, Optimization and Inference\n\n        \n        Website\n      \n        \n        View on Github\n      \n        \n        View on CRAN\n    \nRlinsolve\nIterative Solvers for (Sparse) Linear System of Equations\n\n        \n        View on Github\n      \n        \n        View on CRAN\n    \nT4transport\nTools for Computational Optimal Transport\n\n        \n        Website\n      \n        \n        View on Github\n      \n        \n        View on CRAN\n    \ntvR\nTotal Variation Regularization for Signals and Images\n\n        \n        View on Github\n      \n        \n        View on CRAN"
  },
  {
    "objectID": "notes-blog/blog_001_FisherRao.html",
    "href": "notes-blog/blog_001_FisherRao.html",
    "title": "What is Fisher-Rao distance?",
    "section": "",
    "text": "Consider a parametric family of probability distributions \\(\\lbrace p(x\\vert \\theta)\\rbrace\\) with a parameter \\(\\theta \\in \\Theta \\subset \\mathbb{R}^d\\). Given two distributions, \\(p(x\\vert \\theta_1)\\) and \\(p(x\\vert \\theta_2)\\), a natural question arises: “How different are they?” In this post, I introduce the Fisher-Rao distance, a measure of dissimilarity between probability distributions based on the Fisher information matrix."
  },
  {
    "objectID": "notes-blog/blog_001_FisherRao.html#fisher-information-matrix",
    "href": "notes-blog/blog_001_FisherRao.html#fisher-information-matrix",
    "title": "What is Fisher-Rao distance?",
    "section": "Fisher Information Matrix",
    "text": "Fisher Information Matrix\nIn a standard mathematical statistics course, one encounters the Fisher information matrix (FIM) for a probability density function \\(p(x\\vert \\theta)\\), defined as: \\[\\begin{equation*}\nI(\\theta) = \\mathbb{E} \\left[\n\\left( \\frac{\\partial}{\\partial \\theta} \\log p(x\\vert \\theta) \\right) \\left( \\frac{\\partial}{\\partial \\theta} \\log p(x\\vert \\theta) \\right)^\\top\n\\right].\n\\end{equation*}\\] Alternatively, it can be expressed in terms of second derivatives: \\[\\begin{equation*}\nI(\\theta) = -\\mathbb{E} \\left[\\frac{\\partial^2 \\log p(x\\vert \\theta)}{\\partial \\theta \\partial \\theta^\\top}\\right].\n\\end{equation*}\\]\nThe Fisher information matrix plays a central role in statistical inference. In maximum likelihood estimation (MLE), it helps assess the asymptotic variance of the MLE: \\[\\begin{equation}\n\\sqrt{n} (\\hat{\\theta}_n - \\theta) \\xrightarrow{d} \\mathcal{N}(0, I^{-1}(\\theta)).\n\\end{equation}\\] The Cramér-Rao bound states that for any unbiased estimator \\(\\hat{\\theta}\\): \\[\\begin{equation}\n\\textrm{Cov}(\\hat{\\theta}) \\succeq I^{-1}(\\theta),\n\\end{equation}\\] where \\(A \\succeq B\\) indicates that \\(A-B\\) is positive semi-definite. In other words, the covariance matrix of any unbiased estimator is bounded below by the inverse of the FIM. In Bayesian statistics, the FIM is related to constructing priors1. The Jeffreys prior, an invariant and non-informative prior, is given by: \\[\\begin{equation}\n\\pi(\\theta) \\propto \\sqrt{\\textrm{det}(I(\\theta))},\n\\end{equation}\\] which is an objective, non-information prior that is invariant under reparametrization."
  },
  {
    "objectID": "notes-blog/blog_001_FisherRao.html#fisher-rao-metric",
    "href": "notes-blog/blog_001_FisherRao.html#fisher-rao-metric",
    "title": "What is Fisher-Rao distance?",
    "section": "Fisher-Rao Metric",
    "text": "Fisher-Rao Metric\nA key discovery in information geometry (Amari et al. 2007) is that the FIM induces a Riemannian metric on the statistical manifold \\(\\mathcal{M} = \\lbrace p(x\\vert \\theta) \\rbrace\\), known as the Fisher-Rao metric. Given an infinitesimal displacement \\(d\\theta\\) in the parameter space \\(\\Theta\\), the squared length of the displacement under the Fisher-Rao metric is given by: \\[\\begin{equation}\nds^2 = d\\theta^\\top I(\\theta) d\\theta.\n\\end{equation}\\] Thus, the Fisher-Rao metric captures the local geometry of the statistical manifold by taking the FIM as the local matrix form in a given coordinate system."
  },
  {
    "objectID": "notes-blog/blog_001_FisherRao.html#fisher-rao-distance",
    "href": "notes-blog/blog_001_FisherRao.html#fisher-rao-distance",
    "title": "What is Fisher-Rao distance?",
    "section": "Fisher-Rao Distance",
    "text": "Fisher-Rao Distance\nSince the Fisher-Rao metric defines a Riemannian structure, one can define a distance between two points in the manifold. The Fisher-Rao distance between two distributions, \\(p_1(x) = p(x\\vert \\theta_1)\\) and \\(p_2(x) = p(x\\vert \\theta_2)\\), is the geodesic length connecting \\(\\theta_1\\) and \\(\\theta_2\\):\n\\[\nd_{FR}(p_1, p_2):= d_{FR}(\\theta_1, \\theta_2) = \\underset{\\gamma}{\\inf} \\int \\sqrt{\\dot{\\gamma}(t)^\\top I(\\gamma(t)) \\dot{\\gamma}(t)} dt,\n\\tag{1}\\] where the infimum is taken over all smooth curves \\(\\gamma:[0,1] \\rightarrow \\Theta\\) such that \\(\\gamma (0) = \\theta_1\\) and \\(\\gamma (1) = \\theta_2\\). This formulation reveals that the Fisher-Rao distance is a Riemannian geodesic distance."
  },
  {
    "objectID": "notes-blog/blog_001_FisherRao.html#computing-the-fisher-rao-distance",
    "href": "notes-blog/blog_001_FisherRao.html#computing-the-fisher-rao-distance",
    "title": "What is Fisher-Rao distance?",
    "section": "Computing the Fisher-Rao Distance",
    "text": "Computing the Fisher-Rao Distance\nWhile theoretically elegant, computing the Fisher-Rao distance as in Equation 1 is challenging because it requires solving the geodesic equations: \\[\n\\frac{d^2 \\theta^i}{dt^2} + \\sum_{j,k} \\Gamma^i_{jk} \\frac{d\\theta^j}{dt} \\frac{d\\theta^k}{dt} = 0,\n\\] where \\(\\Gamma^i_{jk}\\) are Christoffel symbols derived from the Fisher information metric: \\[\n\\Gamma^i_{jk} = \\frac{1}{2} \\sum_m I^{im} \\left( \\frac{\\partial I_{mj}}{\\partial \\theta^k} + \\frac{\\partial I_{mk}}{\\partial \\theta^j} - \\frac{\\partial I_{jk}}{\\partial \\theta^m} \\right).\n\\] Exact solutions are known to exist for only a handful of distributions (Miyamoto et al. 2024)."
  },
  {
    "objectID": "notes-blog/blog_001_FisherRao.html#square-root-transformation-and-geodesic-distance",
    "href": "notes-blog/blog_001_FisherRao.html#square-root-transformation-and-geodesic-distance",
    "title": "What is Fisher-Rao distance?",
    "section": "Square-Root Transformation and Geodesic Distance",
    "text": "Square-Root Transformation and Geodesic Distance\nA practical alternative for computing the Fisher-Rao distance is the square-root transformation: \\[\np(x) \\mapsto \\sqrt{p(x)}.\n\\] This procedure embeds probability densities into the Hilbert space \\(L_2\\) with the standard inner product. For \\(f,g \\in L_2\\), the inner product is given by \\[\n\\langle f,g \\rangle = \\int f(x) g(x) dx.\n\\]\nIt is straightforward to verify that the transformed functions lie on the infinite-dimensional unit sphere \\(S^\\infty\\). Specifically, let \\(f \\in L_2\\) be the transformed function of some density \\(\\phi\\), i.e., \\(f(x) = \\sqrt{\\phi(x)}\\). Then, we have \\[\n\\| f \\|^2 = \\langle f, f \\rangle = \\int f(x)^2 dx = \\int \\phi(x) = 1,\n\\] where the last equality follows from the fact that \\(\\phi(x)\\) is a probability density function and thus integrates to 1.\nOn the unit sphere in \\(L^2\\), the geodesic curve between two points is given by the great circle connecting them. Let \\(\\psi_1\\) and \\(\\psi_2\\) be two elements of \\(S^\\infty\\). Then, the geodesic distance between them is canonically known by \\(\\textrm{arccos}(\\langle \\psi_1, \\psi_2 \\rangle)\\)."
  },
  {
    "objectID": "notes-blog/blog_001_FisherRao.html#equivalence-of-fisher-rao-distance-and-geodesic-distance-of-square-root-densities",
    "href": "notes-blog/blog_001_FisherRao.html#equivalence-of-fisher-rao-distance-and-geodesic-distance-of-square-root-densities",
    "title": "What is Fisher-Rao distance?",
    "section": "Equivalence of Fisher-Rao Distance and Geodesic Distance of Square Root Densities",
    "text": "Equivalence of Fisher-Rao Distance and Geodesic Distance of Square Root Densities\nWe are now ready to state the main result of this discussion. The Fisher-Rao distance between two distributions, \\(p_1(x) = p(x\\vert \\theta_1)\\) and \\(p_2(x) = p(x\\vert \\theta_2)\\), is twice the geodesic distance between their square-root transformed densities, \\(\\psi_1(x) = \\sqrt{p_1(x)}\\) and \\(\\psi_2(x) = \\sqrt{p_2(x)}\\), on \\(S^\\infty\\):\n\n\\[d_{FR}(p_1,p_2) = 2 \\textrm{arccos} \\left(\n\\int \\sqrt{p_1 (x) p_2(x)} dx\n\\right) \\tag{2}\\]\n\nTo verify Equation 2, consider an infinitesimal perturbation in \\(p(x)\\) by introducing a small variation: \\[\np(x) \\mapsto p(x) + \\epsilon h(x),\n\\tag{3}\\] where \\(h(x)\\) is an arbitrary density function and \\(\\epsilon &gt; 0\\). Our goal is to quantify how this small variation in \\(p(x)\\) induces a corresponding change in \\(\\psi(x)\\). In other words, we seek to measure the variation in the transformed function. Viewing the right-hand side of Equation 3 as a function of \\(\\epsilon\\), we apply a first-order Taylor expansion to the square-root transformation: \\[\n\\sqrt{p + \\epsilon h} \\approx \\sqrt{p} + \\frac{1}{2} \\frac{\\epsilon h}{\\sqrt{p}} + O(\\epsilon^2).\n\\] Thus, the infinitesimal perturbation in \\(\\psi(x)\\) is: \\[\n\\delta \\psi(x) = \\sqrt{p(x) + \\epsilon h(x)} - \\sqrt{p(x)}\n= \\frac{1}{2}\\frac{h(x)}{\\sqrt{p(x)}} \\epsilon + O(\\epsilon^2).\n\\tag{4}\\]\nFrom Equation 4, we observe that \\(\\delta \\psi(x)\\) is linear in \\(h(x)\\), demonstrating how small changes in \\(p(x)\\) translate to changes in \\(\\psi(x)\\)2. Recall that the Fisher-Rao metric corresponds to infinitesimal displacements in probability space. Given the embedding of densities via the square-root transformation, it is natural to measure perturbations by computing the squared norm: \\[\nds^2 = \\|\\delta \\psi \\|^2 = \\int \\left( \\frac{1}{2}\\frac{h(x)}{\\sqrt{p(x)}} \\right)^2 dx = \\frac{\\epsilon^2}{4} \\int \\frac{h(x)^2}{p(x)} dx.\n\\tag{5}\\]\nSince \\(h(x)\\) represents an infinitesimal change in \\(p(x)\\), we recognize that the integral in Equation 5 corresponds precisely to the Fisher information metric evaluated for an infinitesimal displacement. Consequently, we obtain \\[\nds^2 = \\frac{1}{4} \\int \\frac{h(x)^2}{p(x)} dx,\n\\] which reveals that the Fisher-Rao metric is \\(\\frac{1}{4}\\) times the standard metric on \\(S^\\infty\\) induced by the square-root transformation.\nBy combining these observations with the fact that geodesic distances scale inversely with the metric factor, we conclude that the geodesic distance computed in the Fisher-Rao metric is twice the standard geodesic distance on the unit sphere in \\(L^2\\). That is, for two densities \\(p_1\\) and \\(p_2\\) and their transformations \\(\\phi_1 = \\sqrt{p_1}\\) and \\(\\phi_2 = \\sqrt{p_2}\\) in \\(L_2\\), we have \\[\nd_{FR}(p_1, p_2) = 2 \\times d_{S^\\infty} (\\psi_1, \\psi_2) = 2 \\textrm{arccos} \\left( \\int\n\\sqrt{p_1 (x) p_2 (x)} dx\n\\right).\n\\]"
  },
  {
    "objectID": "notes-blog/blog_001_FisherRao.html#final-thoughts",
    "href": "notes-blog/blog_001_FisherRao.html#final-thoughts",
    "title": "What is Fisher-Rao distance?",
    "section": "Final Thoughts",
    "text": "Final Thoughts\n\nIn the derivation of the Fisher-Rao distance in terms of the scaled geodesic distance on \\(S^\\infty\\), no specific choice or constraint is imposed on \\(\\theta\\). This formulation naturally extends to nonparametric probability densities, as the geodesic distance in \\(L^2\\) depends solely on the embedding.\nDespite the equivalence, some concerns remain regarding the computability of the Fisher-Rao distance. For instance, evaluating the integral can be challenging in many cases, particularly for high-dimensional distributions or arbitrary nonparametric densities, where numerical integration becomes intractable. Approximation via Monte Carlo methods, while useful, presents its own set of difficulties. Moreover, when densities are estimated, they may contain regions of extremely small values, leading to numerical precision issues.\nI would like to express my appreciation to Prof. Marco Radeschi for his kindness and patience in enduring my endless questions in his differential geometry course - many of which, in hindsight, seem absurd."
  },
  {
    "objectID": "notes-blog/blog_001_FisherRao.html#references",
    "href": "notes-blog/blog_001_FisherRao.html#references",
    "title": "What is Fisher-Rao distance?",
    "section": "References",
    "text": "References\n\n\nAmari, Shun’ichi, Hiroshi Nagaoka, Shun’ichi Amari, and Shun’ichi Amari. 2007. Methods of Information Geometry. Translated by Daishi Harada. Nachdruck. Translations of Mathematical Monographs 191. Providence, Rhode Island: American Mathematical Society.\n\n\nMiyamoto, Henrique K., Fábio C. C. Meneghetti, Julianna Pinele, and Sueli I. R. Costa. 2024. “On Closed-Form Expressions for the Fisher-Rao Distance.” Information Geometry 7 (2): 311–54. https://doi.org/10.1007/s41884-024-00143-2."
  },
  {
    "objectID": "notes-blog/blog_001_FisherRao.html#footnotes",
    "href": "notes-blog/blog_001_FisherRao.html#footnotes",
    "title": "What is Fisher-Rao distance?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThere is an interesting connection to the objective Bayesian framework, which will appear in a later post.↩︎\nFrom a geometric point of view, \\(\\delta \\psi(x)\\) belongs to the tangent space of \\(S^\\infty\\) at the point \\(\\psi(x)\\).↩︎"
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "At Baruch, I have taught the following classes.\n\n\n\nCourse\nSemester\n\n\n\n\nMTH 4430 Mathematics of Inferential Statistics\n23F, 24F\n\n\nMTH 4130 Mathematics of Data Analysis\n24F\n\n\nMTH 4125 Introduction to Stochastic Processes\n24S\n\n\nMTH 2003 Precalculus and Elements of Calculus 1A\n23F\n\n\n\n\n\n\n\n\nI maintain a collection of concise technical notes to supplement my classes.\n\n(2022/08/12) Angular Central Gaussian Distribution and its Matrix Variant\n(2022/08/10) Rodrigues’ formula for the Legendre polynomials.\n(2022/08/09) Monte Carlo computation of \\(L_p\\) distance between two densities on the unit hypersphere."
  },
  {
    "objectID": "teaching.html#courses",
    "href": "teaching.html#courses",
    "title": "Teaching",
    "section": "",
    "text": "At Baruch, I have taught the following classes.\n\n\n\nCourse\nSemester\n\n\n\n\nMTH 4430 Mathematics of Inferential Statistics\n23F, 24F\n\n\nMTH 4130 Mathematics of Data Analysis\n24F\n\n\nMTH 4125 Introduction to Stochastic Processes\n24S\n\n\nMTH 2003 Precalculus and Elements of Calculus 1A\n23F"
  },
  {
    "objectID": "teaching.html#miscellaneous-notes",
    "href": "teaching.html#miscellaneous-notes",
    "title": "Teaching",
    "section": "",
    "text": "I maintain a collection of concise technical notes to supplement my classes.\n\n(2022/08/12) Angular Central Gaussian Distribution and its Matrix Variant\n(2022/08/10) Rodrigues’ formula for the Legendre polynomials.\n(2022/08/09) Monte Carlo computation of \\(L_p\\) distance between two densities on the unit hypersphere."
  },
  {
    "objectID": "publication.html",
    "href": "publication.html",
    "title": "Publications",
    "section": "",
    "text": "Publications\n\nPublished\n2024\nHuman-Algorithmic Interaction Using a Large Language Model-Augmented Artificial Intelligence Clinical Decision Support System Niroop Rajashekar, Yeo Eun Shin, Yuan Pu, Sunny Chung, KY, Mauro Giuffrè, Colleen Chan, Theo Saarinen, Allen Hsiao, Jasjeet Sekhon, Ambrose Wong, Leigh Evans, Rene Kizilcec, Loren Laine, Terika Mccall, and Dennis Shung.  CHI Conference on Human Factors in Computing Systems.\n        \n        Publisher's site\n    \nOn the Wasserstein median of probability measures KY, Dennis Shung, and Mauro Giuffrè.  Journal of Computational and Graphical Statistics.\n        \n        Preprint\n     \n        \n        Code\n     \n        \n        Publisher's site\n    \nSystematic review: The use of large language models as medical chatbots in digestive diseases Mauro Giuffrè, Simone Kresevic, KY, Johannes Dupont, Jack Huebner, Alyssa Grimshaw, and Dennis Shung.  Alimentary Pharmacology & Therapeutics.\n        \n        Publisher's site\n    \nBayesian Optimal Two-sample Tests for High-dimensional Gaussian Populations Kyoungjae Lee, KY, and Lizhen Lin.  Bayesian Analysis.\n        \n        Preprint\n     \n        \n        Publisher's site\n    \nAdoption of a gastroenterology hospitalist model and the impact on inpatient endoscopic practice volume: a controlled interrupted time-series analysis Dennis Shung, Darrick Li, KY, Kenneth Hung, Loren Laine, and Michelle Hughes.  iGIE.\n        \n        Publisher's site\n    \nDetection of Gastrointestinal Bleeding with Large Language Models to Aid Quality Improvement and Appropriate Reimbursement Neil Zheng, Vipina Keloth, KY, Daniel Kats, Darrick Li, Ohm Deshpande, Hamita Sachar, Hua Xu, Loren Laine, and Dennis Shung.  Gastroenterology.\n        \n        Publisher's site\n    \nPredicting response to non-selective beta-blockers with liver–spleen stiffness and heart rate in patients with liver cirrhosis and high-risk varices Mauro Giuffrè, Johannes Dupont, Alessia Visintin, Flora Masutti, Fabio Monica, KY, Dennis Shung, Lory Crocè, and The NSBB-Elasto-Response-Prediction Group.  Hepatology International.\n        \n        Publisher's site\n    \nOptimizing large language models in digestive disease: strategies and challenges to improve clinical outcomes Mauro Giuffrè, Simone Kresevic, Nicola Pugliese, KY, and Dennis Shung.  Liver International.\n        \n        Publisher's site\n    \nValidation of an Electronic Health Record–Based Machine Learning Model Compared With Clinical Risk Scores for Gastrointestinal Bleeding Dennis Shung, Colleen Chan, KY, Shinpei Nakamura, Theo Saarinen, Neil Zheng, Michael Simonov, Darrick Li, Cynthia Tsay, Yuki Kawamura, Matthew Shen, Allen Hsiao, Jasjeet Sekhon, and Loren Laine.  Gastroenterology.\n        \n        Publisher's site\n    \n2023\nOn the Spherical Laplace Distribution KY and Dennis Shung.  International Conference on Information Fusion (FUSION).\n        \n        Preprint\n     \n        \n        Publisher's site\n    \nSingle-cell analysis reveals inflammatory interactions driving macular degeneration Manik Kuchroo, Marcello DiStasio, Eric Song, Eda Calapkulu, Le Zhang, Maryam Ige, Amar Sheth, Abdelilah Majdoubi, Madhvi Menon, Alexander Tong, Abhinav Godavarthi, Yu Xing, Scott Gigante, Holly Steach, Jessie Huang, Guillaume Huguet, Janhavi Narain, KY, George Mourgkos, Rahul Dhodapkar, Matthew Hirn, Bastian Rieck, Guy Wolf, Smita Krishnaswamy, and Brian Hafler.  Nature Communications.\n        \n        Publisher's site\n    \nEvaluating ChatGPT in Medical Contexts: The Imperative to Guard Against Hallucinations and Partial Accuracies Mauro Giuffrè, KY, and Dennis Shung.  Clinical Gastroenterology and Hepatology.\n        \n        Publisher's site\n    \nAssessing the Usability of GutGPT: A Simulation Study of an AI Clinical Decision Support System for Gastrointestinal Bleeding Risk Colleen Chan, KY, Sunny Chung, Mauro Giuffrè, Theo Saarinen, Niroop Rajashekar, Yuan Pu, Yeo Eun Shin, Loren Laine, Ambrose Wong, Rene Kizilcec, Jasjeet Sekhon, and Dennis Shung.  Machine Learning for Health (ML4H) Symposium.\n        \n        Preprint\n    \n2022\nNetwork Distance Based on Laplacian Flows on Graphs Dianbin Bao, KY, and Lizhen Lin.  IEEE International Conference on Big Data (Big Data).\n        \n        Preprint\n     \n        \n        Code\n     \n        \n        Publisher's site\n    \nParameter estimation and model-based clustering with spherical normal distribution on the unit hypersphere KY and Changhee Suh.  Computational Statistics and Data Analysis.\n        \n        Preprint\n     \n        \n        Code\n     \n        \n        Publisher's site\n    \nRdimtools: An R package for Dimension Reduction and Intrinsic Dimension Estimation KY and Dennis Shung.  Software Impacts.\n        \n        Preprint\n     \n        \n        Code\n     \n        \n        Publisher's site\n    \n Learning Subspaces of Different Dimensions Brian St. Thomas, KY, Lizhen Lin, Lek-Heng Lim, and Sayan Mukherjee.  Journal of Computational and Graphical Statistics.\n        \n        Preprint\n     \n        \n        Code\n     \n        \n        Publisher's site\n    \nGeometric learning of functional brain network on the correlation manifold KY and Hae-Jeong Park.  Scientific Reports.\n        \n        Code\n     \n        \n        Publisher's site\n    \n2021\nRe-visiting Riemannian geometry of symmetric positive definite matrices for the analysis of functional connectivity KY and Hae-Jeong Park.  NeuroImage.\n        \n        Code\n     \n        \n        Publisher's site\n    \n2020\nData transforming augmentation for heteroscedastic models Hyungsuk Tak, KY, Sujit K. Ghosh, Bingyue Su, and Joseph Kelly.  Journal of Computational and Graphical Statistics.\n        \n        Preprint\n     \n        \n        Publisher's site\n    \n2019\nVolume change pattern of decompression of mandibular odontogenic keratocyst Jin Hoo Park, Eun-Jung Kwak, KY, Young-Soo Jung, and Hwi-Dong Jung.  Maxillofacial Plastic and Reconstructive Surgery.\n        \n        Publisher's site\n    \n2016\nVision-based detection of loosened bolts using the Hough transform and support vector machines Young-Jin Cha, KY, and Wooram Choi.  Automation in Construction.\n        \n        Publisher's site\n    \nPreprint\nPCA, SVD, and Centering of Data Donggun Kim and KY (2023).\n        \n        Preprint\n    \nComparing multiple latent space embeddings using topological analysis KY, Ilmun Kim, Ick Hoon Jin, Minjeong Jeon, and Dennis Shung (2022).\n        \n        Preprint\n     \n        \n        Code\n    \nShape-Preserving Dimensionality Reduction : An Algorithm and Measures of Topological Equivalence Byeongsu Yu and KY (2021).\n        \n        Preprint"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Kisung You",
    "section": "",
    "text": "Welcome!\nI am an assistant professor in the Department of Mathematics at Baruch College, City University of New York and a research affiliate at the Yale School of Medicine. My research revolves around harnessing mathematical tools from statistics, geometry, and topology to design innovative methods aimed at unraveling intricate scientific inquiries. My primary focus resides within the domain of biomedical data analysis. Additionally, I have deep enthusiasm for high-performance computing and the development of statistical software, driven by the aspiration to make methodological breakthroughs accessible to a broader audience of practitioners.\n\n\nContact\nFor students, please use my email available from the department directory."
  },
  {
    "objectID": "notes-teaching/note01-spherical-distance.html",
    "href": "notes-teaching/note01-spherical-distance.html",
    "title": "Monte Carlo computation of \\(L_p\\) distance between two densities on the unit hypersphere",
    "section": "",
    "text": "A \\(d\\)-dimensional unit hypersphere \\(\\mathbb{S}^d = \\lbrace x \\in \\mathbb{R}^{d+1}~|~ \\|x\\|_2^2 = \\sum_{i=1}^{d+1} x_i^2 = 1\\rbrace\\) is one of the standard mathematical spaces in the field of objected-oriented data analysis (Marron and Dryden 2021). Let \\(\\mathcal{P}(\\mathbb{S}^d)\\) denote a space of probability densities on \\(\\mathbb{S}^d\\). For two densities \\(f,g\\in\\mathcal{P}(\\mathbb{S}^d)\\), it is frequently needed to measure dissimilarity between the two. Unfortunately, even for the most well-known distributions on the hypersphere, analytic formula for any discrepancy measure is rarely available, leading to require numerical schemes for approximation. Here we focus on \\(L_p\\) distance between the two densities, \\[\nL_p (f,g) = \\left( \\int_{\\mathbb{S}^d} |f(x) - g(x)|^p \\right)^{1/p}\n\\tag{1}\\] and we show how to combine Monte Carlo way of integration by means of importance sampling to approximate Equation 1."
  },
  {
    "objectID": "notes-teaching/note01-spherical-distance.html#problem-statement",
    "href": "notes-teaching/note01-spherical-distance.html#problem-statement",
    "title": "Monte Carlo computation of \\(L_p\\) distance between two densities on the unit hypersphere",
    "section": "",
    "text": "A \\(d\\)-dimensional unit hypersphere \\(\\mathbb{S}^d = \\lbrace x \\in \\mathbb{R}^{d+1}~|~ \\|x\\|_2^2 = \\sum_{i=1}^{d+1} x_i^2 = 1\\rbrace\\) is one of the standard mathematical spaces in the field of objected-oriented data analysis (Marron and Dryden 2021). Let \\(\\mathcal{P}(\\mathbb{S}^d)\\) denote a space of probability densities on \\(\\mathbb{S}^d\\). For two densities \\(f,g\\in\\mathcal{P}(\\mathbb{S}^d)\\), it is frequently needed to measure dissimilarity between the two. Unfortunately, even for the most well-known distributions on the hypersphere, analytic formula for any discrepancy measure is rarely available, leading to require numerical schemes for approximation. Here we focus on \\(L_p\\) distance between the two densities, \\[\nL_p (f,g) = \\left( \\int_{\\mathbb{S}^d} |f(x) - g(x)|^p \\right)^{1/p}\n\\tag{1}\\] and we show how to combine Monte Carlo way of integration by means of importance sampling to approximate Equation 1."
  },
  {
    "objectID": "notes-teaching/note01-spherical-distance.html#computation",
    "href": "notes-teaching/note01-spherical-distance.html#computation",
    "title": "Monte Carlo computation of \\(L_p\\) distance between two densities on the unit hypersphere",
    "section": "Computation",
    "text": "Computation\nImportance sampling requires a proposal density. The easiest choice is to use uniform density \\(u(x)\\) as an importance proposal since sampling from \\(u(x)\\) is trivial. First, take a random sample from standard normal distribution \\(x \\sim \\mathcal{N}(0,I)\\) in \\(\\mathbb{R}^{d+1}\\). Then, the rest is to take \\(L_2\\) normalization, i.e., \\(x \\leftarrow x / \\|x\\|_2\\), which makes a sampled vector to have a unit norm. Given the sample generation process, we have the following \\[\\begin{aligned}\nL_p (f,g)^p &= \\int_{\\mathbb{S}^d} |f(x)-g(x)|^p dx \\\\\n&= \\int_{\\mathbb{S}^d} \\frac{|f(x)-g(x)|^p}{u(x)} u(x)  dx \\\\\n&= \\mathbb{E}_{u(x)} \\left\\lbrack \\frac{|f(x)-g(x)|^p}{u(x)} \\right\\rbrack\\\\\n&\\approx \\frac{1}{N} \\sum_{n=1}^N \\frac{|f(x_n)-g(x_n)|^p}{u(x_n)} \\,\\,\\textrm{for}\\,\\, x_n \\overset{iid}{\\sim} u(x),\n\\end{aligned}\n\\] where the last term gets better approximation as \\(N\\rightarrow \\infty\\).\nHere the uniform density \\(u(x)\\) is an inverse of the surface area of the \\(d\\)-dimensional sphere \\(S_n\\), which is defined as \\[\nS_n = \\frac{2\\pi^{(n+1)/2}}{\\Gamma((n+1)/2)},\n\\] where \\(\\Gamma(x)\\) is the gamma function."
  },
  {
    "objectID": "notes-teaching/note01-spherical-distance.html#references",
    "href": "notes-teaching/note01-spherical-distance.html#references",
    "title": "Monte Carlo computation of \\(L_p\\) distance between two densities on the unit hypersphere",
    "section": "References",
    "text": "References\n\n\nMarron, James Stephen, and I. L. Dryden. 2021. Object Oriented Data Analysis. Boca Raton: Taylor & Francis Group, LLC."
  },
  {
    "objectID": "notes-teaching/note02-rodriguez.html",
    "href": "notes-teaching/note02-rodriguez.html",
    "title": "Rodrigues’ formula for the Legendre polynomials",
    "section": "",
    "text": "Legendre polynomials \\(P_n (x)\\) are solutions of the following Legendre’s differential equation \\[\n(1-x^2) y'' - 2x y' + n(n+1) y = 0\n\\tag{1}\\] for some \\(n \\in \\mathbb{N}\\cup \\{0\\}\\). An explicit, compact expression for the polynomials is provided by Rodrigues’ formula \\[\nP_n (x) = \\frac{1}{2^n n!} \\frac{d^n}{dx^n} (x^2-1)^n.\n\\tag{2}\\]\nThis means that when \\(P_n (x)\\) is plugged in the position of \\(y\\) for Equation 1, it must satisfy the equality to 0. In this post, we show (a bit tedious) derivations to attain Equation 2."
  },
  {
    "objectID": "notes-teaching/note02-rodriguez.html#introduction",
    "href": "notes-teaching/note02-rodriguez.html#introduction",
    "title": "Rodrigues’ formula for the Legendre polynomials",
    "section": "",
    "text": "Legendre polynomials \\(P_n (x)\\) are solutions of the following Legendre’s differential equation \\[\n(1-x^2) y'' - 2x y' + n(n+1) y = 0\n\\tag{1}\\] for some \\(n \\in \\mathbb{N}\\cup \\{0\\}\\). An explicit, compact expression for the polynomials is provided by Rodrigues’ formula \\[\nP_n (x) = \\frac{1}{2^n n!} \\frac{d^n}{dx^n} (x^2-1)^n.\n\\tag{2}\\]\nThis means that when \\(P_n (x)\\) is plugged in the position of \\(y\\) for Equation 1, it must satisfy the equality to 0. In this post, we show (a bit tedious) derivations to attain Equation 2."
  },
  {
    "objectID": "notes-teaching/note02-rodriguez.html#approach",
    "href": "notes-teaching/note02-rodriguez.html#approach",
    "title": "Rodrigues’ formula for the Legendre polynomials",
    "section": "Approach",
    "text": "Approach\nI will proceed in two steps. Let \\(f_n (x) = (x^2 - 1)^n\\) then we first show that the \\(n\\)-th derivative of \\(f_n (x)\\) is a solution of Legendre equation. Then, we find a proper scaling factor of \\(1/ 2^n n!\\) to recover \\(P_n (x)\\) in line with a common constraint that \\(P_n (x) = 1\\) for all \\(n\\) when \\(x=1\\). For notational simplicity, we denote \\(g^{(n)}\\) for the \\(n\\)-th derivative of a function \\(g(x)\\), i.e, \\[\ng^{(n)} = \\frac{d^n}{dx^n} g(x).\n\\]\nBefore proceeding, we need (generalized) Leibniz’s rule. Suppose we have \\(n\\)-times differentiable functions \\(f(x)\\) and \\(g(x)\\), then \\[\n\\frac{d^n}{dx^n} f(x) g(x) = \\sum_{k=0}^n\n\\begin{pmatrix}\nn \\\\ k\n\\end{pmatrix}\nf^{(n-k)} (x) g^{(k)} (x) = \\sum_{k=0}^n \\frac{n!}{k! (n-k)!} f^{(n-k)} (x) g^{(k)} (x)\n\\tag{3}\\] where the choice of \\(f\\) and \\(g\\) can help in reducing the number of terms when there exists a polynomial term. For example, when \\(g(x) = x^2\\), \\(g^{(k)} = 0\\) for all \\(k \\geq 3\\).\n\nStep 1. \\(f_n^{(n)} (x)\\) is one solution.\nOur goal here is to show that \\(f_n^{(n)}(x)\\) is a solution for Equation 1. As a first step, let’s take derivative on \\(f_n (x)\\), \\[\n\\begin{align*}\n\\frac{d}{dx} f_n (x) &= 2 n (x^2  - 1)^{n-1} x \\\\\n&= 2nx (x^2 - 1)^{n-1}.\n\\end{align*}\n\\] By multiplying \\((x^2 - 1)\\) both side of the above, we have \\[\n(x^2 - 1) \\frac{d}{dx} f_n (x)  = 2nx (x^2 - 1)^n.\n\\]\nNow, differentiate both sides \\((n+1)\\) times, which leads to \\[\n\\begin{align*}\n\\frac{d^{n+1}}{dx^{n+1}} \\left[ \\frac{d}{dx} f_n (x) \\right]  (x^2 -1)\n&= \\sum_{k=0}^{n+1}\n\\begin{pmatrix}\nn+1 \\\\ k\n\\end{pmatrix}\n\\left( \\frac{d}{dx} f_n (x) \\right)^{(n+1-k)} (x^2-1)^{(k)} \\\\\n&=\n\\begin{pmatrix}\nn+1 \\\\ 0\n\\end{pmatrix}\nf_n^{(n+2)} (x) (x^2-1) +\n\\begin{pmatrix}\nn+1 \\\\ 1\n\\end{pmatrix}\n2x f_n^{(n+1)}  (x) +\n\\begin{pmatrix}\nn+1 \\\\ 2\n\\end{pmatrix}\nf_n ^{(n)} (x) \\cdot 2 \\\\\n&= (x^2-1) f_n^{(n+2)} (x) + 2(n+1)x f_n^{(n+1)} (x) +\nn(n+1) f_n^{(n)} (x)\n\\end{align*}\n\\] for the left-hand side. We also have that \\[\n\\begin{align*}\n\\frac{d^{n+1}}{dx^{n+1}} f_n(x) 2nx &=\n\\begin{pmatrix}\nn+1 \\\\ 0\n\\end{pmatrix}\nf_n^{(n+1)} (x) 2nx +\n\\begin{pmatrix}\nn+1 \\\\ 1\n\\end{pmatrix}\nf_n^{(n)} (x) 2n \\\\\n&= 2nx f_n^{(n+1)} (x) + 2n(n+1)f_n^{(n)}(x).\n\\end{align*}\n\\] Therefore, we have the following arrangement, \\[\n\\begin{equation*}\n    \\begin{gathered}\n    (x^2 - 1) f_n^{(n+2)} (x) + 2x(n+1)f_n^{(n+1)} (x) + n(n+1)f_n^{(n)} (x) =\n    2nx f_n^{(n+1)} (x) + 2n(n+1) f_n^{(n)} (x) \\\\\n    (x^2-1) f_n^{(n+2)} (x) + 2x f_n^{(n+1)}(x) - n(n+1)f_n^{(n)} (x) = 0 \\\\\n    (1- x^2) f_n^{(n+2)} (x) - 2x f_n^{(n+1)}(x) + n(n+1)f_n^{(n)} (x) = 0\n    \\end{gathered}\n\\end{equation*}\n\\] where the last line is in the form of Equation 1 so that we have \\(f_n^{(n)} (x)\\) as a solution.\n\n\nStep 2. Find a scaling factor.\nEven though \\(f_n^{(n)} (x)\\) as a solution, we have a requirement for the standard Legendre polynomial that \\(P_n (x)=1\\) for \\(x=1\\). Let us take a closer look at \\(f_n^{(n)}(x)\\) when evaluated at \\(x=1\\). \\[\n\\begin{align*}\nf_n^{(n)} (x) &= \\frac{d^n}{dx^n} (x^2-1)^n \\\\\n&= \\frac{d^n}{dx^n} (x+1)^n (x-1)^n \\\\\n&= \\sum_{k=0}^n \\begin{pmatrix}\nn \\\\ k\n\\end{pmatrix}\n\\left( (x+1)^n \\right)^{(k)} \\left( (x-1)^n \\right) ^{(n-k)} \\quad \\textrm{by the Leibniz's rule} \\\\\n&= \\sum_{k=0}^n \\begin{pmatrix}\nn \\\\ k\n\\end{pmatrix}\n\\frac{n!}{(n-k)!} (x+1)^{n-k} \\frac{n!}{k!} (x-1)^k \\qquad (*) \\\\\n&= n! \\sum_{k=0}^n \\begin{pmatrix}\nn \\\\ k\n\\end{pmatrix} \\frac{n!}{(n-k)! k!} (x+1)^{n-k} (x-1)^k \\\\\n&= n! \\sum_{k=0}^n \\begin{pmatrix}\nn \\\\ k\n\\end{pmatrix}^2 (x+1)^{n-k} (x-1)^k,\n\\end{align*}\n\\] Since we want to evaluate \\(f_n^{(n)} (x)\\) at \\(x=1\\), the last line of equations above tells us that all the terms but \\(k=0\\) become zero, \\[\n\\begin{equation*}\nf_n^{(n)} (x=1) = n! \\begin{pmatrix}\nn \\\\ 0\n\\end{pmatrix}^2 2^{n-0} = n! 2^n\n\\end{equation*}\n\\] which finally leads to define \\(P_n (x)\\) as \\[\nP_n (x) = \\frac{1}{n! 2^n}f_n^{(n)}(x)  = \\frac{1}{n! 2^n} \\frac{d^n}{dx^n} (x^2-1)^n\n\\] to fulfill the condition of \\(P_n (x) =1\\) for \\(x=1\\)."
  }
]